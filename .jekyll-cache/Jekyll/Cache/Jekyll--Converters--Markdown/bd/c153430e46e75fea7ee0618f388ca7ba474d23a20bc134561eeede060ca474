I"Ÿ><p>We consider a population of N coupled-phase oscillators, $\theta_i(t)$, $i=1,\ldots,N$ and we want to compute a control such that in time $T$ we reach the state $\theta_i(T)=\ldots =\theta_N(T)=:\bar{\theta}$ in which all the oscillators are sincronized.</p>

<h2 id="the-mathematical-model">The mathematical model</h2>

<p>The dynamics of the oscillators is described by the Kuramoto model</p>

<script type="math/tex; mode=display">\dot{\theta}_i(t) = \frac{1}{N}\sum_{j=1}^N \sin(\theta_j(t)-\theta_i(t)) + b_i(t),\;\;\; i=1,\ldots,N</script>

<script type="math/tex; mode=display">\theta_i(0) = \theta_i^0.</script>

<p>The model is non-linear, then, we apply a standard linearization process around the steady state $\bar{\theta}$ obtaining</p>

<script type="math/tex; mode=display">\dot{\Theta}(t) = A\Theta(t),\;\;\;\Theta(0) = \Theta^0,\;\;\;\Theta =(\theta_1,\ldots,\theta_N)^T.</script>

<p>By classical techniques, the control is obtained by minimizing the following cost functional, subject to the linearized Kuramoto model, for each node</p>

<script type="math/tex; mode=display">J(b_i) = \frac{1}{2N} \sum_{j=1}^N(\theta_j(T)-\theta_i(T))^2+\frac{\beta}{2}\int_0^T b_i(t)^2dt,\;\;\;\beta>0,</script>

<p>in which</p>

<ul>
  <li>
    <p>the first term measures the distance between the rest of nodes and the target.</p>
  </li>
  <li>
    <p>the second term is a penalization one introduced in order to   avoid using control with a too large size.</p>
  </li>
</ul>

<p>In fact, through the above minimization procedure, we find the control $b = (b_1,\ldots,b_N)$ which has minimum norm among all the controls capable to steer the system to the equilibrium dynamics.</p>

<h2 id="how-do-you-usually-solve-this-type-of-problem">How do you usually solve this type of problem?</h2>

<p>The usual technique for solving minimizing problems based on quadratic functionals is the Gradient Descent Method, which is based on the following iterative algorithm</p>

<script type="math/tex; mode=display">b^{k+1}_i = b^k_i - \eta\nabla J(b_i^k).</script>

<p>This technique is tipycally chosen because: - It is (relatively) easy to implement. - It is not very memory demanding.</p>

<h2 id="what-we-propose">What we propose?</h2>

<p>As the functional J(b_i) depend on all nodes $\Theta_j$, the gradient $\nabla J(b_i)$ too. Therefore, in each iteration of the Gradient Descent algorithm we shall consider all nodes in the network. Then, when the number of nodes is large, the complexity if substantial per iteration. To avoid evaluate the full gradient, we propose to approach the problem by means of the Stochastic Descent Gradient method which only uses a small portion of data.</p>

<p>Matrix $A$ of the linearized Kuramoto model</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span> <span class="o">=</span> <span class="mi">7</span><span class="p">;</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">/</span><span class="n">N</span><span class="p">)</span><span class="o">*</span><span class="nb">ones</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">);</span>
<span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">:</span><span class="n">N</span>
    <span class="n">A</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
<span class="k">end</span>
<span class="n">A</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
A =

   -1.0000    0.1429    0.1429    0.1429    0.1429    0.1429    0.1429
    0.1429   -1.0000    0.1429    0.1429    0.1429    0.1429    0.1429
    0.1429    0.1429   -1.0000    0.1429    0.1429    0.1429    0.1429
    0.1429    0.1429    0.1429   -1.0000    0.1429    0.1429    0.1429
    0.1429    0.1429    0.1429    0.1429   -1.0000    0.1429    0.1429
    0.1429    0.1429    0.1429    0.1429    0.1429   -1.0000    0.1429
    0.1429    0.1429    0.1429    0.1429    0.1429    0.1429   -1.0000


</code></pre></div></div>

<p>Initial condition: <script type="math/tex">\Theta(0) = \Theta^0.</script></p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mu</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span><span class="n">sigma</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span>
<span class="n">theta0</span> <span class="o">=</span> <span class="n">normrnd</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
theta0 =

   16.2469
    3.5642
    9.8242
   -6.0650
    4.6452
   10.0029
   -9.6151


</code></pre></div></div>

<p>Now, choose initial control,</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t0</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span><span class="n">T</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span><span class="n">dt</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">;</span>
<span class="n">tspan</span> <span class="o">=</span> <span class="n">t0</span><span class="p">:</span><span class="n">dt</span><span class="p">:</span><span class="n">T</span><span class="p">;</span>
<span class="n">u0</span> <span class="o">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">length</span><span class="p">(</span><span class="n">tspan</span><span class="p">),</span><span class="n">N</span><span class="p">);</span>
</code></pre></div></div>

<p>We can solve with the StochasticGradient function.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Results_Stochastic</span> <span class="o">=</span>  <span class="n">StochasticGradient</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">theta0</span><span class="p">,</span><span class="n">tspan</span><span class="p">,</span><span class="n">u0</span><span class="p">);</span>
</code></pre></div></div>

<p>And see the convergence</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig1</span> <span class="o">=</span> <span class="n">JPlotConvergence</span><span class="p">(</span><span class="n">Results_Stochastic</span><span class="p">,</span><span class="s1">'Convergence of Stochastic Method'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP01/P0007/copiaRM_01.png" alt="" /></p>

<p>We can see the result obtained</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig2</span> <span class="o">=</span> <span class="n">LastThetaPlot</span><span class="p">(</span><span class="n">Results_Stochastic</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP01/P0007/copiaRM_02.png" alt="" /></p>

<h2 id="comparison-clasical-vs-stochastic">Comparison Clasical vs Stochastic</h2>

<p>In this example, we show that stochastic method is faster than the classical descent in a small network.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Results_Classical</span> <span class="o">=</span> <span class="p">{};</span><span class="n">Results_Stochastic</span> <span class="o">=</span> <span class="p">{};</span>
<span class="c1">%%</span>
<span class="n">maxN</span> <span class="o">=</span> <span class="mi">100</span><span class="p">;</span> <span class="c1">%% &lt;=== Maximal number of oscillator allowed in the model</span>
<span class="n">iter</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">for</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">1</span><span class="p">:</span><span class="mi">10</span><span class="p">:</span><span class="n">maxN</span>
    <span class="c1">%% We solve the problem for each N</span>
    <span class="n">iter</span> <span class="o">=</span> <span class="n">iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
    <span class="c1">%% Definition of the linearized Kuramoto problem</span>
    <span class="n">A</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">/</span><span class="n">N</span><span class="p">)</span><span class="o">*</span><span class="nb">ones</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">);</span>
    <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">:</span><span class="n">N</span>
        <span class="n">A</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="k">end</span>
    <span class="c1">%% Initial state</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span><span class="n">sigma</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span>
    <span class="n">theta0</span> <span class="o">=</span> <span class="n">normrnd</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">,</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
    <span class="c1">%% Initial control</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span><span class="n">T</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span><span class="n">dt</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">;</span>
    <span class="n">tspan</span> <span class="o">=</span> <span class="n">t0</span><span class="p">:</span><span class="n">dt</span><span class="p">:</span><span class="n">T</span><span class="p">;</span>
    <span class="n">u0</span> <span class="o">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">length</span><span class="p">(</span><span class="n">tspan</span><span class="p">),</span><span class="n">N</span><span class="p">);</span>

    <span class="c1">%% Classical Gradient Method</span>
    <span class="n">Results_Classical</span><span class="p">{</span><span class="n">iter</span><span class="p">}</span>  <span class="o">=</span>  <span class="n">ClassicalGradient</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">theta0</span><span class="p">,</span><span class="n">tspan</span><span class="p">,</span><span class="n">u0</span><span class="p">);</span>
    <span class="c1">%% Stochastic Gradient Method</span>
    <span class="n">Results_Stochastic</span><span class="p">{</span><span class="n">iter</span><span class="p">}</span> <span class="o">=</span>  <span class="n">StochasticGradient</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">theta0</span><span class="p">,</span><span class="n">tspan</span><span class="p">,</span><span class="n">u0</span><span class="p">);</span>
<span class="k">end</span>
</code></pre></div></div>

<p>We can graphically see the following result. If the number of nodes increase we can see better the efectiveness of the stochastic gradient descent method with respect to the classical.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">%% For the next step it is necessary to convert the cells into arrays</span>
<span class="n">Results_Classical</span>  <span class="o">=</span> <span class="p">[</span><span class="n">Results_Classical</span><span class="p">{:}];</span>
<span class="n">Results_Stochastic</span> <span class="o">=</span> <span class="p">[</span><span class="n">Results_Stochastic</span><span class="p">{:}];</span>
<span class="c1">%%</span>
<span class="n">fig2</span> <span class="o">=</span> <span class="nb">figure</span><span class="p">;</span> <span class="n">ax</span>  <span class="o">=</span> <span class="nb">axes</span><span class="p">;</span>
<span class="n">ax</span><span class="o">.</span><span class="n">FontSize</span> <span class="o">=</span> <span class="mi">17</span><span class="p">;</span>
<span class="n">ax</span><span class="o">.</span><span class="n">XLabel</span><span class="o">.</span><span class="n">String</span> <span class="o">=</span> <span class="s1">'Number of Oscillators'</span><span class="p">;</span> <span class="n">ax</span><span class="o">.</span><span class="n">YLabel</span><span class="o">.</span><span class="n">String</span> <span class="o">=</span> <span class="s1">'time(s)'</span><span class="p">;</span>
<span class="n">ax</span><span class="o">.</span><span class="n">XGrid</span> <span class="o">=</span> <span class="s1">'on'</span><span class="p">;</span> <span class="n">ax</span><span class="o">.</span><span class="n">YGrid</span> <span class="o">=</span> <span class="s1">'on'</span><span class="p">;</span>
<span class="c1">%%</span>
<span class="nb">line</span><span class="p">([</span><span class="n">Results_Stochastic</span><span class="o">.</span><span class="n">N</span><span class="p">],[</span><span class="n">Results_Stochastic</span><span class="o">.</span><span class="n">t</span><span class="p">],</span><span class="s1">'Parent'</span><span class="p">,</span><span class="n">ax</span><span class="p">,</span><span class="s1">'Color'</span><span class="p">,</span><span class="s1">'red'</span><span class="p">,</span><span class="s1">'Marker'</span><span class="p">,</span><span class="s1">'s'</span><span class="p">)</span>
<span class="nb">line</span><span class="p">([</span><span class="n">Results_Classical</span><span class="o">.</span><span class="n">N</span><span class="p">],[</span><span class="n">Results_Classical</span><span class="o">.</span><span class="n">t</span><span class="p">],</span><span class="s1">'Parent'</span><span class="p">,</span><span class="n">ax</span><span class="p">,</span><span class="s1">'Color'</span><span class="p">,</span><span class="s1">'blue'</span><span class="p">,</span><span class="s1">'Marker'</span><span class="p">,</span><span class="s1">'s'</span><span class="p">)</span>
<span class="nb">legend</span><span class="p">({</span><span class="s1">'Stochastic Gradient'</span><span class="p">,</span><span class="s1">'Classical Gradient'</span><span class="p">})</span>
</code></pre></div></div>

<p><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP01/P0007/copiaRM_03.png" alt="" /></p>

<p>Altough in this example we are dealing with a small network, we have shown that stochastic method is faster than the classical descent. If the number of nodes increase we can see better the efectiveness of the stochastic gradient descent method with respect to the classical. <img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP01/P0007/ClassicalVStochastic.png" alt="" /></p>

:ET