---
title: "Understanding Deep Learning: Neural Networks as bending manifolds"
author: [SergiA]
date: 2020-11-01
description: We analyze the propagation properties of the numerical versions of one and two-dimensional wave equations, semi-discretized in space by finite difference schemes, and we illustrate that numerical solutions may have unexpected behaviours with respect to the analytic ones.
layout: pretutorial
categories: [pretutorial,WP99]
noshow: true
url_zip: assets/imgs/WP99/P0008/P0016_Propagation_of_discretes_waves.zip
avatar: https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0008/animation.gif
code: MATLAB
equation: WAVES
---


## Data and classification
### Supervised Learning

Consider a dataset $\mathcal{D}$ consisting on $S$ distinct points $\{x_i\}_{i=1}^{S}$, each of them with a corresponding value $y_i$ such that


$$
 \mathcal{D} = \{(x_i , y_i)\}_{i=1}^S
$$


$$
 y_i = \mathcal{F} (x_i) + \epsilon _i \quad \forall i \in \{1, ... S\}
$$


where $\epsilon _i$ is included so as to consider, in principle, noisy data.

The aim of supervised learning is to recover the function $\mathcal{F}(\cdot)$ such that not only fits well in the database $\mathcal{D}$ but also generalises to previously unseen data. 

That is, it is inferred the existence of 

$$
\tilde{\mathcal{D}} = \{ (\tilde{x}_i , \tilde{y}_i) \}_{i=1}^{\infty}
$$

where 

$$
\tilde{y}_i \approx \mathcal{F}(\tilde{x}_i) \quad \forall i 
$$


being $\mathcal{F} (\cdot)$ the desired previous application, and $\mathcal{D} \subset \tilde{\mathcal{D}}$.


![GeneralisedDataset]({{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/generaliseddataset.jpg)

<br>

###  Noisy vs. noisless data

The generalised dataset $\tilde{\mathcal{D}}$ can be seen as the realisation of a random vector $(\mathbf{x}, \mathbf{y})$ generated by an unknown multivariate joint distribution between $\mathcal{X}$ and  $\mathcal{Y}$. 

Then, the datasets used in Supervised Learning can be seen as a finite sample of the random vector $(\mathbf{x}, \mathbf{y})$, and so the underlying joint distribution between $\mathcal{X}$ and $\mathcal{Y}$ could be in principle recovered in the limit of infinitely many samples, $\tilde{\mathcal{D}}$ .

We refer to noisy data when the samples in our dataset $\mathcal{D}$ actually comes from a random vector $(\mathbf{x}, \mathbf{y})$ with a non-deterministic joint distribution, such that it is not possible to find a function $\mathcal{F}$ such that $\mathcal{F}(\mathbf{x}) = \mathbf{y}$.

The aim of supervised learning, in this case, would be to find a function such that $\mathcal{F}(\tilde{x}_i) \approx \tilde{y}_i$ for almost every sample, as with respect to a given loss function.

In case the data is considered to be noisless (which does not happen in most practical situations, but it makes things far simpler) then the joint distribution between $\mathcal{X}$ and $\mathcal{Y}$ of the random vector $(\mathbf{x}, \mathbf{y})$ is rather deterministic. In this case, it is possible to find a function $F$ such that $\mathcal{F}(\tilde{x}_i) = \tilde{y}_i \; \forall i$, or equivalently $\mathcal{F}(\mathbf{x}) = \mathbf{y}$. 

Note that, given that the probability of a collision (of having two different data-points such that $x_i = x_j$ for $i \neq j$) is practically $0$, it is in principle possible to find a function $\mathcal{F}$ such that $\mathcal{F}(x_i)=y_i \; \forall i = \{ 1, ...  ,S \}$, even in the noisy case (i.e. we may use interpolation). 
However, the aim of *Supervised Learning* is to be able to generalise well to previously unseen data, $\tilde{\mathcal{D}}$, and so interpolation is not effective. 

Some systems are often approximated as being noisless. Since the aim of supervised learning is finding functions $\mathcal{F}$, there is always the implicit assumption that the generalised dataset $\tilde{\mathcal{D}}$ can be approximately represented by a function $\mathcal{F}$, and so the noise is expected to be small, compared to the "real data". Distinguishing data from noise is often far a from trivial issue.

The idea of working with noisy data can be recasted as a Mean-Field problem, as proposed by Weinan E. et al [2]. The idea is to control a distribution, instead of a set of points in an euclidean space. Although understanding Supervised Learning with noisless data from a Dynamical Control perspective is often enough ill-posed, considering noisy data unlocks the full theoretical experience.

![Noisless]({{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/generalnoisless.gif)

<br>

### Binary classification
In most cases, the data points $x_i$ are in $\mathbb{R}^{n}$. In general, $x_i \in X \; \forall i$, with $X \subset \mathbb{R}^{n}$. So as to ease the training of the models, it is usually considered $X = [-1,+1]^{n}$, with $n$ an arbitrary dimension. This is called *data normalization* in the Machine Learning literature.

There are different supervised learning problems depending on the space $Y$ of the corresponding labels, such that $y_i \in Y$.  
If $Y$ is discrete, such that $Y = \{ 0, ... , L-1\}$, then we are dealing with the problem of classification. If $L=2$, then it is a binary classification problem.

In this particular setting, the aim is to find a function $F (\cdot)$ such that it returns $0$ whenever the corresponding data point is of class $0$, and $1$ conversely. 
Note that such a function cannot be continuous except for trivial cases, given that  the data points in $X$ cover all the space $[-1, +1]$, due to the existence of a boundary between points of class $0$ and $1$.  


### Data, subspaces and manifolds

Considering $X^0$ the subset of $X$ such that $F(\tilde{x}_i) =0 \; \; \forall \tilde{x}_i \in X^0$ and $X^1$ s.t.  $\, F(\tilde{x}_i)=1 \; \; \forall \tilde{x}_i \in X^1$, it is often useful to consider $d(X^0 , X^1) > \delta \in \mathbb{R}^{+}$. 
Consider that the $x-$points in the dataset $\tilde{\mathcal{D}}$ are in $X= X^0 \cup X^1$.

In this aforementioned case, it is in principle possible to find a continuous function $F(\cdot)$ such that $F(\tilde{x}_i) = \tilde{y}_i \; \forall i$ , since it does not need to be defined in the boundary (the boundary is not in $X$).

The simpler case is one in which $X^0$ and $X^1$ are connected spaces. 

<br>





## Deep Learning
### Neural Networks

The problem is now how to obtain the functions $\mathcal{F}$. 

In classical settings, an hypothesis space is often proposed as 

$$
 \mathcal{H} = \left\{  f (\cdot \, ; \mathbf{\theta }) \; \bigg\vert  f(\cdot ; \theta ) = \phi \left(\sum_{i=1}^{m} a_i \psi _i (\cdot) \right) \right\}
$$


where $\theta = \{ a_i \}_{i=1}^{m}$ are the coefficients, $\{ \psi _i\}$ are the proposed functions (for example, they would be monomials if we are doing polynomial regression, sine and cosines if using Fourier, ...) and the function $\phi$ is the terminal loss function, that is used to match the dimensionality of $f(x_i \; ;  \theta)$ with $y_i$; for example, in the case of binary classification, $\phi (  \cdot )$ can be chosen to be the function $sign ( \cdot )$, and so $\phi (\cdot ) : \mathbb{R} \to \{0,1 \}$. 


The function $\mathcal{F}$ correspondent to the dataset $\mathcal{D}$ is then defined as


$$
\mathcal{F}(\cdot ) \equiv f(\cdot \, ; \theta ^*) \in \mathcal{H} \quad s.t. \quad \theta ^* = \arg \min \sum_{i=1}^{S} d(\phi (f(x_i \, ; \theta )) , y_i)  
$$


where $d ( , )$ is a distance defined by a loss function. 
That is, it is the function in the hypothesis space that minimizes the "distance" with the target function, which connects the datapoints $x_i$ with their correspondent labels $y_i$.


When using Neural Networks, the hypothesis space $\mathcal{H}$ is generated by the composition of functions, such that


$$
 \mathcal{H} =  \left\{  \text{NN}(\cdot \, ; \Theta ) \; \bigg\vert  \text{NN}(\cdot ; \Theta ) = \phi \circ f_{\theta_L}^L \circ f_{\theta_{L-1}}^{L-1} \circ ... \circ f_{\theta_1}^1 (\cdot) \right\} 
$$


where $\Theta = \{ \theta_k \}_{k=1}^{L}$ are the training weights of the network and $L$ the number of hidden layers.




#### Multilayer perceptrons

In the case of the so-called multilayer perceptrons, the functions $f^k_{\theta_k}$ are constructed as


$$
 f^k_{\theta_k} (\cdot) = \sigma (A^k  (\cdot) +  b^k)
$$

being $A^k$ a matrix,$b^k$ a vector and $\sigma \in C^{0,1}(\mathbb{R})$ a fixed nondecreasing function. 

Since we restrict to the case in which the number of neurons is constant through the layers, we have that $A^k \in \mathbb{R}^{d \times d}$ and $b^k \in \mathbb{R}^d$.

![Multilayer]({{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/multilayerdiagram.jpg)

#### Residual Neural Networks
In the simple case of a Residual Neural Network (often referred to as ResNets) the functions $f_{\theta_k}^{k} (\cdot)$ are constructed as


$$
f_{\theta_k}^{k} (\cdot) = \mathbb{I} (\cdot) + \sigma (A^k  (\cdot) +  b^k)  
$$


where $\mathbb{I}$ is the identity function. The parameters $\sigma$, $A^k$ and $b^k$ are equivalent to those of the multilayer perceptrons.

It is sometimes convenient to numerically add a parameter $h$ to the residual block, such that


$$
f_{\theta_k}^{k} (\cdot) = \mathbb{I} (\cdot) + h  \, \sigma (A^k  (\cdot) +  b^k)  
$$


being $h \in \mathbb{R}$ a scalar.

![ResNet]({{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/resnetsdiagram.jpg)

### Training Neural Networks

The aim is to find the weights $\{ A, b \}$ of every layer such that the final function represented by the Neural Network minimizes a given loss function. 

The weights are initialized randomly, and the algorithms (i.e. gradient descent) are aimed at updating them, such that eventually the Neural Network represents a function  $\hat{F}$  such that $\hat{F} (x_i) \approx y_i \, \forall i$.




<br>

## Visualizing Deep Learning

<table style="width:100%">
  <tr>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/layertransformation0.gif"></th>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/layertransformation1.gif"></th>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/layertransformation2.gif"></th>
  </tr>
  <tr>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/layertransformation3.gif"></th>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/layertransformation4.gif"></th>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/layertransformation5.gif"></th>
  </tr>
</table>

### Classification

When dealing with data classification, it is very useful to just assign a color / shape to every label, and so be able to visualize data in a lower-dimensional plot. 

The aim of classification is to associate different classes to different regions of the initial space,
When using Neural Networks, the initial space $X$ is propagated by the Neural Network as $f ^L \circ  f^{L-1} \circ ... \circ f^1 (X)$. 
Then the function $\phi$ would be the one in charge of associating different classes to different regions, but the functions $\phi$ are pretty simple (essentially linear separators)!

So, while it may be difficult to classificate the datapoints in the initial space $X$, the Neural Networks should make things simpler and just propagate the data so as to make it linearly separable (and we would see this in real action!).

### Neural Networks as homeomorphisms



***Lemma 1.1***
*A Neural Network represents a continuous function if each one of its blocks are continuous.*

***Lemma 1.2*** 
*A Neural Network represents a bijection if each one of its blocks are bijective.*

These come trivially from the fact that the composite of continuous functions is continuous, and that the composite of bijections is a bijection. 

***Theorem 1*** **(Sufficient condition for invertible ResNets [1]):**

Let $\hat{\mathcal{F}} \, : \, \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ with $\hat{\mathcal{F}} (\cdot ) = ( L^{1}\_{\theta} \, \circ ... \circ \, L^{T}_{\theta}  )$ denote a *ResNet* with blocks defined by 
$$
L^{t} _{ \theta } = \mathbb{I} + h \cdot g_{\theta _t} 
$$

Then, the ResNet $\hat{\mathcal{F}}_{\theta}$ is invertible if 

$$
h \cdot \text{Lip} (g_{\theta_t}) < 1 \text{ for all } t = 1, ..., T
$$




***Proof*** 
Consider that each block of the *ResNet* is given by

$$
Z(x) =  x + h\cdot G(x) 
$$


where $x$ is the input of the block, and $G(x)$ given by the residual block.
That can be rewritten as


$$
x_{t+1} = x_t + g(x) 
$$


We can construct the inverse function of the block as


$$
x_t = x_{t+1} - g(x)
$$


Which is not an explicit inverse, since $g(x)$ depends on $x$. Approximating the solution as an interation,


$$
x^{0}_{t}  \doteq x_{t+1} \quad \text{and} \quad x_t ^{k+1} \doteq x_{t+1} - g (x_{t}^{k})
$$


where $\lim_{k \to \infty} x_t ^{k} = x_t$ if the iteration converges. 
Since $g(\cdot) : \mathbb{R}^{d} \to \mathbb{R}^{d}$ is an operator on a Banach space, due to the Banach fixed point theorem is it enough that $Lip(g) < 1$, or equally that $Lip(h\cdot G) <1$ for the iteration to converge, and so for the block of the *ResNet* to be invertible. [1]

***Comment:*** *In the limit $h \to 0$, and $G(x)$ Lipschitz, we have a trivial explicit expression of the inverse of the function represented by the ResNet.*

Consider the *ResNet* as generated by the iteration of blocks

$$
Z(x) =  x + h\cdot G(x) 
$$


where $G(x)$ is generated by a residual block [](). We consider $G(x)$ Lipschitz.

Then,


$$
Z^{-1} (x) \equiv x - h\cdot G(x)
$$


is an second order approximation of the inverse function of $Z$ in terms of $h$, since


$$
Z^{-1} (Z(x)) = Z^{-1}  (x + h\cdot G(x) ) = 
$$


$$
= x + h\cdot G(x) - h \cdot G(x + h \cdot G(x)) = 
$$


$$
= x + \mathcal{o}(h^2)
$$


Indeed, if we express the residual blocks as


$$
x_{t+1} = x_{t} + h\cdot G(x_t)
$$


the limit $h\to 0$ can be recasted as


$$
\lim_{h \to 0} \frac{x_{t+1}-x_{t}}{h} = \dot{x}_t = G(x_t)
$$


and, since the discrete index $t$ becomes continuous in this limit, it is useful to express it as

$$
 \dot{x}(t) = G(x(t), t)
$$

where in this case we make explicit the dependence of $G$ on $t$, that was hidden in the previous notation.

<br>

<table style="width:100%">
  <tr>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/3dmultilayer_.gif"></th>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/3dmultilayer_spiral.gif"></th>
  </tr>
</table>


## Setup

Consider a binary classification problem with noisless data, such that


$$
\mathcal{D} =  \{ ( x_i , y_i = \mathcal{F}(x_i) )\} _{i=1}^{S}
$$


$$
x_i \in [-1,+1]^{n} , \; \; y_i \in \{0,1\} \quad \forall i \in \{1, ... , S\}
$$


<table style="width:100%">
  <tr>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/2dbending0.gif"></th>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/2dbending1.gif"></th>
  </tr>
  <tr>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/2dbending2.gif"></th>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/2dbending3.gif"></th>
  </tr>
</table>

<br>

<table style="width:100%">
  <tr>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/2dbendingspiral0.gif"></th>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/2dbendingspiral1.gif"></th>
  </tr>
  <tr>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/2dbendingspiral2.gif"></th>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/2dbendingspiral3.gif"></th>
  </tr>
</table>

<br>

<table style="width:100%">
  <tr>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/3dbendingspiral2.gif"></th>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/3dbendingspiral3.gif"></th>
  </tr>
</table>


<br>


<table style="width:100%">
  <tr>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/bending0.gif"></th>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/bending1.gif"></th>
  </tr>
  <tr>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/bending2.gif"></th>
    <th><img src="{{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/bending3.gif"></th>
  </tr>
</table>


<br>

## Open questions, issues and perspectives

![NoisyDistribution]({{site.url}}{{site.baseurl}}/assets/imgs/WP99/P0011/NoisyRing.gif)

## References

[1] J. Behrmann, W. Grathwohl, R. T. Q. Chen, D. Duvenaud, J. Jacobsen (2018). <i> Invertible Residual Networks</i> [pdf](https://arxiv.org/abs/1811.00995)

[2] Weinan E, Jiequn Han, Qianxiao Li, (2018). <i> A Mean-Field Optimal Control Formulation of Deep Learning</i>. volume 6. Research in the Mathematical Sciences[pdf](https://doi.org/10.1007/s40687-018-0172-y)

[3] S. Shalev-Shwartz, S. Ben-David, <i> The Understanding Machine Learning: From Theory to Algorithms </i>. [pdf](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/)

[4] B. Hanin, (2018). <i> Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients? </i>  [pdf](https://arxiv.org/abs/1801.03744)

[5] C. Olah, <i> Neural Networks, Manifolds and Topology </i>. [blog post](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)

[6] G. Naitzat, A. Zhitnikov, L-H. Lim, (2020). <i> Topology of Deep Neural Networks </i> [pdf](https://arxiv.org/abs/2004.06093)

[7] Q. Li, T. Lin, Z. Shen, (2019). <i> Deep Learning via Dynamical Systems: An Approximation Perspective </i>[pdf](https://arxiv.org/abs/1912.10382)

[8] B. Geshkovski, <i> The interplay of control and deep learning </i>. [blog post](https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0010)

[9] C. Fefferman, S. Mitter, H. Narayanan, <i> Testing the manifold hypothesis </i>. [pdf](http://www.mit.edu/~mitter/publications/121_Testing_Manifold.pdf)

[10] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, D. Duvenaud, (2018). <i> Neural Ordinary Differential Equations </i> [pdf](https://arxiv.org/abs/1806.07366)

