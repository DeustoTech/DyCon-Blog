<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DyCon Blog</title>
    <description>Welcome to the web interface of DyCon Toolbox, the computational platform developed within the &lt;a href='https://cmc.deusto.eus/dycon/' target='_blank'&gt;ERC DyCon - Dynamic Control&lt;/a&gt; project.</description>
    <link>https://deustotech.github.io/DyCon-Blog/</link>
    <atom:link href="https://deustotech.github.io/DyCon-Blog/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 04 Nov 2019 10:31:24 +0100</pubDate>
    <lastBuildDate>Mon, 04 Nov 2019 10:31:24 +0100</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Optimal Control Problem with CasADi on null-controllability of the network system</title>
        <description>This code needs the installation of CasADi 3.4.5: https://web.casadi.org/get/

In this post, we will consider the null-controllability in the finite difference scheme of the 2D Heat equation. As a modification to the optimal control problem, we use CasADi and IpOpt to simulate optimization.

A similar problem has been treated in the DyCon Blog post, https://deustotech.github.io/DyCon-Blog/tutorial/wp03/WP03-P0022 which deals with the one-dimensional fractional heat equation under positivity constraints.

We start with the finite difference scheme of the 2D Heat equation and the control on one boundary of the square domain $[0,1]^2$. 

We will consider the problem of steering the initial datum: 

to the zero solution. This is a well-known controllable problem.

Problem formulation

Parameters for the problem

N_size = 4; %% Space discretization for 1D
Nx = N_size^2; %% Space discretization for 2D
Nt = 20; %% Time discretization : need to check the CFL condition
T = 0.1; %% Final time

%% Discretization of the Space
xline = linspace(0,1,N_size+2);
xline = xline(2:end-1);
dx = xline(2) - xline(1);
%%
xline_f = linspace(xline(1),xline(end),2*N_size);

[xms,yms]   = meshgrid(xline,xline);
[xmsf,ymsf] = meshgrid(xline_f,xline_f);

%% Initial data
Y0_2d = zeros(N_size);
for i=1:N_size
    for j=1:N_size
        Y0_2d(i,j) = sin(xline(i)*pi)*sin(xline(j)*pi);
    end
end
Y0 = Y0_2d(:);
%% Target data
Y1 = 0.3*Y0;

%% Definition of the dynamics : Y' = AY+BU
B = zeros(N_size^2,N_size); B(1:N_size,1:N_size) = eye(N_size);

A1_mat = diag(-4*ones(N_size,1),0);
A1_mat = A1_mat + diag(ones(N_size-1,1),1) +diag(ones(N_size-1,1),-1);
A2_mat = A1_mat;
for j=1:N_size-1
    A2_mat = blkdiag(A2_mat,A1_mat);
end
A_mat = A2_mat + diag(ones(N_size.*(N_size-1),1),N_size) + diag(ones(N_size.*(N_size-1),1),-N_size);
A = A_mat; %% A: Discrete Laplacian in 2D with uniform squared mesh


clf
options_plot_graphs = {'LineWidth',2,'DisplayName','off','NodeColor','r','ArrowSize',9,'MarkerSize',7,'NodeLabel',{}};
plot(digraph(A),'XData',xms(:)','YData',yms(:)',options_plot_graphs{:})
xlabel('x-axis')
ylabel('y-axis')
grid on




%% Discretization of the time : we need to check CFL condition to change 'Nt'.
tline = linspace(0,T,Nt+1); %%uniform time mesh

dt = tline(2)-tline(1);

%% Simulation of the uncontrolled trajectory
M = eye(Nx) - 0.5*dt/dx.^2*A;
L = eye(Nx) + 0.5*dt/dx.^2*A;
P = 0.5*dt*B;

Y = zeros(Nx,Nt+1);
Y(:,1) = Y0;
for k=1:Nt %% loop over time intervals
   %% Crank-Nicolson method without control
   Y(:,k+1) = M\L*Y(:,k);
end
YT = Y(:,Nt+1);


clf
Z = reshape(Y(:,1),N_size,N_size);
Z = interp2(xms,yms,Z,xmsf,ymsf);
isurf = surf(xmsf,ymsf,Z,'FaceAlpha',0.5);
isurf.CData = isurf.CData*0 + 1;
hold on
Z = reshape(Y(:,end),N_size,N_size);
Z = interp2(xms,yms,Z,xmsf,ymsf);
jsurf = surf(xmsf,ymsf,Z,'FaceAlpha',0.5);
jsurf.CData = jsurf.CData*0 + 10;
%%
jsurf.Parent.Color = 'none';
lightangle(10,10)
%%
legend({'Initial Condition','Target'})




Optimization problem

opti = casadi.Opti();  %% CasADi function

%% ---- Input variables ---------
X = opti.variable(Nx,Nt+1); %% state trajectory
U = opti.variable(N_size,Nt+1);   %% control

%% ---- Dynamic constraints --------
for k=1:Nt %% loop over control intervals
   %% Crank-Nicolson method : this helps us to boost the optimization
   opti.subject_to(M*X(:,k+1)== L*X(:,k) + P*(U(:,k)+U(:,k+1)));
end

%% ---- State constraints --------
opti.subject_to(X(:,1)==Y0);
opti.subject_to(X(:,Nt+1)==Y1);

%% ---- Optimization objective  ----------
Cost = (dx*sum(sum(U.^2))*(T/Nt));
opti.minimize(Cost); %% minimizing L2 at the final time

%% ---- initial guesses for solver ---
opti.set_initial(X, Y);
opti.set_initial(U, 0);

%% ---- solve NLP              ------
p_opts = struct('expand',true);
s_opts = struct('max_iter',10000); %% iteration limitation

opti.solver('ipopt',p_opts,s_opts); %% set numerical backend
tic
sol = opti.solve();   %% actual solve
toc


This is Ipopt version 3.12.3, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation).

Number of nonzeros in equality constraint Jacobian...:     2752
Number of nonzeros in inequality constraint Jacobian.:        0
Number of nonzeros in Lagrangian Hessian.............:       84

Total number of variables............................:      420
                     variables with only lower bounds:        0
                variables with lower and upper bounds:        0
                     variables with only upper bounds:        0
Total number of equality constraints.................:      352
Total number of inequality constraints...............:        0
        inequality constraints with only lower bounds:        0
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0

iter    objective    inf_pr   inf_du lg(mu)  \vert\vertd\vert\vert  lg(rg) alpha_du alpha_pr  ls
   0  0.0000000e+00 1.38e-01 0.00e+00  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0
   1  6.9867496e+02 8.88e-16 2.49e-08  -2.5 1.78e+02    -  1.00e+00 1.00e+00h  1

Number of Iterations....: 1

                                   (scaled)                 (unscaled)
Objective...............:   6.9867496486950017e+02    6.9867496486950017e+02
Dual infeasibility......:   2.4853882507613889e-08    2.4853882507613889e-08
Constraint violation....:   8.8817841970012523e-16    8.8817841970012523e-16
Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00
Overall NLP error.......:   2.0349560410724553e-09    2.4853882507613889e-08


Number of objective function evaluations             = 2
Number of objective gradient evaluations             = 2
Number of equality constraint evaluations            = 2
Number of inequality constraint evaluations          = 0
Number of equality constraint Jacobian evaluations   = 2
Number of inequality constraint Jacobian evaluations = 0
Number of Lagrangian Hessian evaluations             = 1
Total CPU secs in IPOPT (w/o function evaluations)   =      0.140
Total CPU secs in NLP function evaluations           =      0.000

EXIT: Optimal Solution Found.
               t_proc [s]   t_wall [s]    n_eval
       nlp_f      1.1e-05        1e-05         2
       nlp_g      5.7e-05      5.7e-05         2
  nlp_grad_f        2e-05      1.9e-05         3
  nlp_hess_l        4e-06        4e-06         1
   nlp_jac_g      9.3e-05      9.4e-05         3
      solver        0.154        0.149         1
Elapsed time is 0.186246 seconds.





Post-processing

Sol_x = sol.value(X); %% solved controlled trajectory
Sol_u = sol.value(U); %% solved control
%%Sol_x = opti.debug.value(X);
%%Sol_u = opti.debug.value(U); %% final data if algorithm stops with error

Sol_z = zeros(N_size+2,N_size+2,1); %% displaying variable
Sol_z(2:end-1,2:end-1) = reshape(Sol_x(:,end),[N_size,N_size]);
Sol_y = zeros(N_size+2,N_size+2,1); %% displaying uncontrolled variable
Sol_y(2:end-1,2:end-1) = reshape(YT,[N_size,N_size]);

xxline = repmat(linspace(0,1,N_size+2),[N_size+2 1]);
yyline = repmat(linspace(0,1,N_size+2)',[1 N_size+2]);
max_y = max(Sol_z(:));

figure
surf(xxline,yyline,Sol_y(:,:));
xlabel('x'); ylabel('y'); zlabel('z'); title('Uncontrolled final values');
zlim([-max_y max_y])
%%hold on
figure
%%hold on
surf(xxline,yyline,Sol_z(:,:));
zlim([-max_y max_y])
xlabel('x'); ylabel('y'); zlabel('z'); title('Controlled final values');






Final_2 = zeros(N_size+2,N_size+2,Nt+1); %% displaying variable
Final_2(2:end-1,2:end-1,:) = reshape(Sol_x(:,:),[N_size,N_size,Nt+1]);

Final_1 = zeros(N_size+2,N_size+2,Nt+1); %% displaying variable
Final_1(2:end-1,2:end-1,:) = reshape(Y(:,:),[N_size,N_size,Nt+1]);

%% Free dynamics
fig = figure;
isurf = surf(Final_1(:,:,1));
zlim([-1 1])
for it = 1:Nt+1
   isurf.ZData =  Final_1(:,:,it);
%%    pause(0.1)
end
%% Controlled dynamics
fig = figure;
isurf = surf(Final_2(:,:,1));
zlim([-1 1])
for it = 1:Nt+1
   isurf.ZData =  Final_2(:,:,it);
%%    pause(0.1)
end






The simulation needs careful regularization coefficients, 1e5 which is big enough, in the cost function. This is from the nature of the Laplacian, which has significantlly different eigenvalues and high control cost for small time horizon.

Next, we consider a small modification of the dynamics ‘A_mat’, which makes the dynamics controllable by only one node (1,1). This is impossible in the current operator since the initial data ‘Y0’ is not controllable.

The following matrix ‘AS_mat’ links the nodes in a line, for example, 1-2-3-6-5-4-7-8-9 for $N=3$, which is a partial network of ‘A_mat’. Since the 1D heat equation is controllable, ‘AS_mat’ is controllable by the first node:

AC_mat = diag(ones(N_size-1,1),-1);
for j=1:2:N_size-2
    AC_mat = blkdiag(AC_mat,diag(ones(N_size-1,1),1));
    AC_mat(end,end-N_size)=1;
    if N_size==2
        break
    end
    AC_mat = blkdiag(AC_mat,diag(ones(N_size-1,1),-1));
    AC_mat(end-N_size+1,end-2*N_size+1)=1;
end
if length(AC_mat) &amp;lt; (N_size.^2)
    AC_mat = blkdiag(AC_mat,diag(ones(N_size-1,1),1));
    AC_mat(end,end-N_size)=1;
end

AS_mat = (AC_mat + AC_mat') - 2*eye(Nx);


options_plot_graphs = {'LineWidth',2,'DisplayName','off','NodeColor','r','ArrowSize',9,'MarkerSize',7,'NodeLabel',{}};
plot(digraph(AS_mat),'XData',xms(:)','YData',yms(:)',options_plot_graphs{:})
xlabel('x-axis');ylabel('y-axis')
title('Network')
grid on




Note that ‘AS_mat’ has the nonzero elements in the positions that ‘A_mat’ has. By deleting several interactions of ‘A_mat’, ‘AS_mat’ is now exactly controllable by one node. This is the idea of the structural controllability in the network system, called ‘network control’.

B = zeros(N_size^2,1); B(1) = 1;
A = N_size.^2*AS_mat;
%% Y' = AY + BU

T = 0.1;
Nt = 30;
tline = linspace(0,T,Nt+1); %%uniform time mesh
dt = tline(2)-tline(1);

%% Simulation of the uncontrolled trajectory
M = eye(Nx) - 0.5*dt/dx.^2*A;
L = eye(Nx) + 0.5*dt/dx.^2*A;
P = 0.5*dt*B;

Y = zeros(Nx,Nt+1);
Y(:,1) = Y0;
for k=1:Nt %% loop over time intervals
   %% Crank-Nicolson method without control
   Y(:,k+1) = M\L*Y(:,k);
end
YT = Y(:,Nt+1);
Y1 = 1.5*YT;
%% surf(Y)
%% plot(YT)


Optimization problem

opti = casadi.Opti();  %% CasADi function

%% ---- Input variables ---------
X = opti.variable(Nx,Nt+1); %% state trajectory
U = opti.variable(1,Nt+1);   %% control

%% ---- Dynamic constraints --------
for k=1:Nt %% loop over control intervals
   %% Crank-Nicolson method
   opti.subject_to(M*X(:,k+1)== L*X(:,k) + P*(U(:,k)+U(:,k+1)));
end

%% ---- State constraints --------
opti.subject_to(X(:,1)==Y0);
opti.subject_to(X(:,Nt+1)==Y1);

%% ---- Optimization objective  ----------
Cost = (sum(U.^2)*(T/Nt)); %%1e3*dx^2*sum(sum((X(:,Nt+1)-Y1).^2))+;
opti.minimize(Cost); %% minimizing L2 at the final time

%% ---- Initial guess ----
opti.set_initial(X, Y);
opti.set_initial(U, 0);

%% ---- solve NLP              ------
p_opts = struct('expand',true);
s_opts = struct('max_iter',1e5); %% cut down the algorithm at the 1000-th iteration.
opti.solver('ipopt',p_opts,s_opts); %% set numerical backend
tic
sol = opti.solve();   %% actual solve
toc


This is Ipopt version 3.12.3, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation).

Number of nonzeros in equality constraint Jacobian...:     2852
Number of nonzeros in inequality constraint Jacobian.:        0
Number of nonzeros in Lagrangian Hessian.............:       31

Total number of variables............................:      527
                     variables with only lower bounds:        0
                variables with lower and upper bounds:        0
                     variables with only upper bounds:        0
Total number of equality constraints.................:      512
Total number of inequality constraints...............:        0
        inequality constraints with only lower bounds:        0
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0

iter    objective    inf_pr   inf_du lg(mu)  \vert\vertd\vert\vert  lg(rg) alpha_du alpha_pr  ls
   0  0.0000000e+00 1.04e-01 0.00e+00  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0
   1  2.9522421e+04 8.88e-16 6.25e-02  -2.5 1.08e+03    -  1.00e+00 1.00e+00h  1

Number of Iterations....: 1

                                   (scaled)                 (unscaled)
Objective...............:   2.9522420946189726e+04    2.9522420946189726e+04
Dual infeasibility......:   6.2500000000000000e-02    6.2500000000000000e-02
Constraint violation....:   8.8817841970012523e-16    8.8817841970012523e-16
Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00
Overall NLP error.......:   1.4002320737719130e-12    6.2500000000000000e-02


Number of objective function evaluations             = 2
Number of objective gradient evaluations             = 2
Number of equality constraint evaluations            = 2
Number of inequality constraint evaluations          = 0
Number of equality constraint Jacobian evaluations   = 2
Number of inequality constraint Jacobian evaluations = 0
Number of Lagrangian Hessian evaluations             = 1
Total CPU secs in IPOPT (w/o function evaluations)   =      0.357
Total CPU secs in NLP function evaluations           =      0.000

EXIT: Optimal Solution Found.
               t_proc [s]   t_wall [s]    n_eval
       nlp_f      1.4e-05      1.4e-05         2
       nlp_g      5.7e-05      5.8e-05         2
  nlp_grad_f      2.1e-05        2e-05         3
  nlp_hess_l        5e-06        5e-06         1
   nlp_jac_g      8.9e-05      8.8e-05         3
      solver        0.378        0.125         1
Elapsed time is 0.391622 seconds.









Post-processing

%%Sol_x = sol.value(X); %% solved controlled trajectory
%%Sol_u = sol.value(U); %% solved control
Sol_x = opti.debug.value(X);
Sol_u = opti.debug.value(U); %% final data if algorithm stops with error

Sol_z = zeros(N_size+2,N_size+2,1); %% displaying variable
Sol_z(2:end-1,2:end-1) = reshape(Sol_x(:,end),[N_size,N_size]);
Sol_y = zeros(N_size+2,N_size+2,1); %% displaying uncontrolled variable
Sol_y(2:end-1,2:end-1) = reshape(YT,[N_size,N_size]);

xxline = repmat(linspace(0,1,N_size+2),[N_size+2 1]);
yyline = repmat(linspace(0,1,N_size+2)',[1 N_size+2]);
max_y = max(abs(Sol_z(:)));

figure(1)
surf(xxline,yyline,Sol_y(:,:));
xlabel('x'); ylabel('y'); zlabel('z'); title('Uncontrolled final values');
zlim([-max_y max_y])
%%hold on
figure(2)
%%hold on
surf(xxline,yyline,Sol_z(:,:));
zlim([-max_y max_y])
xlabel('x'); ylabel('y'); zlabel('z'); title('Controlled final values');






Recover the trajectories

The algorithm of IpOpt does not guarantee the dynamics exactly follows the discrete time-evolution, but approximate it. We may use the linear solver in Matlab to recover the exact dynamics with the implicit C-N method. This process is needed if we stop the algorithm and test that it is an approximate solution.

Yc = zeros(Nx,Nt+1);
Yc(:,1) = Y0;
for k=1:Nt %% loop over control intervals
   %% Crank-Nicolson method
   Yc(:,k+1) = M\L*Yc(:,k) + M\P*(Sol_u(k)+Sol_u(k+1));
   %% M Y(:,k+1) = L Y(:,k) + P (u(k)+u(k+1));
end
YTc = Yc(:,Nt+1);
figure
plot(YT,'-')
hold on
plot(Sol_x(:,end),'r*')
plot(YTc,'b*')

%% Save the data
Final_3 = zeros(N_size+2,N_size+2,Nt+1); %% displaying variable
Final_3(2:end-1,2:end-1,:) = reshape(Y(:,:),[N_size,N_size,Nt+1]);

Final_4 = zeros(N_size+2,N_size+2,Nt+1); %% displaying variable
Final_4(2:end-1,2:end-1,:) = reshape(Yc(:,:),[N_size,N_size,Nt+1]);

%% Free dynamics
fig = figure;
isurf = surf(Final_3(:,:,1));
zlim([-2 2])
for it = 1:Nt+1
   isurf.ZData =  Final_3(:,:,it);
   pause(0.1)
end
%% Controlled dynamics
fig = figure;
isurf = surf(Final_4(:,:,1));
zlim([-2 2])
for it = 1:Nt+1
   isurf.ZData =  Final_4(:,:,it);
   pause(0.1)
end










Sure, this problem needs a big control cost (or long time horizon) and also more calculation costs for optimization since its close approximation ‘A_mat’ was not controllable.

</description>
        <pubDate>Fri, 01 Nov 2019 00:00:00 +0100</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp06/P0027-DongnamHeat2D</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp06/P0027-DongnamHeat2D</guid>
        
        
        <category>tutorial</category>
        
        <category>WP06</category>
        
      </item>
    
      <item>
        <title>Numerical simulation of nonlinear Population Dynamics Structuring by age and Spatial diffusion</title>
        <description>In this tutorial, we propose the Hum method to approximate numerically the control in a null controllability problem for a non linear population dynamics model structuring in age and spatial diffusion.

For given the positive function $F$, we consider the following Population Dynamics Model Structuring by age and spatial diffusion:



where, $\Omega$ is a boundary subset of $\mathbb{R}^{N}$, $A$ is the maximal age and $T&amp;gt;0$ the final time. Also, $\Theta=\omega× (a_1,a_2)× (0,T),$ is the control support. Here, $\beta$ is the fertility rate depending also the age a, $\mu$ the mortality rate depending of the age a, $y_0$ the initial condition in $L^2(\Omega× (0,A)),$ $V$ is the control term and $y$ represent the density of the population of age $a$ at time $t$ and location $x∈ \Omega.$

We assume that the fertility rate $\beta$ and the mortality rate $\mu$ satisfy the demographic property:



and



where



The function $F,$ verify:



Numerical Simulations

In this part, the objective is to illustrate numerically the approximate null controllability of the nonlinear problem.

Indeed, we consider the following system:



Discretization and simulation of uncontrolled system

The idea in this part is to highlight the numerical simimulation of the nonlinear problem. The first parts is to reduce the PDE to the finite dimensional system of the form



where $A_l$ and $B$ are matrices, and



is the finite dimensional state and control vector. Here, VectorF is the contribution of the nonlinear part, which comes from births. So let’s consider the following system



where



To solve $(4)$, the space (dimension 1) and age discretization is performed with finite difference method on rectangular grid 
on $[0,L]\times [0,A]$. For a given rectangular grid $\mathcal{T}$ with vertex $(x_i,a_j)\quad 1\leq i\leq N,\quad 1\leq j\leq M$ and uniform step size (without loss of generality) $\Delta x$ in $x-$direction and $\Delta a$ in $a-$direction, we denote by  the diameter of the grid
The finite difference approximation of the diffusion term of the operator $\textbf{A}$ is given by



The finite difference approximation of the aging term of the operator A is given by



Let $y_{i,j}(t)$ be the approximation of $y(x_i,a_j)$ and



where $y_{i,j}$ is at the position $i+j*(N-1)$ and $A_l$ the matrix of the mortality approximation, the diffusion approximation and the aging approximation.

Take into account the newborns

We denote by $\beta(a),$ the fertility rate, the newborn is given by:



We approximate $\int_{0}^{A}\beta(a)y(x,a,t)da$ by



Then



But as



then



We create also the nonlinear vectorF from the births. Indeed, if we denote by



the matrix of the births,



and we define the vectorF by:



The corresponding discrete system is given for a continuous initial solution $y_0$ by



Example 1

Simulation of the uncontrolled system:

For the simulation, we take $L=1,\text{ } A=10\text{, }\Delta x=1/54\quad \Delta a= 5/54$ and $T=40$. Moreover $F(t)=t\Phi(t)$ a Globally Lipchitz function where



The fertility $\beta$ is given by:



here $v=1$, $\gamma=5$, $\alpha=7$ and the mortality rate $\mu$ by









Video 1. Evolution of the state of the uncontrolled system between t=0 and t=30 

We observe in this video the growth (aging) of news borns over time. We also notice the high birth rate of this population.

Construction of the control and numerical simulation

We construct the control problem, which consists in minimizing the functional and we choose the classical Hum functional and the control matrix $B=\chi_{\Theta}$ where $\Theta=(x_1,x_2)\times (a_1,a_2)\times (0,T),$ and we suppose that $U_T$ is the desired state:



Here $U_T=0.$
The approximate null controllability become the minimization of  the functional $J,$ where $(U_l,V_l)$ verify the following system



In this part the fertility $\beta$ is given by:



here $v=1$, $\gamma=5$, $\alpha=7$
 and



and $VectorF=(P_lU_l).^2$.This means that we consider that $F(t)=t^2$ which is globally lipchizt on a compact which is our case.

The system becomes:



Example 2

In this example, we take $ε=0.05$, $\Delta x=0.05$ and $\Delta a= 0.5$, with the initial condition



the following numerical results were obtained for $T=12$ and $\Delta t=120$.

Evolution of the control V between t=0 and t=12.



Video 2. Evolution of the state of controlled system between t=0 and t=12 

We notice that, with a control domain that takes into account the most populated part of the domain, the Hum method applied to the Casadi algorithm gives us an interesting approximative controllability result while maintaining the positivity of the density.
Evolution of the controlled state (density of the population) between t=0 and t=12.




Video 3. Evolution of the control function between t=0 and t=12 

Example 3

Here, we keep the same parameters of the Example 1 but we change the final time (here T=20) , and we take $\Delta x=1/18$, $Δ a=5/18$, $\epsilon=0.05$ the following numerical results were obtained:



Video 4. Evolution of the state of the controlled system between t=0 and t=20 





Video 4. Evolution of the state of the controlled system between t=0 and t=20 

Example 4

In this part we consider a smaller control domain. Indeed we take 
$\Theta =(0,2/5)\times(0,4)\times(0,T)$. 
In addition we also remove $\epsilon=0.001$ and $T=20.$ The rest is identical to the data of example 2.

We obtain the following numerical results:

Evolution of the control V between t=0 and t=20.





Video 4. Evolution of the state of the controlled system between t=0 and t=20 

Here although the result is acceptable, we notice that the state of the controlled system is negative for most of the time.
Evolution of the controlled state (the density of the population) between t=0 and t=20.





Video 5. Evolution of the control function between t=0 and t=20 
We notice here that the solution to the final time tends to 0, but during the time of control much of the state of the system was negative.
We notice that numerically we have approximation and the positivity of the state, if the support of the control covers the most populated part of the domain (space, age).

However it should be noted that we could not take a positivity constraint in our simulations. It should be noted that we have better approximations when we do not consider the cost of control.

References
[1] Dycon Toolbox

[2] CasaDi toolbox

[3] Luca Gerardo-Giorda: NUMERICAL APPROXIMATION OF DENSITY DEPENDENT DIFFUSION IN AGE-STRUCTURED POPULATION DYNAMICS
</description>
        <pubDate>Wed, 30 Oct 2019 00:00:00 +0100</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp05/P0051</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp05/P0051</guid>
        
        
        <category>tutorial</category>
        
        <category>WP05</category>
        
      </item>
    
      <item>
        <title>Movil control strategy for models with memory terms</title>
        <description>En este tutorial enseñaremos como implentar la estrategía del control interíor móvil para la ecuación del calor. En el ejemplo típico de control interior en la ecuación del calor se pretende llevar al sistema desde un estado inicial, $u_0$ hasta el estado nulo en todo el espacio, $\Omega$. Para ello se define una subconjunto, $\omega \in \Omega$, donde el control, $f(x,t)$ puede actuar, es decir que para puntos, $x \notin \omega$, el control es cero. Es conocido que la controlabilidad de la ecuación del calor para todo tiempo esta asegurada, sin embargo en el caso de la ecuación del calor con términos de memoria este resultado no es válido.



Donde $f$ es el control definido de solo en el subconjunto, $\omega \in \Omega$.




    
        
          
             
           
        
        
          
             
           
        
    
    
        
          
              Video 1: Heat equation
           
        
        
          
              Video 2:  Heat equation with memory
           
        
     
    
        
          
              Podemos ver la evolucion de dos modelos, con un región de control, $\X_{(a,b)}$, situado en la posición del elipsoide azul en la parte superior de la simulación. Podemos ver que el control LQR es eficaz para la ecuación del calor, sin embargo débil ante la presencia de término de memoria
           
        
     


En problema donde el laplaciano, $\Delta$, es un laplaciano fracionario, $(-\Delta)^s$, la controlabilidad del sistema esta condicionada por la potencia, $s$; el control móvil puede ser útil. Esta estrategía puede recuperar la propiedad de controlabilidad del sistema. Este modelo se pueda iamginar como un fuente/sumidero móvil que se mueve a puntos estratégicos del espacio para estabilizar la temperatura del lugar.

Por simplicidad, relizaremos la simulación con el laplaciano entero, sin embargo el control del laplaciano fraccionario se puede realizar de manera análoga.

Lo primero que deberemos definir es la función caracteristica del control, $\chi$ dependiente de un punto del espacio. Si trabajamos en coordenadas cartesianas podemos definir la función:



donde $\Theta(x)$ es la fución theta de heaviside. Un representación gráfica de $W(x,a,b)$ permiter reconocer a esta función con valor unidad entre los valores $a$ y $b$ y nulo fuera de ellos.





donde $u$ representa el estado



clear;


Ns = 8;
Nt = 5;
xline = linspace(-1,1,Ns);
yline = linspace(-1,1,Ns);

[xms,yms] = meshgrid(xline,yline);


Bfunction

xwidth = 0.35;
ywidth = 0.35;
B = @(xms,yms,xs,ys) WinWP05(xms,xs,xwidth).*WinWP05(yms,ys,ywidth);
Bmatrix =  @(xs,ys) diag(reshape(B(xms,yms,xs,ys),1,Ns^2));


A = FDLaplacial2D(xline,yline);

Atotal = zeros(Ns^2+4,Ns^2+4);
Atotal(1:Ns^2,1:Ns^2) = A;

RumbaMatrixDynamics = [0 0 1 0; ...
                       0 0 0 1; ...
                       0 0 0 0; ...
                       0 0 0 0 ];

Atotal(Ns^2+1:end,Ns^2+1:end) = RumbaMatrixDynamics;


gaussian function in 2D

gs = @(x,y,x0,y0,alpha) exp(-(x-x0).^2/alpha^2  - (y-y0).^2/alpha^2);

alpha = 0.1;
alphamid = 0.2;
%% We build the initial condition

U0 = + 2*gs(xms,yms,+0.25,+0.25,alpha) ...
     + 2*gs(xms,yms,+0.25,-0.25,alpha) ...
     + 2*gs(xms,yms,-0.25,-0.25,alpha);


surf(xms,yms,U0);




U0 = U0(:);


T = 0.05;
tspan = linspace(0,T,Nt+1);


Ysym = sym('Y',[Ns^2+4 1]);
Usym = sym('U',[Ns^2+2 1]);
FDT = @(t,X,U,Params) Atotal*X+ [Bmatrix(X(end-3),X(end-2))*U(1:end-2) ;0; 0; U(end-1:end)];

idynammics = pde(FDT,Ysym,Usym);
idynammics.mesh = {xline,yline};
idynammics.InitialCondition = [U0;0;0;0;0];
idynammics.Nt = Nt+1;
idynammics.FinalTime = T;
idynammics.Solver = @eulere;
%%
[~ , Xnum_free] = solve(idynammics);

Xnum_free = Xnum_free';


opti = casadi.Opti();  %% CasADi optimization structure

%% ---- Input variables ---------
Xcas = opti.variable(Ns^2+4,Nt+1); %% state trajectory
Ucas = opti.variable(Ns^2+2,Nt+1);   %% control

%% ---- Dynamic constraints --------
Fcas = @(x,u) Atotal*x+ [Bmatrix(x(end-3),x(end-2))*u(1:end-2) ;0;0; u(end-1:end)]; %% dx/dt = f(x,u)

for k=1:Nt %% loop over control intervals
   %% Euler forward method
   x_next = Xcas(:,k) + (T/Nt)*Fcas(Xcas(:,k),Ucas(:,k));
   opti.subject_to(Xcas(:,k+1)==x_next); %% close the gaps
end

%% ---- State constraints --------
opti.subject_to(Xcas(:,1)==[U0;0.5;0.5; 0 ;0]);

HeatCas = Ucas(1:end-2,:);
Max = 1e4;
opti.subject_to(HeatCas(:) &amp;lt;= Max)
opti.subject_to(HeatCas(:) &amp;gt;= -Max)

%% ---- Optimization objective  ----------
beta = 1e-2;
%%Cost = (Xcas(1:end-4,Nt+1))'*(Xcas(1:end-4,Nt+1)) + beta*sum(sum((Ucas(1:end-2,:))'*(Ucas(1:end-2,:))));
Cost = (Xcas(1:end-4,Nt+1))'*(Xcas(1:end-4,Nt+1));

opti.minimize(Cost); %% minimizing L2 at the final time

%% ---- initial guesses for solver ---
opti.set_initial(Xcas, Xnum_free);
opti.set_initial(Ucas, 0);


%% ---- solve NLP              ------
p_opts = struct('expand',false);
s_opts = struct('acceptable_tol',1e3,'constr_viol_tol',1e-3,'compl_inf_tol',1e-3);
opti.solver('ipopt',p_opts,s_opts); %% set numerical backend
tic
sol = opti.solve();   %% actual solve
toc


This is Ipopt version 3.12.3, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation).

Number of nonzeros in equality constraint Jacobian...:    23858
Number of nonzeros in inequality constraint Jacobian.:      768
Number of nonzeros in Lagrangian Hessian.............:      719

Total number of variables............................:      804
                     variables with only lower bounds:        0
                variables with lower and upper bounds:        0
                     variables with only upper bounds:        0
Total number of equality constraints.................:      408
Total number of inequality constraints...............:      768
        inequality constraints with only lower bounds:      384
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:      384

iter    objective    inf_pr   inf_du lg(mu)  \vert\vertd\vert\vert  lg(rg) alpha_du alpha_pr  ls
   0  2.8698289e-02 5.00e-01 2.50e-04  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0
   1  3.9465117e-02 5.25e-02 1.03e-01  -1.0 1.00e+01  -4.0 9.89e-01 1.00e+00h  1
   2  2.6459139e-02 2.64e-08 2.73e-04  -1.0 2.35e+00  -4.5 9.91e-01 1.00e+00h  1
   3  2.4596415e-02 3.87e-08 5.99e-04  -1.0 4.38e+00  -5.0 1.00e+00 1.00e+00f  1
...
...
...

Number of Iterations....: 227

                                   (scaled)                 (unscaled)
Objective...............:   2.3074995023555472e-06    2.3074995023555472e-06
Dual infeasibility......:   4.8878051464110080e-08    4.8878051464110080e-08
Constraint violation....:   5.3675648722691441e-05    5.3675648722691441e-05
Complementarity.........:   1.9187414051909107e-06    1.9187414051909107e-06
Overall NLP error.......:   5.3675648722691441e-05    5.3675648722691441e-05


Number of objective function evaluations             = 375
Number of objective gradient evaluations             = 228
Number of equality constraint evaluations            = 375
Number of inequality constraint evaluations          = 375
Number of equality constraint Jacobian evaluations   = 228
Number of inequality constraint Jacobian evaluations = 228
Number of Lagrangian Hessian evaluations             = 227
Total CPU secs in IPOPT (w/o function evaluations)   =      6.393
Total CPU secs in NLP function evaluations           =      0.339

EXIT: Solved To Acceptable Level.
               t_proc [s]   t_wall [s]    n_eval
       nlp_f      0.00229      0.00226       375
       nlp_g        0.067       0.0428       375
  nlp_grad_f       0.0025      0.00251       229
  nlp_hess_l       0.0464       0.0411       227
   nlp_jac_g        0.202        0.175       229
      solver         7.02         5.23         1
Elapsed time is 5.290691 seconds.



Xnum_with_control = sol.value(Xcas);


interpolation in time

AnimationMovilControl(Xnum_with_control,Xnum_free,tspan,xline,yline,Bmatrix)




</description>
        <pubDate>Wed, 30 Oct 2019 00:00:00 +0100</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp05/P0050</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp05/P0050</guid>
        
        
        <category>tutorial</category>
        
        <category>WP05</category>
        
      </item>
    
      <item>
        <title>System Identification of PDE equations from data</title>
        <description>En esta entrada veremos como podremos estimar  el sistema dinámico apartir de datos sobre la evolución de este sistema.

Lo primero que haremos es inspeccionar de que datos disponemos
&amp;gt;&amp;gt; whos

  Name               Size                  Bytes  Class     Attributes

  solution         100x200x50            8000000  double              
  tspan              1x200                  1600  double    

En este caso tenemos 100 simulaciones de una ecuación diferencial
</description>
        <pubDate>Tue, 01 Oct 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/ot02/P0001</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/ot02/P0001</guid>
        
        
        <category>tutorial</category>
        
        <category>OT02</category>
        
      </item>
    
      <item>
        <title>Touchdown localization for the MEMS problem with variable dielectric permittivity</title>
        <description>Background and motivation

An idealized version of a MEMS device consists of two conducting plates connected to an electric circuit
(see Figure 1).
The upper plate is rigid and fixed while the lower one is elastic and fixed only at the boundary.
Initially the plates are parallel and at unit distance from each other.
When a voltage (difference of potential between the two plates) is applied, the lower
plate starts to bend and, if the voltage is large enough,
the lower plate eventually touches the upper one, conducting to the so-called  pull-in instability. In the mathematical framework, this phenomenon is known as quenching or  touchdown.
Such device can be used for instance as an actuator, a microvalve (the touching-down part closes the valve),
or a fuse.


  



 Figure 1:  Two-membrane MEMS device


We consider a well-known model for micro-electromechanical systems (MEMS)
with variable dielectric permittivity, based on the following parabolic equation with singular nonlinearity:



where $\Omega$ is a smoothly bounded domain of $\mathbb{R}^2$ and 
 is a Hölder continuous function in .

In this mathematical model, the domain $\Omega$ represents the shape of the elastic plate in the horizontal direction, the solution $u=u(t,x)$ measures its vertical deflection,
while the function $f(x)$ is proportional to the constant voltage and characterizes the varying dielectric permittivity of the elastic plate.
We can write the permittivity profile $f$ as follows:



where $\lambda&amp;gt;0$ is the applied voltage, $\varepsilon_0&amp;gt;0$ is the permittivity of the vacuum and $\varepsilon_1(x)$ is the dielectric permittivity of the material. 
As a key feature, in our model the permittivity of the elastic plate may be inhomogeneous,
and this can be used to trigger the properties of the device. 
We refer to [1],[4],[5] and the references therein for the full details of the model derivation.

It is well known that problem (\ref{quenching problem}) admits a unique maximal classical solution $u$.
We denote its maximal existence time by $T=T_f\in (0,\infty]$. Moreover,
under some largeness assumption on $f$, it is known that the maximum of $u$ reaches the value $1$
at a finite time, so that $u$ ceases to exist in the classical sense. This is what we call  touchdown in finite time $T_f&amp;lt;\infty$.
Actually, if we consider $f(x) = \lambda\dfrac{\varepsilon_0}{\varepsilon_1(x)}$,
it is well-known that there exists a positive $\lambda^\ast$, known as  pull-in voltage, which depends on $\Omega$ and $\varepsilon_1(x)$, such that

$\bullet \quad $ If $\lambda&amp;lt;\lambda^\ast$, then $T_f=\infty$ and the global classical solution converges to the minimal positive steady state.

$\bullet \quad $ If $\lambda&amp;gt;\lambda^\ast$, then $T_f&amp;lt;\infty$, and then touchdown occurs in finite time.

In the actual design of a MEMS device there are several issues that must be considered.
Typically, one of the design goals is to achieve the maximum possible stable steady-state with relatively small applied voltage $\lambda$.  Another consideration may be to increase the stable operating range of the device by increasing the pull-in voltage.
For other devices, such as micropumps and microvalves, where touchdown behavior is explicitly exploited,
it is of interest to be able to localize the touchdown points, or similarly, to be able to avoid touchdown in certain parts of the device.
Our approach to achieve this last goal is to consider a material with varying dielectric permittivity allowing us to localize the touchdown points by a suitable choice of the permittivity profile.

Results

We start by the definition of touchdown point.
A point $x = x_0$ is called a  touchdown or  quenching point if there exists a sequence ${(x_n , t_n )} \in \Omega\times (0, T)$ such that



The set of all such points is called the  touchdown or  quenching set, denoted by $\mathcal{T}=\mathcal{T}_f\subset\overline{\Omega}$.

We show in [2] that touchdown can actually be ruled out in subregions of $\Omega$ 
where $f$ is positive but suitably small, below a positive threshold.

 Theorem 1:
Let $\Omega\subset \mathbb{R}^n$ a smooth bounded domain and $f$ a function satisfying



There exists $\gamma_0&amp;gt;0$ depending only on $\Omega,M,\mu,r$ such that:

(i) $\quad$ For any $x_0\in \Omega$, if $f(x_0) &amp;lt;\gamma_0 {\hskip 1pt} \text{dist}^3 (x_0,\partial\Omega)$, then $x_0$ is not a touchdown point.

(ii) $\quad $ For any $\omega\subset\subset \Omega$, if $\displaystyle\sup_{x\in \overline{\Omega}\setminus \omega} f(x) &amp;lt;
\gamma_0 {\hskip 1pt} \text{dist}^3(\omega,\partial\Omega)$, then the touchdown set 
 is contained in $\omega$.

Roughly speaking, the statement (i) in the above theorem allows one to rule out touchdown in an interior point by choosing a permittivity profile which is sufficiently small at that point, while statement (ii) allows to design devices producing touchdown inside any given subset of $\Omega$ by choosing a permittivity profile concentrated in that subset.

Numerical Simulation 1:  no touchdown at the origin 

In our first example, we consider a circular elastic plate of radius 1, that is $\Omega = B(0,1)\subset\mathbb{R}^2$.
It is well known that if the dielectric permittivity is constant, the unique touchdown point is the center of the disk.

As consequence of Theorem 1 (i), we can prevent touchdown to occur at the origin by choosing a permittivity profile $f$ such that $f(0)$ is sufficiently small.
Let’s assume that the applied voltage $\lambda$, together with the permittivity of the vacuum $\varepsilon_0$ and the permittivity of the material $\varepsilon_1$ are such that



In this case, we might cover the center of the disk with a material with higher permittivity so that the value of $\varepsilon_1(x)$ in $B(0,1/4)$ is (for example) twice its value in $\Omega\setminus B(0,1/4)$.
That is, we consider the following permittivity profile:



The following video represents the evolution of the elastic plate from the rest position to the pull-in instability.
For the numerical simulation we have applied an implicit Crank-Nicolson scheme in polar coordinates with number of meshpoint $N=100$ in the radial interval $[0,1]$.







 Simulation 1:   Evolution of the elastic plate until pull-in instability (touchdown). The solution reaches the value 1 in a circumference and then, touchdown is avoided at the center of the plate.
In the schematic video, we remove the upper plate of the MEMS device in order to see the final profile of the solution.  


Numerical Simulation 2:  touchdown near a prescribed circumference 

Let us suppose now that we want our system to touch down in a circumference of radius near $0.7$.
This time we can use Theorem 1 (ii). We will choose a permittivity profile $f$ sufficiently concentrated near a circumference of radius $0.7$. Consider for example:



The following video represents the evolution of the elastic plate from the rest position to the pull-in instability. 
In Figure 2 below, we plot the radial final touchdown profile.
For the numerical simulation we have applied an implicit Crank-Nicolson scheme in polar coordinates with number of meshpoint $N=100$ in the radial interval $[0,1]$.







 Simulation 2:   Evolution of the elastic plate until pull-in instability (touchdown). The solution reaches the value 1 in a circumference of radius near $0.7$.
In the schematic video, we remove the upper plate of the MEMS device in order to see the final profile of the solution.  





 Figure 2:   Plot of the  radial touchdown profile. Observe that, as expected, touchdown occurs on a circumference of radius between $0.65$ and $0.75$. 


Nontrivial touchdown sets

In view of Theorem 1,
it is a natural question whether such smallness conditions are actually necessary, 
or whether touchdown could be shown to occur only at or near the maximum points of the permittivity profile $f$.
In this connection, using stability properties of the touchdown set,
in [2] we construct radial ‘‘M’‘-shaped profiles giving rise to 
single-point touchdown at the origin.

Numerical simulation 3:

For this example, we have considered the following radial M-shaped profile in a disk of radius 1:



For this profile, the unique touchdown point is at the origin, far away from the maxima of $f$.
Therefore, if we want to prevent touchdown at the origin, we need to choose a permittivity profile $f$ such that $f(0)$ is small enough (below a certain thershold).
For the numerical simulation we have applied an implicit Crank-Nicolson scheme in polar coordinates with number of meshpoint $N=100$ in the radial interval $[0,1]$.







 Simulation 3:   Evolution of the elastic plate until pull-in instability (touchdown). Although we consider a ''M''-shaped permittivity profile, the solution touches down only at the origin, far away from the maximum of $f$.
In the schematic video, we remove the upper plate of the MEMS device in order to see the final profile of the solution.  


Numerical simulation 4:

Another rather surprising example is the following radially increasing profile:



For this example, the unique touchdown point is the origin, which is actually the global minimum of  $f$. In [2], we give a rigorous proof of the existence of this kind of behaviors. It is based on a stability result of the touchdown set combined with the fact that single-point touchdown occurs for constant permittivity profiles.

For the numerical simulation we have applied an implicit Crank-Nicolson scheme in polar coordinates with number of meshpoint $N=100$ in the radial interval $[0,1]$.







 Simulation 4:   Evolution of the elastic plate until pull-in instability (touchdown). Although the permittivity profile is radially increasing,  the unique touchdown point is the origin, the minimum point of $f$.
In the schematic video, we remove the upper plate of the MEMS device in order to see the final profile of the solution.  


Quantitative results in 1 dimension

Motivated by practical considerations of MEMS design, our aim in [3] is to further investigate the 
touchdown localization problem
and to show that in one space dimension, where analytic computations can be made more precise,
one can obtain quite quantitative conditions.
Namely, we look for a lower estimate of the ratio $\rho$ between $f$ and its maximum,
below which no touchdown occurs on a subregion of $\Omega$.
Rather surprisingly, it turns out that, under suitable assumptions on $f$,
our methods yield values of
 the threshold-ratio $\rho$ which are not ‘‘small’’ but can actually be up to the order



which could hence be quite appropriate for robust practical use.

In order to give good estimates of the  ratio $\rho$, we shall consider two typical situations, which roughly correspond to a one-bump or a two-bump shape for the profile $f$. 
 The touchdown is ruled out in a subinterval respectively located between a bump and 
an endpoint of $\Omega$, or between two bumps.
The idea behind this is that the plate can be covered with two dielectric materials, 
one with a high permittivity and the other with a lower permittivity. We then seek for a ratio between the two permittivities, allowing to rule out touchdown in the low permittivity region.

Reduction to a finite-dimensional optimization problem

As a consequence of our method, the threshold-ratio $\rho$  is rigorously obtained as the solution of a suitable finite-dimensional optimization problem, with either three or four parameters. 
An advantage of our results is that they apply to large classes of configurations, so that detailed numerics need not be carried out to localize the touchdown set for particular cases.

For the precise statements of the results, as well as for a detailed study of the optimization problem and numerical estimates of the threshold ratio, we refer the lector to [3].
The Matlab codes to compute numerical estimates of the solution of the optimization problem are availabe at the beginig of the page.

Here we give two concrete examples in order to illustrate 
the results.
The threshold ratio is obtained by a simple numerical procedure applied to the finite-dimensional optimization problem. 
The following two figures represent some typical permittivity profiles $f(x)$ and the localization of the corresponding touchdown sets
 in the one-bump and two-bump cases respectively. 
The touchdown sets are localized in a neighborhood of the bumps, represented by the fat lines.


 



 Figure 3: An illustration of the localization of thouchdown set for a one-bump permittivity profile.
Here, the figure represents the permittivity profile $f$ and  not  the profile of the solution.







 Figure 4: An illustration of the localization of thouchdown set for a two-bump permittivity profile.
Here, the figure represents the permittivity profile $f$ and  not  the profile of the solution.


Numerical simulation 5:

We give an example of a one-bump permittivity profile, similar to the one in Figure 3.
Although our result applies to a large class of profiles, here we consider, for computational simplicity, the following bang-bang profile:



The following video represents  the evolution of a 1-dimensional membrane
with a one-bump permittivity profile $f$. 
For the numerical simulation we have applied an implicit Crank-Nicolson scheme with number of meshpoint $N=1000$ in the interval $[-6,6]$.







 Simulation 5:   Evolution of the elastic 1-dimensional membrane until pull-in instability (touchdown). We see that touchdown is localized in the region where $f$ is big (see Figure 3).  


Numerical simulation 6:

We give an example of a two-bump permittivity profile, similar to the one in Figure 4.
Although our result applies to a large class of profiles, here we consider, for computational simplicity,
the following bang-bang profile:



The following video represents  the evolution of a 1-dimensional membrane
with a two-bump permittivity profile $f$.
For the numerical simulation we have applied an implicit Crank-Nicolson scheme with number of meshpoint $N=1000$ in the interval $[-10,10]$.







 Simulation 6:   Evolution of the elastic 1-dimensional membrane until pull-in instability (touchdown). We see that touchdown is localized in the region where $f$ is big (see Figure 4).  


References

[1] P. Esposito, N. Ghoussoub, Y. Guo,  Mathematical analysis of partial differential equations modeling electrostatic MEMS. Courant Lecture Notes in Mathematics, 20. Courant Institute of Mathematical Sciences, New York; American Mathematical Society, Providence, RI, 2010. xiv+318 pp.

[2] C. Esteve, Ph. Souplet,  No touchdown at points of small permittivity and nontrivial touchdown sets for the MEMS problem. Advances in Differential Equations, Vol 24, Number 7-8(2019), 465-500.

[3] C. Esteve, Ph. Souplet,  Quantitative touchdown localization for the MEMS problem with variable dielectric permittivity. Nonlinearity} 31 4883 (2018).

[4] Ph. Laurençot, Ch. Walker,  Some singular equations modeling MEMS. Bull. Amer. Math. Soc. 54 (2017), 437-479.

[5] J.A. Pelesko, D.H. Bernstein,  Modeling MEMS and NEMS, Chapman Hall and CRC Press, 2002.
</description>
        <pubDate>Mon, 30 Sep 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp99/P0040</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp99/P0040</guid>
        
        
        <category>tutorial</category>
        
        <category>WP99</category>
        
      </item>
    
      <item>
        <title>Uniform exponential and polynomial stability and approximation in control of a thermoelastic model</title>
        <description>The main objective of this blog is to explain numerically the difference between exponential and polynomial decay in term of energy and spectral properties. We Show also that system with polynomial decay are very  sensitive to the choice of initial data.

Model Problem

We consider the numerical approximation of the following coupled thermoelastic wave models





where $u(x,t)$ is the displacement (longitudinal or transverse, depending upon the application) at position $x$ along a bounded smooth domain $\Omega\subset\mathbb{R}$ and time $t$ , and $\theta(x,t)$ is the temperature deviation from the reference temperature at position $x$ and time $t$, $u_0(x)$, $v_0(x)$ and $\theta_0(x)$ are initial data in a suitable space. The small positive constant $\gamma$ is a thermo-mechanical coupling parameter and is generally small in comparison to 1. System (\ref{eq1}) differs from system $\eqref{eq2}$ at the coupling terms, where we have replaced the strong coupling ($\gamma\theta_x$ and $\gamma u_{tx}$) by a weak coupling ($\gamma\theta$ and $\gamma u_{t}$).

It is well known from literature that system (\ref{eq1}) and (\ref{eq2}) are respectively exponentially and polynomially stable, see [3,7] and [5,6,9].

In this post, we will show by numerical experiments, how the coupling terms affect quantitative and qualitative properties of thermoelastic systems (\ref{eq1}) and (\ref{eq2}). These results could be found in [8,10].

To do this, we consider a semi discretization version of both systems (\ref{eq1}) and (\ref{eq2}), obtained with finite element method, which has the following form



where for all $n\in\mathbb{N}$, $z_n=(u_n,v_n,\theta_n)^T$ is the semi discrete solution, $z_{n0}$ is the discretized initial data, $A_{i,n}$ the discretized dynamic and the subscript $\cdot_i$ refers to system (\ref{eq1}) and (\ref{eq2}) with



and $F_{2,n}=I_n$,



Spectral properties of thermoelastic systems

Figure 1 and Figure 2 show how the coupling terms affect the placement of eigenvalues of the dynamic $A_{n}$. In Figure 1, we see that a uniform distance between the eigenvalues and the imaginary axis is preserved, see Table 1. Another observation is that for fixed $n$, the eigenvalues of higher frequency modes, in particular, the one of the $n^{th}$ mode, are closer to the imaginary axis. Moreover, as the number of modes increases, these eigenvalues bend back towards the vertical line $\lambda=-\frac{\gamma^2}{2}$, a fact which has been already shown in [4]. Therefore, the corresponding spectral element approximation scheme preserves the property of exponential stability.

 Table 1. Distance between $\sigma(A_{1,n})$ and the imaginary axis form the spectral element method


    
        n
        $min \{ -Re(\Lambda),\Lambda \in \sigma(A_{1,n}) \}$
    
    
        8
        $8.9227x10^{-4}$
    
    
        16
        $8.9383x10^{-4}$
    
    
        24
        $8.9402x10^{-4}$
    
    
        32
        $8.9407x10^{-4}$
                


 Location of the complex eigenvalues of the matrix $A_{1,n}$ with the finite element method

error
Nx = 30;
stabexpsem(Nx);


Error using error
Not enough input arguments.

Error in tp1d0d1572_49d1_41a8_914f_f09a199a8ec3 (line 114)
error



In Figure 2, conversely to Figure 1 where a uniform distance between the eigenvalues and the imaginary axis is preserved, we observe that, as the number of modes increases, an asymptotic behaviour appears in the neighborhood of the imaginary axis at $\pm\infty$. This property is mainly related to systems with polynomial decay, see \cite{BEPS2006}.

 Location of the complex eigenvalues of the matrix $A_{2,n}$ with the finite

Nx = 30;
stabpolysem(Nx);


Uniform and polynomial decay of the energy

The discrete energy associated to system (\ref{eq3}) is given by



The discrete energy $E_{1,n}$ associated to system (ref{eq1}) decays exponentially to zero, see Figure 3, in the following sense: $\exists M,\alpha$ positive constants such that



However, the introduction of the weak coupling term in system (ref{eq1}) has changed the dynamic and consequently the behavior of energy (\ref{eq4}). In this case, we say that system (\ref{eq2}) decays polynomially to zero, see Figure 4, in the following sense: $\exists M,\alpha$ positive constants such that



 Exponential decay of $E_{1,n}(t)$

 Polynomial decay of $E_{2,n}(t)$

Effect of smoothness of the initial data on the rate of decay of energy

It has been shown theoretically, see [1,2], that the energy associated to system (\ref{eq2}) is very sensitive to the smoothness of its initial data. This fact, has been also observed numerically, see Figure 6. we use

Nx = 100;
FinalTime = 100;
dt = 0.5;
mode = 1;
Gamma = 0.2;
n = 100;
k= 2;


and we consider the following initial data

u0=zeros(1,n);
x = linspace(0,pi,100);
v0=sqrt(2/pi)*sin(k*x);
teta0=zeros(1,n);


Through Figure 6, we notice that for $j = 1$, the approximate energy $E_{2,n}(t)$ decays to zero as the time $t$ increases. Moreover, we observe that the decay rate depends strongly on $j$. That is, when $j$ increases, initial data are very oscillating. We say in this case that the rate of decay of the discrete energy $E_{2,n}(t)$ is very sensitive to the choice of the initial data. However, the behavior of the energy assosiated to system (\ref{eq1}) remains indifferent to the smoothness of initial data when $n\to\infty$, see Figure 5.

  Exponential decay of $E_{1,n}(t)$

  Polynomial decay of $E_{2,n}(t)$

References

[1] A. B' atkai, K.J. Engel, J. Pr&quot;uss and R. Schnaubelt, Polynomial stability of operator semigroups, Math. Nachr. 279, pp.

[2] A. Borichev and Y. Tomilov, Optimal polynomial decay of functions and operator semigroups, Math. Ann., 347(2), pp.455-478, 2010.

[3] S. W. Hansen, Exponential energy decay in a linear thermoelastic rod. J. Math. Anal. Appli.,167, pp. 429-442, 1992.

[4] F.A. Khodja, A. Benabdallah, and D. Teniou, Stability of coupled systems, Abstr. Appl. Anal. Volume 1, Number 3, 327-340, 1996.

[5] F. A. Khodja, A. Benabdallah and D. Teniou, Dynamical stabilizers and coupled systems}, ESAIM Proceeding,2, pp. 253-262, 1997.

[6] F. A. Khodja, A. Bader and A. Benabdallah, Dynamic stabilization of systems via decoupling techniques, ESAIM: COCV,4,

[7] Z. Liu and S. Zheng, Exponential stability of semigroup associated with thermoelastic system, Quart. Appl. Math, 51, pp.535-545, 1993.

[8] Z. Y. Liu and S. Zheng, Uniform exponential stability and approximation in control of a thermoelastic system, SIAM J. Control Optim. 32, pp. 1226-1246, 1994.

[9] Z. Liu and B. Rao, Characterization of polynomial decay rate for the solution of linear evolution equation. Zeitschrift  angewandte Mathematik und Physik ZAMP,56, pp. 630-644, 2005.

[10] L. Maniar and S. Nafiri, Approximation and uniform polynomial stability of C_0-semigroups,ESAIM: COCV 22, pp. 208-235, 2016.

</description>
        <pubDate>Thu, 18 Jul 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp99/P0030</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp99/P0030</guid>
        
        
        <category>tutorial</category>
        
        <category>WP99</category>
        
      </item>
    
      <item>
        <title>Controllability to steady states of a reaction diffusion system and emergence of barriers.</title>
        <description>asda
</description>
        <pubDate>Mon, 08 Jul 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp99/WP99-P0028</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp99/WP99-P0028</guid>
        
        
        <category>tutorial</category>
        
        <category>WP99</category>
        
      </item>
    
      <item>
        <title>Analysis, numerics and control for the obstacle problem</title>
        <description>In this tutorial, we present some aspects of the obstacle problem in the stationary (elliptic) and the evolutionary (parabolic) setting by means of numerical simulations, performed in FEniCS and Matlab.




 Figure 1. The solution (blue) of the one-dimensional elliptic obstacle problem is superharmonic (concave down) and matches derivatives with the obstacle (black). 


1. Mathematical formulation

1.1. Elliptic problem
Let $\Omega \subset \mathbb{R}^d$ be open, regular and bounded, let $\psi \in H^2(\Omega)$ (the obstacle) and $g \in H^1(\Omega)$ (boundary data) with $\psi \leq g$ on $\partial \Omega$ be given. We consider the functional



where $f \in L^2(\Omega)$ is given, and the closed convex set



The classical obstacle problem can be written as



and admits a unique minimizer $\phi \in \mathcal{K}^g_{\text{OB}}$, which is also a solution of the variational inequality



The terminology “obstacle problem” comes from the physical origin of the model $-$ the solution $\phi$ represents the equilibrium position of an elastic membrane, which is constrained to lie above an obstacle (given by the graph of the function $\psi$).

Notation:  The notation



designates the Gateaux derivative ($L^2$-gradient) of the functional ${F}_{\text{OB}}$ at the point $\phi$ in the direction $\zeta$; in occurence,



Most of the regularity theory for the obstacle problem has been developed for the model case $\Delta \psi=-1$. We refer to [3,4] for a detailed presentation. 
For simplicity of the presentation, let us thus assume for a moment that $f \equiv -1$ and $\psi \equiv 0$. Choosing appropriate test functions bring us from the variational inequality for $\phi$ to



The set ${ \phi=0 }$ is called contact set, while $\partial { \phi&amp;gt;0 }$ is the free boundary.



 Figure 2. The solution $\phi$ to the elliptic obstacle problem just above. The obstacle is equal to 0 (gray plane), and the free boundary (boundary of the contact set between the membrane and the obstacle) is displayed in red, projected just below. 


Remark: At this point, we may see why solving the elliptic obstacle problem is different to just solving the Poisson equation for $\phi$ in a subdomain of $\Omega$ and setting $\phi=\psi$ elsewhere. Indeed, solving the latter problem does not guarantee that we would have



along the boundary.

1.2. Parabolic problem

The parabolic counterpart onsists in finding $u \in H^1(]0, +\infty[; L^2(\Omega))\cap L^2(]0,+\infty[; H^2(\Omega) \cap \mathcal{K}^g_{\text{OB}})$ satisfying



The name “parabolic obstacle problem” for the above system may be misleading. 
While the mathematical formulation is similar, the physical interpretation of the elliptic and parabolic problem is different. In the parabolic setting, seeing $\psi$ as a physical obstacle in space is not correct. Rather, one should interpret $\psi$ as a barrier for the temperature $u$. More accurately, we are dealing with a parabolic variational inequality with an obstacle-type constraint.

The function $\psi$ may be time-dependent, but for simplicity we omit this case.











 Figure 3. Here $\psi(x)=3*\sqrt{(1-x^2)}$ in $(-1,1)$ (black) is a barrier for the evolving solution (blue). The initial data coincides with $\psi$ on (-1.5, 1.5). We observe that the contact set $\{ u = \psi \}$ changes (gets smaller) as time grows. There is also matching of normal derivatives on the contact points (i.e. the free boundary). 
Finally, the solution of the parabolic problem (left, blue) converges to that of the elliptic problem (right, red) for large time, as seen below.   











Figure 4. The solution to the parabolic problem with Dirichlet boundary condition =1. The obstacle is given by the &quot;three towers&quot;. The free boundary (boundary of the contact set with the obstacle) is projected on a plane below in red. It is seen that the source term $-1$ acts as a gravity force pushing the membrane towards the obstacle. We moreover observe a convergence to the equilibrium position (see also the section below). 

For $\psi \equiv 0$ and $f\equiv-1$ (or more generally $f + \Delta \psi = -1$), a manifestation of the parabolic variational inequality above is as a “weak form” of the one-phase Stefan problem (see [3,4]). In this setting, we may pass from the variational inequality to the equivalent problem with two boundary conditions at the free boundary



This is an overdetermined problem, which means that the free boundary/the contact set ${u=0}$ has to change with time to ensure that both BC hold for every time. The evolution of the contact set may be observed in the numerics of Figures 3,4,5.

Remark: $\quad$
As for the elliptic problem, solving the above parabolic problem is different to just solving the heat equation for $u$ in a subdomain of $\Omega$ and setting $u$ elsewhere, with a Dirichlet boundary condition. Indeed, solving the latter problem does not guarantee that we would have $|\nabla u|= 0$ along the boundary.

Remark: $\quad$
While in Figure 3 the contact set is shrinking does not disappear, it may happen that the contact set vanishes (depending on the magnitude of the boundary data $g$, the support of the initial data $u_0$, and if present, the source term $f$).

1.3. Relationship between both problems

In the recent paper [5], it is shown that the solution $u(t)$ to the parabolic variational inequality converges to the solution $\phi$ of the elliptic probelem in $H^1(\Omega)$ norm as $t\rightarrow \infty$ with exponential rate. This is done by means of new techniques including a so-called constrained Lojaseiwicz inequality.

We may observe this convergence numerically in Figures 4 and 5. In both cases, we work in the ball $B_2$, with boundary data $g\equiv 0.8$, barrier $\psi\equiv 0$ and 
initial datum supported outside $B_{1.5}$.

The one-dimensional case:










Figure 5. The solution of the parabolic problem (left, blue) converges to that of the elliptic problem (right, red) as time increases. We again see that the contact set $\{ u = 0\}$ changes with time. 

The two-dimensional case:












Figure 6. We see that as time increases, the contact region $\{ u = 0\}$ (white patch) gets smaller. In terms of the Stefan problem, the white patch represents the ice temperature. Ice is melting as time increases, but does not fully melt, because the support of the initial data is too small and the temperature on the boundary is not high enough.  










Figure 7. In this figure, the obstacle is set to zero, and we consider Dirichlet boundary conditions $=0.8$ on the fixed boundary. We see that even when considering initial data which are not an equilibrum membrane position (but do satisfy the obstacle condition), as time increases, the solution of the parabolic problem (left) converges to that of the elliptic problem (right)  

2. Numerical implementation

2.1. Possible strategies

A common approach for proving the well-posedness and $W^{2,p}$ regularity for variational inequalities is a technique called penalization, which consists of approximating by a sequence of semilinear equations [8]. For any $\epsilon&amp;gt;0$, we will consider



to approximate the parabolic problem, and remove $u’$ and time dependence for the elliptic one. As $\epsilon\rightarrow0$, the solution $u_\epsilon$ of the above equation converges to the variational inequality.  Here



may be replaced by another penalty function, for instance $\beta_\epsilon(u) = -\exp(-u/\epsilon)$.

The approximated problem is relatively simple to implement numerically. We may use finite elements (say $P1$ elements) to discretize in space, and time-stepping (Euler implicit for instance) to discretize in time.

There are other ways to solve the variational inequality after discretizing in space and time (using one’s favorite metood). One which is popular in the literature for the elliptic problem is the primal dual active set method, which we will present in a future post.

2.2. Code

i). Elliptic problem.

Let us start by solving the two-dimensional elliptic problem from Figure 4.

The fenics module [1] contains all of the necessary finite element tools for the space discretization of PDEs.

from fenics import *
from obstacles import dome
from mshr import mesh

It is advantageous to regularize the “kink” appearing in penalty function (the negative part of a function) in view of using a Newton method for solving the nonlinear problem

def smoothmax(r, eps=1e-4): 
	return conditional(gt(r, eps), r-eps/2, conditional(lt(r, 0), 0, r**2/(2*eps)))

We mesh the domain $B_2$.

mesh = generate_mesh(Circle(Point(0, 0), 2), 25)


$P1$ elements are used for the FEM spaces (but one may easily choose higher order in the definition of V).

V = FunctionSpace(mesh, &quot;CG&quot;, 1)
w = Function(V)
v = TestFunction(V)
psi = Constant(0.)
f = Constant(-1)		
eps = Constant(pow(10, -8))


Using the symbolic expressions of FEniCS, we define the variational formulation of the penalized PDE, which we write in the form $F(w)=0$:

bc = DirichletBC(V, 0.8, &quot;on_boundary&quot;)
F = dot(grad(w), grad(v))*dx - 1/eps*inner(smoothmax(-w+psi), v)*dx - f*v*dx


We solve the nonliner problem with the command solve, which makes use of a Newton method to solve the nonlinear equation $F(w)=0$.

solve(F == 0, w, bcs=bc)


We visualise in ParaView by storing the solution in .vtk format using:
vtkel = File(&quot;output/el.pvd&quot;)
vtkel &amp;lt;&amp;lt; w


ii). Parabolic problem.

We set $T=2$, use $100$ subdivisions and set

dt = T/100


This part only differs by the presence of a time-loop. We define the initial datum in a separate class:

class indata(Expression):
	def eval(self, value, x):
		if sqrt(x[0]*x[0]+x[1]*x[1]) &amp;lt;= 1.95:
			value[0] = 0
		elif 1.95 &amp;lt;= sqrt(x[0]*x[0] + x[1]*x[1]) &amp;lt;= 2:
			value[0] = (sqrt(x[0]*x[0]+x[1]*x[1])-1.95)/(2-1.95)*0.8
		else:
			value[0] = 0.8


Repeating as in the elliptic case up to defining the variational form, we now set the inital datum (interpolating the expression onto the FEM space) and time:

un = interpolate(indata(degree=2), V)
t = 0

We set up the time loop:

vtksol = File(&quot;output/popsol.pvd&quot;)
for n in range(num_steps):
    t+=dt
    solve(F==0, u, bcs=bc)
    vtksol &amp;lt;&amp;lt; (u, t)
    un.assign(u) 


Optimal Control

We will now briefly present the implementation of an optimal control strategy for the elliptic and parabolic problems. We will make use of the penalized problems where $\epsilon&amp;gt;0$ is small (of the order $10^{-8}$). This strategy has been succesfully applied in [8].

3.1. Elliptic problem

Given a target $\phi_d$ and a regularization parameter $\delta&amp;gt;0$ (we usually use $\delta = 10^{-2}$), we seek to minimize



subject to



For simplicity of the presentation, we will consider







on the domain $\Omega = [-2,2]^2$. The target is the function $\phi_d(x) = \frac12(\cos(x_1)+\cos(x_2))$.

The uncontrolled solution of the elliptic obstacle problem in this case is:



	

	
	

	



Figure 8. For comparison with the controlled problem, we show also the solution to the free elliptic problem with the &quot;dome&quot; like obstacle given on the right. 

The FEniCS optimization code will have the effect of obtaining the following results.



	

	
	
	
	















Figure 9. The optimal state (top left) can be seen to be above the obstacle (top right), and is very much alike the shape of the target (bottom left). Finally, the optimal control is given (bottom right). 

Let us present the numerical implementation. After importing the dolfin_adjoint optimization module [2], we make use of the code for simulating the uncontrolled problem. A “tape” of the forward model is built. This tape is used to drive the optimization by repeatedly solving the forward model and the adjoint model for varying control inputs.

Defining the objective functional to be minimized:

delta = Constant(1e-2)
j = assemble(0.5*inner(w-wd, w-wd)*dx + delta/2*inner(u, u)*dx)	
m = Control(u)


Writing the state $y$ as the output of the solution map corresponding to the input $m$, one sees that the objective functional depends only on the control:

J = ReducedFunctional(j, m)									


We run the optimization, based on a conjugate-gradient method:

u_opt = minimize(J, options={&quot;method&quot;=&quot;CG&quot;})


The tape is modified such that the initial guess for y (to be used in the Newton solver in the forward problem) is set to y_opt.

y_opt = y.block_variable.saved_output
Control(y).update(y_opt)


The algorithm converged after 8 iterations, and the value of the functional at the final iteration is $0.0967$.

3.2. The parabolic problem

Given a final time $T&amp;gt;0$, a target $u_d$ and a regularization parameter $\delta&amp;gt;0$, we seek to minimize



subject to



For simplicity of the presentation, we will consider the same setting as in Figure 3, namely $f\equiv-1\,$,  $g\equiv0.8\,$, on the domain $\Omega = [-2,2]$. The target temperature is $u_d(x) = 0.8$. This would correspond to “melting”, as this target is never equal to zero, thus has no contact set. We would thus expect the action of the control $v$ to lift-up $u$ from $0$ and thus the contact set (and free boundary) to vanish.

We appeal to DyCon-Toolbox [9] for the numerical implementation of this problem. DyCon-Toolbox is an efficient and easy to use solver for nonlinear control problems.



Figure 10. The optimal state (left, red), starting from an initial datum with a non-empty contact set (left, blue) and the optimal control (red, right). 


The action and magnitude of the control ensures that the solution to the parabolic problem will lift-off from the initial contact set. 
For comparison, we recall that the uncontroled free dynamics have a very different behavior:





Figure 11. The uncontrolled free dynamics of the parabolic obstacle problem in 1 dimension, where the obstacle is zero. 

We begin by symbolically defining the time variable:
syms t


We discretize the interval $[-2, 2]$:
N = 70;
xi = -2; xf = 2;
xline = linspace(xi,xf,N+2);
xline = xline(2:end-1);

We define symbolically (as in FEniCS) the state vector $Y$ and control vector $U$:
Y = SymsVector('y',N);
U = SymsVector('u',N);

We discretize by finite differences, and define the semilinear penalty term:
alpha = 1e-3;
epsilon = 1e-2;
A = FDLaplacian(xline);

F  = @(Y) NonLinearTerm(Y,alpha);
dF = @(Y) DiffNonLinearTerm(Y,alpha);


We may define the parabolic problem to be solved.
dx = xline(2) - xline(1);
Y_t = @(t,Y,U,Params) A*Y + 1/epsilon*F(-Y) + U + (1/dx^2)*[0.8;zeros(N-2,1);0.8];
Dyn = pde(Y_t,Y,U);

We set the mesh, and other relevant parameters such as the final time and initial data. We consider the same data as in the free problem.

Dyn.mesh   = xline;
Dyn.Solver = @ode23;
Dyn.Nt     = 150;
Dyn.FinalTime        = 0.3;
Dyn.InitialCondition = InitialConditionFcn(xline);


We now compute the necessary Jacobian matrices.

Dyn.Derivatives.Control.Num = @(t,Y,U,Params) eye(N);
Dyn.Derivatives.State.Num   = @(t,Y,U,Params) A + 1/epsilon*dF(-Y);


We may solve the free problem over the given time interval. We recall that we have used finite differences to discretize in space (but finite elements can be used as well in more complicated settings), and the underlying solve command solves the system of ODEs by means of some Runge-Kutta scheme.

[tspan,Ysolution] = solve(Dyn);
figure
surf(Ysolution)

Having solved the free dynamics, we are now in a position to solve the optimal control problem:

YT = 0.8 + 0*xline';
beta = dx^4;
Psi  = @(T,Y) dx*(YT - Y).'*(YT - Y);
L    = @(t,Y,U)  dx*(YT - Y).'*(YT - Y) + ...
                 beta*dx*0.5*alpha*(U.'*U);
OCP = Pontryagin(Dyn,Psi,L);
U0 = -ones(Dyn.Nt,Dyn.ControlDimension);
U0 = GradientMethod(OCP,U0,'Graphs',true,'EachIter',2,'DescentAlgorithm',@AdaptativeDescent)


References:

[1] The FEniCS Project Version 1.5
M. S. Alnaes, J. Blechta, J. Hake, A. Johansson, B. Kehlet, A. Logg, C. Richardson, J. Ring, M. E. Rognes and G. N. Wells
Archive of Numerical Software, vol. 3, 2015.

[2] Patrick E. Farrell, David A. Ham, Simon W. Funke and Marie E. Rognes (2013). Automated derivation of the adjoint of high-level transient finite element programs, SIAM Journal on Scientific Computing 35.4, pp. C369-C393. doi:10.1137/120873558. arXiv:1204.5577

[3] Figalli A. (2018), Free boundary regularity in obstacle problems 
Journées EDP 2018, to appear.

[4] Figalli, A. (2018), Regularity of interfaces in phase transitions via obstacle problems 
Proceedings ICM 2018, to appear.

[5] Colombo M. and Spolaor, L. and Velichkov, B. (2018), On the asymptotic behavior of the solutions to parabolic variational inequalities, ArXiV preprint.

[6] Borjan Geshkovski (2018). Obstacle Problems: Theory and Applications. Master Thesis.

[7] Hintermüller M. and Kopacka I., A smooth penalty approach and a nonlinear multigrid algorithm for elliptic MPECs. Computational Optimization and Applications, 50(1):111–145, 2011.

[8] D. Kinderlehrer and G. Stampacchia. An introduction to variational inequalities and their applications. Volume 31 of Classics in Applied Mathematics. SIAM, 2000.

[9] DyCon-Toolbox, https://deustotech.github.io/dycon-toolbox-documentation/.

</description>
        <pubDate>Mon, 08 Jul 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp03/WP03-P0031</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp03/WP03-P0031</guid>
        
        
        <category>tutorial</category>
        
        <category>WP03</category>
        
      </item>
    
      <item>
        <title>Rotors imbalance suppression by optimal control</title>
        <description>Consider a rotor rotating around a fixed axis. Because of wear and damage, the mass distribution is not homogeneous. This leads to dangerous vibrations in the rotation. A prototypical example can be a wind turbine, affected by misalignment of the blades and/or mass imbalance of the hub and blades [2].

In order to compensate the imbalance, two balancing heads are mounted at the endpoints of the axle, as in figure 1. Each balancing head is made of two masses free to rotate.

Our goal is to determine the optimal movement of the balancing masses to minimize the vibrations. Control theoretical techniques are employed. For further details, see [6].

 Figure 1: representation of the rotor and the balancing. The balancing heads are located at the endpoints of the spindle. The four balancing masses (two for each balancing head) are drawn in red. 

Consider a rotor-fixed reference frame $(O;(x,y,z))$. Figures 1 and 2 show a front view of the device and a scheme of the balancing heads. 

Figure 2: front view of the system made of rotor and balancing device. 

Figure 3: scheme of one balancing head. The balancing masses $(m_i,P_{i,1})$ and $(m_i,P_{i,2})$ are drawn in red. The bisector of the angle generated by $\overset{\longrightarrow}{OP_{i,1}}$ and $\overset{\longrightarrow}{OP_{i,2}}$ is the dashed line. The *intermediate* angle $\alpha_i$ is represented in 3(A), while the *gap* angle $\gamma_i$ is depicted in 3(B). The angles $\alpha_i$ and $\gamma_i$ give the position of the balancing masses in each balancing head. 

We consider two planes



orthogonal to the rotation axis $z$. The balancing device (see figures 1 and 2) is made of two heads lying in each of these planes.

The heads are fixed to the rotor and rotate with it. In particular, $\alpha_i$ and $\gamma_i$ are defined with respect to the rotor-fixed reference frame $(O;(x,y,z))$.

Each head is made of a pair of balancing masses, which are free to rotate orthogonally to the rotation axis $z$.

Namely, we have

  two mass-points $(m_1,P_{1,1})$ and $(m_1,P_{1,2})$ lying on $\pi_1$ at distance $r_1$ from the axis $z$, i.e., in the reference frame $(O;(x,y,z))$





  two mass-points $(m_2,P_{2,1})$ and $(m_2,P_{2,2})$ lying on $\pi_2$ at distance $r_2$ from the axis $z$, namely, in the reference frame $(O;(x,y,z))$




For any $i=1,2$, let $b_i$ be the bisector of the angle generated by $\overset{\longrightarrow}{OP_{i,1}}$ and $\overset{\longrightarrow}{OP_{i,2}}$ (see figure 3). The intermediate angle $\alpha_i$ is the angle between the $x$-axis and the bisector $b_i$, while the gap angle $\gamma_i$ is the angle between $\overset{\longrightarrow}{OP_{i,1}}$ and the bisector $b_i$.

The imbalance is modelled by a resulting force $F$ and a momentum $N$ orthogonal to the rotation axis. In the rotor-fixed reference frame $(O;(x,y,z))$, set $P_1 :=  (0,0,-a)$, $P_2 :=  (0,0,b)$, $F :=  (F_x,F_y,0)$ and $N :=  (N_x,N_y,0)$. By imposing the equilibrium condition on forces and momenta, the force $F$ and the momentum $N$ can be decomposed into a force $F_1$ exerted at $P_1$ contained in plane $\pi_1$ and a force $F_2$ exerted at $P_2$ contained in $\pi_2$.

In each plane, we generate a force to balance the system, by moving the balancing masses:

  in the plane $\pi_1$, we compensate $F_1$ by the centrifugal force:





  in the plane $\pi_2$, we compensate $F_2$ by the centrifugal force:




The overall imbalance of the system is then given by the resulting forces in $\pi_1$ and $\pi_2$,



and



respectively.

The overall imbalance on the system made of rotor and balancing device is measured by the imbalance indicator



Our task is to find a control strategy such that

  the balancing masses move from their initial configuration $\Phi_0$ to a final configuration $\overline{\Phi}$, where the imbalance is compensated;
  the imbalance and velocities should be kept small during the correction process.


We address the minimization problem



where $Q(\Phi) :=  G(\Phi)-\inf G$ and



with .

The theoretical analysis of the above problem is in [6]. The existence of the optimum is proved. The stabilization of the optimal trajectories towards steady optima is proved in any condition.

Simulation

In order to perform some numerical simulations, we firstly discretize our cost functional and then we run AMPL-IPOpt to minimize the resulting discretized functional.

For the purpose of the numerical simulations, it is convenient to rewrite the cost functional as



subject to the state equation



Discretization
Choose $T$ sufficiently large and $Nt \in \mathbb{N} \setminus {0,1}$. Set



The discretized state is  , whereas the discretized control (velocity) is $(\psi_{i})_{i=0, … ,Nt-2}$. The discretized functional then reads as



subject to the state equation



Execution

The discretized minimization problem is



We address the above minimization problem by employing the interior-point optimization routine IPOpt (see [3,4]) coupled with AMPL [1], which serves as modelling language and performs the automatic differentiation. The interested reader is referred to [8, Chapter 9] and [7] for a survey on existing numerical methods to solve an optimal control problem.

In figures 4, 5, 6 and 7, we plot the computed optimal trajectory for \eqref{functional}, with initial datum $\Phi_0=\left(\alpha_{0,1},\gamma_{0,1};\alpha_{0,2},\gamma_{0,2}\right) :=  \left(2.6,0.6, 2.5,1.5\right)$. We choose $F$, $N$ and $m_i$. The exponential stabilization proved in [6] emerges. 

Figure 4: intermediate angle $\alpha_1$ versus time. 

Figure 5: gap angle $\gamma_1$ versus time. 

 Figure 6: intermediate angle $\alpha_2$ versus time. 
In figure 8, we depict the imbalance indicator versus time along the computed trajectories. As expected, it decays to zero exponentially.

Figure 7: gap angle $\gamma_2$ versus time. 

 Figure 8: system response. 



 Figure 9: We represent the evolution in time of the rotor vibrations.
In the uncontrolled case, the balancing device does not act. In the controlled case, our balancing strategy suppress the vibrations. 

References:
[1] Robert Fourer, David M Gay, and Brian W Kernighan. A modeling language for mathematical programming. Management Science, 36(5):519{554, 1990.

[2] Mike Jeffrey, Michael Melsheimer, and Jan Liersch. Method and system for determining an imbalance of a wind turbine rotor, September 11 2012. US Patent 8,261,599.

[3] Andreas Waechter, Carl Laird, F Margot, and Y Kawajir. Introduction to ipopt: A tutorial for downloading, installing, and using ipopt. Revision, 2009.

[4] Andreas Waechter and Lorenz T Biegler. On the implementation of an interior-point lterline-search algorithm for large-scale nonlinear programming. Mathematical programming, 106(1):25{57, 2006.

[6] Matteo Gnuffi, Dario Pighin and Noboru Sakamoto. Rotors imbalance suppression by optimal control. Preprint.

[7] Noboru Sakamoto and Arjan J van der Schaft. Analytical approximation methods for the stabilizing solution of the Hamilton-Jacobi equation. IEEE Transactions on Automatic Control, 53(10):2335{2350, 2008.

[8] Emmanuel Trélat. Contrôle optimal : théorie et applications
Mathématiques Concrètes. Vuibert, Paris, 2005. available
online:
https://www.ljll.math.upmc.fr/trelat/chiers/livreopt2.pdf.
</description>
        <pubDate>Mon, 08 Jul 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp02/P0005</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp02/P0005</guid>
        
        
        <category>tutorial</category>
        
        <category>WP02</category>
        
      </item>
    
      <item>
        <title>Propagation of one and two-dimensional discrete waves under finite difference approximation</title>
        <description>In [1], we discuss several aspects of wave propagation in a computational
framework. In particular, we consider the following one-dimensional wave equation



and we discuss the propagation properties of its finite-difference solutions on uniform and non-uniform grids, by establishing comparisons with the usual behavior of the continuos waves.

Our approach is based on the study of the propagation of high-frequency Gaussian beam solutions (that is, solutions originated from highly concentrated and oscillating initial data), both in continuous and
discrete media.

Roughly speaking, the idea at the basis of this techniques is that the energy of Gaussian beam solutions propagates along bi-characteristic rays, which are obtained from the Hamiltonian system associated to the
symbol of the operator under consideration.

At the continuous level of equation \eqref{main_eq_1d}, these mentioned rays are straight lines and travel with a uniform velocity.

On the other hand, the finite difference space semi-discretization of \eqref{main_eq_1d} may introduce different dynamics, with a series of unexpected propagation properties at high frequencies. For instance, one can generate spurious solutions traveling at arbitrarily small velocities which, therefore, show lack of propagation in space.

In addition, the introduction of a non-uniform mesh for the discretization may generate further pathologies such as internal
reflections, meaning that the waves change direction without hitting the boundary.

In order to illustrate these mentioned pathological phenomena, we perform simulations on a uniform space-mesh of $N$ points and mesh-size $h=2/(N+1)$



and on two non-uniform ones produced by applying to $\mathcal G_h$ the transformations



Moreover, the initial data are constructed starting from the following Gaussian profile



Low frequency simulations

We present in Figure 1 our simulations for low-frequency solutions of \eqref{main_eq_1d}. In particular, we considered initial position and frequency $(x_0,\xi_0)=(0,\pi/4)$ and a time horizon $T=5s$.

In this case, the numerical solutions behave basically like the continuous ones: they start traveling to the left along the straight characteristic line $x+t$ and, after having hit the boundary, they reflect following the Descartes-Snell’s law and continue propagating, this time to the right along the other branch of the characteristic ($x−t$).


Figure 1. Numerical solutions of \eqref{main_eq_1d} with $(x_0,\xi_0)=(0,\pi/4)$ on uniform and non-uniform meshes.

High frequency simulations

When increasing the frequency, the situation changes and we encounter several interesting phenomena and pathologies:

• The so-called umklapp or U-process, also known as internal reflection, consisting in the reflection of waves without touching only one or both the endpoints of the space interval. This phenomenon is typical for the semi-discretization of high-frequecncy solutions of \eqref{main_eq_1d} on non-uniform meshes, which may produce waves oscillating in the interior of the computational domain and reflecting without touching the boundary (see Figure 2 - middle) or touching the boundary only at one of the endpoints (see Figure 2 - right).


Figure 2. Numerical solutions of \eqref{main_eq_1d} with $(x_0,\xi_0)=(1/2,\pi)$ on uniform and non-uniform meshes.

• Non-propagating waves, corresponding to equilibrium (fixed) points on the phase diagram (see Figure 3).


Figure 3. Numerical solutions of \eqref{main_eq_1d} with $(x_0,\xi_0)=(0,\pi)$ on uniform and non-uniform meshes.

These phenomena are related with the particular nature of the discrete group velocity which, in the finite difference setting, is given by



and vanishes for $\xi = (2k+1)\pi$, $k\in\mathbb{Z}$. Moreover, they can be understood by looking at the phase portrait of the hamiltonian system associated to the finite difference semi-discretization of \eqref{main_eq_1d} (see Figure 4).


Figure 4. Phase portrait of the Hamiltonian system for the numerical wave equation corresponding to the and the grid transformation $g_1$ (left) and $g_2$ (right).

In particular, the solutions displayed in Figure 2 correspond to
trajectories which remain always in the red area of these phase portraits, while Figure 3 shows solutions starting from the equilibrium point $(0,\pi)$ (the green one in Figure 4)

Notice that, for the grid transformation $g_1$, this equilibrium point is a center (stable) while it is a saddle (unstable) for the grid transformation $g_2$. For this reason, in the first case the non-propagating wave remains concentrated along the vertical ray, while in the second case the wave presents more very dispersive features.

Two-dimensional simulations

Analogous phenomena can be detected also in the case of the finite-difference semi-discretization of the two-dimensional wave equation



Also in this case, we perform simulations on  a uniform mesh of $N$ points both in the $x$ and $y$ direction and mesh-sizes $h_x=2/(N+1)=h_y$



and on two non-uniform ones produced by applying to $\mathcal G_h$ the transformations $g_1$ and $g_2$ above introduced.

Moreover, the initial data are constructed once again starting from a Gaussian profile:



Then, it can be observed in Video 1 that, as for the one-dimensional case, at low frequencies the solution remains concentrated and propagates along straight characteristics which reach the boundary, where there is reflection according to the Descartes-Snell’s law. This independently on whether we use a uniform or a non-uniform mesh.







Video 1. Numerical solutions of \eqref{main_eq_2d} with $(x_0,y_0,\xi_0,\eta_0)=(0,0,\pi/4,\pi/4)$ on uniform and non-uniform meshes.

Nevertheless, increasing the frequencies similar phenomena as in the one-dimensional case show up. For instance, on the non-uniform mesh corresponding to the transformation $g_1$, we observe the so-called
rodeo effect, according to which, waves that should propagate along straight lines are trapped along closed circles (Video 2 - middle).




Video 2. Numerical solutions of \eqref{main_eq_2d} with $(x_0,y_0,\xi_0,\eta_0)=(0,\tan(\arccos(\sqrt[4]{1/2},\pi/2,\pi)$ on uniform and non-uniform meshes.

Finally, waves starting from the point $(x_0,y_0,\xi_0,\eta_0)=(0,0,\pi,\pi)$, which is an equilibrium for the phase Hamiltonian system, cannot move, and remain trapped around the point $(0,0)$ in the physical plane for any time (see Video 3).




Video 3. Numerical solutions of \eqref{main_eq_2d} with $(x_0,y_0,\xi_0,\eta_0)=(0,0,\pi,\pi)$ on uniform and non-uniform meshes.

Conclusions

Summarizing, our analysis shows that the finite-difference semi-discretization of one and two-dimensional waves may modify the dynamics of the continuous model. In particular, as a result of the accumulation of the local effects introduced by the heterogeneity of the employed grid, numerical high-frequency solutions can bend in a singular and unexpected manner. Moreover, this phenomenon has to be added to the well known numerical dispersion effect, producing the high-frequency discrete group velocity to vanish, even in uniform grids.
Our results constitute a warning both for adaptivity and for the treating of control and inverse problems. In broad terms, the goal of adaptivity is to refine a mesh on the support of the solution, keeping it coarse where the solution has little oscillations and energy. Our analysis shows that, in this context, adaptivity has to be performed with some attention. Indeed, if one is not careful enough when refining the mesh, they can be produced spurious effects due to the fact that waves feel the fictitious numerical boundaries that are generated when the grid passes from fine to coarse. Finally, our results are also a signal that the dangers of uniform meshes in the study of numerical control and inverse problems (already observed in [2]) may be enhanced when the mesh is non-uniform. In more details, the heterogeneity of the grid may introduce added trapping effects, which need to be avoided in
order to prove convergence in the context of controllability, stabilization or inversion algorithms.

References

[1] U. Biccari, A. Marica and E. Zuazua, Propagation of one and two-dimensional discrete waves under finite difference approximation. Submitted

[2] E. Zuazua, Propagation, observation, control and numerical approximation of waves. SIAM Rev. 47, 2 (2005), 197-243.

</description>
        <pubDate>Wed, 26 Jun 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp99/WP99-P0023</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp99/WP99-P0023</guid>
        
        
        <category>tutorial</category>
        
        <category>WP99</category>
        
      </item>
    
  </channel>
</rss>
