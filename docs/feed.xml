<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DyCon Blog</title>
    <description>Welcome to the web interface of DyCon Toolbox, the computational platform developed within the &lt;a href='https://cmc.deusto.eus/dycon/' target='_blank'&gt;ERC DyCon - Dynamic Control&lt;/a&gt; project.</description>
    <link>https://deustotech.github.io/DyCon-Blog/</link>
    <atom:link href="https://deustotech.github.io/DyCon-Blog/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 01 Oct 2019 14:51:58 +0200</pubDate>
    <lastBuildDate>Tue, 01 Oct 2019 14:51:58 +0200</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>System Identification of PDE equations from data</title>
        <description>En esta entrada veremos como podremos estimar  el sistema dinámico apartir de datos sobre la evolución de este sistema.

Lo primero que haremos es inspeccionar de que datos disponemos
&amp;gt;&amp;gt; whos

  Name               Size                  Bytes  Class     Attributes

  solution         100x200x50            8000000  double              
  tspan              1x200                  1600  double    

En este caso tenemos 100 simulaciones de una ecuación diferencial
</description>
        <pubDate>Tue, 01 Oct 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/ot02/P0001</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/ot02/P0001</guid>
        
        
        <category>tutorial</category>
        
        <category>OT02</category>
        
      </item>
    
      <item>
        <title>Touchdown localization for the MEMS problem with variable dielectric permittivity</title>
        <description>Background and motivation

An idealized version of a MEMS device consists of two conducting plates connected to an electric circuit
(see Figure 1).
The upper plate is rigid and fixed while the lower one is elastic and fixed only at the boundary.
Initially the plates are parallel and at unit distance from each other.
When a voltage (difference of potential between the two plates) is applied, the lower
plate starts to bend and, if the voltage is large enough,
the lower plate eventually touches the upper one, conducting to the so-called  pull-in instability. In the mathematical framework, this phenomenon is known as quenching or  touchdown.
Such device can be used for instance as an actuator, a microvalve (the touching-down part closes the valve),
or a fuse.


  



 Figure 1:  Two-membrane MEMS device


We consider a well-known model for micro-electromechanical systems (MEMS)
with variable dielectric permittivity, based on the following parabolic equation with singular nonlinearity:



where $\Omega$ is a smoothly bounded domain of $\mathbb{R}^2$ and 
 is a Hölder continuous function in .

In this mathematical model, the domain $\Omega$ represents the shape of the elastic plate in the horizontal direction, the solution $u=u(t,x)$ measures its vertical deflection,
while the function $f(x)$ is proportional to the constant voltage and characterizes the varying dielectric permittivity of the elastic plate.
We can write the permittivity profile $f$ as follows:



where $\lambda&amp;gt;0$ is the applied voltage, $\varepsilon_0&amp;gt;0$ is the permittivity of the vacuum and $\varepsilon_1(x)$ is the dielectric permittivity of the material. 
As a key feature, in our model the permittivity of the elastic plate may be inhomogeneous,
and this can be used to trigger the properties of the device. 
We refer to [1],[4],[5] and the references therein for the full details of the model derivation.

It is well known that problem (\ref{quenching problem}) admits a unique maximal classical solution $u$.
We denote its maximal existence time by $T=T_f\in (0,\infty]$. Moreover,
under some largeness assumption on $f$, it is known that the maximum of $u$ reaches the value $1$
at a finite time, so that $u$ ceases to exist in the classical sense. This is what we call  touchdown in finite time $T_f&amp;lt;\infty$.
Actually, if we consider $f(x) = \lambda\dfrac{\varepsilon_0}{\varepsilon_1(x)}$,
it is well-known that there exists a positive $\lambda^\ast$, known as  pull-in voltage, which depends on $\Omega$ and $\varepsilon_1(x)$, such that

$\bullet \quad $ If $\lambda&amp;lt;\lambda^\ast$, then $T_f=\infty$ and the global classical solution converges to the minimal positive steady state.

$\bullet \quad $ If $\lambda&amp;gt;\lambda^\ast$, then $T_f&amp;lt;\infty$, and then touchdown occurs in finite time.

In the actual design of a MEMS device there are several issues that must be considered.
Typically, one of the design goals is to achieve the maximum possible stable steady-state with relatively small applied voltage $\lambda$.  Another consideration may be to increase the stable operating range of the device by increasing the pull-in voltage.
For other devices, such as micropumps and microvalves, where touchdown behavior is explicitly exploited,
it is of interest to be able to localize the touchdown points, or similarly, to be able to avoid touchdown in certain parts of the device.
Our approach to achieve this last goal is to consider a material with varying dielectric permittivity allowing us to localize the touchdown points by a suitable choice of the permittivity profile.

Results

We start by the definition of touchdown point.
A point $x = x_0$ is called a  touchdown or  quenching point if there exists a sequence ${(x_n , t_n )} \in \Omega\times (0, T)$ such that



The set of all such points is called the  touchdown or  quenching set, denoted by $\mathcal{T}=\mathcal{T}_f\subset\overline{\Omega}$.

We show in [2] that touchdown can actually be ruled out in subregions of $\Omega$ 
where $f$ is positive but suitably small, below a positive threshold.

 Theorem 1:
Let $\Omega\subset \mathbb{R}^n$ a smooth bounded domain and $f$ a function satisfying



There exists $\gamma_0&amp;gt;0$ depending only on $\Omega,M,\mu,r$ such that:

(i) $\quad$ For any $x_0\in \Omega$, if $f(x_0) &amp;lt;\gamma_0 {\hskip 1pt} \text{dist}^3 (x_0,\partial\Omega)$, then $x_0$ is not a touchdown point.

(ii) $\quad $ For any $\omega\subset\subset \Omega$, if $\displaystyle\sup_{x\in \overline{\Omega}\setminus \omega} f(x) &amp;lt;
\gamma_0 {\hskip 1pt} \text{dist}^3(\omega,\partial\Omega)$, then the touchdown set 
 is contained in $\omega$.

Roughly speaking, the statement (i) in the above theorem allows one to rule out touchdown in an interior point by choosing a permittivity profile which is sufficiently small at that point, while statement (ii) allows to design devices producing touchdown inside any given subset of $\Omega$ by choosing a permittivity profile concentrated in that subset.

Numerical Simulation 1:  no touchdown at the origin 

In our first example, we consider a circular elastic plate of radius 1, that is $\Omega = B(0,1)\subset\mathbb{R}^2$.
It is well known that if the dielectric permittivity is constant, the unique touchdown point is the center of the disk.

As consequence of Theorem 1 (i), we can prevent touchdown to occur at the origin by choosing a permittivity profile $f$ such that $f(0)$ is sufficiently small.
Let’s assume that the applied voltage $\lambda$, together with the permittivity of the vacuum $\varepsilon_0$ and the permittivity of the material $\varepsilon_1$ are such that



In this case, we might cover the center of the disk with a material with higher permittivity so that the value of $\varepsilon_1(x)$ in $B(0,1/4)$ is (for example) twice its value in $\Omega\setminus B(0,1/4)$.
That is, we consider the following permittivity profile:



The following video represents the evolution of the elastic plate from the rest position to the pull-in instability.
For the numerical simulation we have applied an implicit Crank-Nicolson scheme in polar coordinates with number of meshpoint $N=100$ in the radial interval $[0,1]$.







 Simulation 1:   Evolution of the elastic plate until pull-in instability (touchdown). The solution reaches the value 1 in a circumference and then, touchdown is avoided at the center of the plate.
In the schematic video, we remove the upper plate of the MEMS device in order to see the final profile of the solution.  


Numerical Simulation 2:  touchdown near a prescribed circumference 

Let us suppose now that we want our system to touch down in a circumference of radius near $0.7$.
This time we can use Theorem 1 (ii). We will choose a permittivity profile $f$ sufficiently concentrated near a circumference of radius $0.7$. Consider for example:



The following video represents the evolution of the elastic plate from the rest position to the pull-in instability. 
In Figure 2 below, we plot the radial final touchdown profile.
For the numerical simulation we have applied an implicit Crank-Nicolson scheme in polar coordinates with number of meshpoint $N=100$ in the radial interval $[0,1]$.







 Simulation 2:   Evolution of the elastic plate until pull-in instability (touchdown). The solution reaches the value 1 in a circumference of radius near $0.7$.
In the schematic video, we remove the upper plate of the MEMS device in order to see the final profile of the solution.  





 Figure 2:   Plot of the  radial touchdown profile. Observe that, as expected, touchdown occurs on a circumference of radius between $0.65$ and $0.75$. 


Nontrivial touchdown sets

In view of Theorem 1,
it is a natural question whether such smallness conditions are actually necessary, 
or whether touchdown could be shown to occur only at or near the maximum points of the permittivity profile $f$.
In this connection, using stability properties of the touchdown set,
in [2] we construct radial ‘‘M’‘-shaped profiles giving rise to 
single-point touchdown at the origin.

Numerical simulation 3:

For this example, we have considered the following radial M-shaped profile in a disk of radius 1:



For this profile, the unique touchdown point is at the origin, far away from the maxima of $f$.
Therefore, if we want to prevent touchdown at the origin, we need to choose a permittivity profile $f$ such that $f(0)$ is small enough (below a certain thershold).
For the numerical simulation we have applied an implicit Crank-Nicolson scheme in polar coordinates with number of meshpoint $N=100$ in the radial interval $[0,1]$.







 Simulation 3:   Evolution of the elastic plate until pull-in instability (touchdown). Although we consider a ''M''-shaped permittivity profile, the solution touches down only at the origin, far away from the maximum of $f$.
In the schematic video, we remove the upper plate of the MEMS device in order to see the final profile of the solution.  


Numerical simulation 4:

Another rather surprising example is the following radially increasing profile:



For this example, the unique touchdown point is the origin, which is actually the global minimum of  $f$. In [2], we give a rigorous proof of the existence of this kind of behaviors. It is based on a stability result of the touchdown set combined with the fact that single-point touchdown occurs for constant permittivity profiles.

For the numerical simulation we have applied an implicit Crank-Nicolson scheme in polar coordinates with number of meshpoint $N=100$ in the radial interval $[0,1]$.







 Simulation 4:   Evolution of the elastic plate until pull-in instability (touchdown). Although the permittivity profile is radially increasing,  the unique touchdown point is the origin, the minimum point of $f$.
In the schematic video, we remove the upper plate of the MEMS device in order to see the final profile of the solution.  


Quantitative results in 1 dimension

Motivated by practical considerations of MEMS design, our aim in [3] is to further investigate the 
touchdown localization problem
and to show that in one space dimension, where analytic computations can be made more precise,
one can obtain quite quantitative conditions.
Namely, we look for a lower estimate of the ratio $\rho$ between $f$ and its maximum,
below which no touchdown occurs on a subregion of $\Omega$.
Rather surprisingly, it turns out that, under suitable assumptions on $f$,
our methods yield values of
 the threshold-ratio $\rho$ which are not ‘‘small’’ but can actually be up to the order



which could hence be quite appropriate for robust practical use.

In order to give good estimates of the  ratio $\rho$, we shall consider two typical situations, which roughly correspond to a one-bump or a two-bump shape for the profile $f$. 
 The touchdown is ruled out in a subinterval respectively located between a bump and 
an endpoint of $\Omega$, or between two bumps.
The idea behind this is that the plate can be covered with two dielectric materials, 
one with a high permittivity and the other with a lower permittivity. We then seek for a ratio between the two permittivities, allowing to rule out touchdown in the low permittivity region.

Reduction to a finite-dimensional optimization problem

As a consequence of our method, the threshold-ratio $\rho$  is rigorously obtained as the solution of a suitable finite-dimensional optimization problem, with either three or four parameters. 
An advantage of our results is that they apply to large classes of configurations, so that detailed numerics need not be carried out to localize the touchdown set for particular cases.

For the precise statements of the results, as well as for a detailed study of the optimization problem and numerical estimates of the threshold ratio, we refer the lector to [3].
The Matlab codes to compute numerical estimates of the solution of the optimization problem are availabe at the beginig of the page.

Here we give two concrete examples in order to illustrate 
the results.
The threshold ratio is obtained by a simple numerical procedure applied to the finite-dimensional optimization problem. 
The following two figures represent some typical permittivity profiles $f(x)$ and the localization of the corresponding touchdown sets
 in the one-bump and two-bump cases respectively. 
The touchdown sets are localized in a neighborhood of the bumps, represented by the fat lines.


 



 Figure 3: An illustration of the localization of thouchdown set for a one-bump permittivity profile.
Here, the figure represents the permittivity profile $f$ and  not  the profile of the solution.







 Figure 4: An illustration of the localization of thouchdown set for a two-bump permittivity profile.
Here, the figure represents the permittivity profile $f$ and  not  the profile of the solution.


Numerical simulation 5:

We give an example of a one-bump permittivity profile, similar to the one in Figure 3.
Although our result applies to a large class of profiles, here we consider, for computational simplicity, the following bang-bang profile:



The following video represents  the evolution of a 1-dimensional membrane
with a one-bump permittivity profile $f$. 
For the numerical simulation we have applied an implicit Crank-Nicolson scheme with number of meshpoint $N=1000$ in the interval $[-6,6]$.







 Simulation 5:   Evolution of the elastic 1-dimensional membrane until pull-in instability (touchdown). We see that touchdown is localized in the region where $f$ is big (see Figure 3).  


Numerical simulation 6:

We give an example of a two-bump permittivity profile, similar to the one in Figure 4.
Although our result applies to a large class of profiles, here we consider, for computational simplicity,
the following bang-bang profile:



The following video represents  the evolution of a 1-dimensional membrane
with a two-bump permittivity profile $f$.
For the numerical simulation we have applied an implicit Crank-Nicolson scheme with number of meshpoint $N=1000$ in the interval $[-10,10]$.







 Simulation 6:   Evolution of the elastic 1-dimensional membrane until pull-in instability (touchdown). We see that touchdown is localized in the region where $f$ is big (see Figure 4).  


References

[1] P. Esposito, N. Ghoussoub, Y. Guo,  Mathematical analysis of partial differential equations modeling electrostatic MEMS. Courant Lecture Notes in Mathematics, 20. Courant Institute of Mathematical Sciences, New York; American Mathematical Society, Providence, RI, 2010. xiv+318 pp.

[2] C. Esteve, Ph. Souplet,  No touchdown at points of small permittivity and nontrivial touchdown sets for the MEMS problem. Advances in Differential Equations, Vol 24, Number 7-8(2019), 465-500.

[3] C. Esteve, Ph. Souplet,  Quantitative touchdown localization for the MEMS problem with variable dielectric permittivity. Nonlinearity} 31 4883 (2018).

[4] Ph. Laurençot, Ch. Walker,  Some singular equations modeling MEMS. Bull. Amer. Math. Soc. 54 (2017), 437-479.

[5] J.A. Pelesko, D.H. Bernstein,  Modeling MEMS and NEMS, Chapman Hall and CRC Press, 2002.
</description>
        <pubDate>Mon, 30 Sep 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp99/P0040</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp99/P0040</guid>
        
        
        <category>tutorial</category>
        
        <category>WP99</category>
        
      </item>
    
      <item>
        <title>Uniform exponential and polynomial stability and approximation in control of a thermoelastic model</title>
        <description>The main objective of this blog is to explain numerically the difference between exponential and polynomial decay in term of energy and spectral properties. We Show also that system with polynomial decay are very  sensitive to the choice of initial data.

Model Problem

We consider the numerical approximation of the following coupled thermoelastic wave models





where $u(x,t)$ is the displacement (longitudinal or transverse, depending upon the application) at position $x$ along a bounded smooth domain $\Omega\subset\mathbb{R}$ and time $t$ , and $\theta(x,t)$ is the temperature deviation from the reference temperature at position $x$ and time $t$, $u_0(x)$, $v_0(x)$ and $\theta_0(x)$ are initial data in a suitable space. The small positive constant $\gamma$ is a thermo-mechanical coupling parameter and is generally small in comparison to 1. System (\ref{eq1}) differs from system $\eqref{eq2}$ at the coupling terms, where we have replaced the strong coupling ($\gamma\theta_x$ and $\gamma u_{tx}$) by a weak coupling ($\gamma\theta$ and $\gamma u_{t}$).

It is well known from literature that system (\ref{eq1}) and (\ref{eq2}) are respectively exponentially and polynomially stable, see [3,7] and [5,6,9].

In this post, we will show by numerical experiments, how the coupling terms affect quantitative and qualitative properties of thermoelastic systems (\ref{eq1}) and (\ref{eq2}). These results could be found in [8,10].

To do this, we consider a semi discretization version of both systems (\ref{eq1}) and (\ref{eq2}), obtained with finite element method, which has the following form



where for all $n\in\mathbb{N}$, $z_n=(u_n,v_n,\theta_n)^T$ is the semi discrete solution, $z_{n0}$ is the discretized initial data, $A_{i,n}$ the discretized dynamic and the subscript $\cdot_i$ refers to system (\ref{eq1}) and (\ref{eq2}) with



and $F_{2,n}=I_n$,



Spectral properties of thermoelastic systems

Figure 1 and Figure 2 show how the coupling terms affect the placement of eigenvalues of the dynamic $A_{n}$. In Figure 1, we see that a uniform distance between the eigenvalues and the imaginary axis is preserved, see Table 1. Another observation is that for fixed $n$, the eigenvalues of higher frequency modes, in particular, the one of the $n^{th}$ mode, are closer to the imaginary axis. Moreover, as the number of modes increases, these eigenvalues bend back towards the vertical line $\lambda=-\frac{\gamma^2}{2}$, a fact which has been already shown in [4]. Therefore, the corresponding spectral element approximation scheme preserves the property of exponential stability.

 Table 1. Distance between $\sigma(A_{1,n})$ and the imaginary axis form the spectral element method


    
        n
        $min \{ -Re(\Lambda),\Lambda \in \sigma(A_{1,n}) \}$
    
    
        8
        $8.9227x10^{-4}$
    
    
        16
        $8.9383x10^{-4}$
    
    
        24
        $8.9402x10^{-4}$
    
    
        32
        $8.9407x10^{-4}$
                


 Location of the complex eigenvalues of the matrix $A_{1,n}$ with the finite element method

error
Nx = 30;
stabexpsem(Nx);


Error using error
Not enough input arguments.

Error in tp1d0d1572_49d1_41a8_914f_f09a199a8ec3 (line 114)
error



In Figure 2, conversely to Figure 1 where a uniform distance between the eigenvalues and the imaginary axis is preserved, we observe that, as the number of modes increases, an asymptotic behaviour appears in the neighborhood of the imaginary axis at $\pm\infty$. This property is mainly related to systems with polynomial decay, see \cite{BEPS2006}.

 Location of the complex eigenvalues of the matrix $A_{2,n}$ with the finite

Nx = 30;
stabpolysem(Nx);


Uniform and polynomial decay of the energy

The discrete energy associated to system (\ref{eq3}) is given by



The discrete energy $E_{1,n}$ associated to system (ref{eq1}) decays exponentially to zero, see Figure 3, in the following sense: $\exists M,\alpha$ positive constants such that



However, the introduction of the weak coupling term in system (ref{eq1}) has changed the dynamic and consequently the behavior of energy (\ref{eq4}). In this case, we say that system (\ref{eq2}) decays polynomially to zero, see Figure 4, in the following sense: $\exists M,\alpha$ positive constants such that



 Exponential decay of $E_{1,n}(t)$

 Polynomial decay of $E_{2,n}(t)$

Effect of smoothness of the initial data on the rate of decay of energy

It has been shown theoretically, see [1,2], that the energy associated to system (\ref{eq2}) is very sensitive to the smoothness of its initial data. This fact, has been also observed numerically, see Figure 6. we use

Nx = 100;
FinalTime = 100;
dt = 0.5;
mode = 1;
Gamma = 0.2;
n = 100;
k= 2;


and we consider the following initial data

u0=zeros(1,n);
x = linspace(0,pi,100);
v0=sqrt(2/pi)*sin(k*x);
teta0=zeros(1,n);


Through Figure 6, we notice that for $j = 1$, the approximate energy $E_{2,n}(t)$ decays to zero as the time $t$ increases. Moreover, we observe that the decay rate depends strongly on $j$. That is, when $j$ increases, initial data are very oscillating. We say in this case that the rate of decay of the discrete energy $E_{2,n}(t)$ is very sensitive to the choice of the initial data. However, the behavior of the energy assosiated to system (\ref{eq1}) remains indifferent to the smoothness of initial data when $n\to\infty$, see Figure 5.

  Exponential decay of $E_{1,n}(t)$

  Polynomial decay of $E_{2,n}(t)$

References

[1] A. B' atkai, K.J. Engel, J. Pr&quot;uss and R. Schnaubelt, Polynomial stability of operator semigroups, Math. Nachr. 279, pp.

[2] A. Borichev and Y. Tomilov, Optimal polynomial decay of functions and operator semigroups, Math. Ann., 347(2), pp.455-478, 2010.

[3] S. W. Hansen, Exponential energy decay in a linear thermoelastic rod. J. Math. Anal. Appli.,167, pp. 429-442, 1992.

[4] F.A. Khodja, A. Benabdallah, and D. Teniou, Stability of coupled systems, Abstr. Appl. Anal. Volume 1, Number 3, 327-340, 1996.

[5] F. A. Khodja, A. Benabdallah and D. Teniou, Dynamical stabilizers and coupled systems}, ESAIM Proceeding,2, pp. 253-262, 1997.

[6] F. A. Khodja, A. Bader and A. Benabdallah, Dynamic stabilization of systems via decoupling techniques, ESAIM: COCV,4,

[7] Z. Liu and S. Zheng, Exponential stability of semigroup associated with thermoelastic system, Quart. Appl. Math, 51, pp.535-545, 1993.

[8] Z. Y. Liu and S. Zheng, Uniform exponential stability and approximation in control of a thermoelastic system, SIAM J. Control Optim. 32, pp. 1226-1246, 1994.

[9] Z. Liu and B. Rao, Characterization of polynomial decay rate for the solution of linear evolution equation. Zeitschrift  angewandte Mathematik und Physik ZAMP,56, pp. 630-644, 2005.

[10] L. Maniar and S. Nafiri, Approximation and uniform polynomial stability of C_0-semigroups,ESAIM: COCV 22, pp. 208-235, 2016.

</description>
        <pubDate>Thu, 18 Jul 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp99/P0030</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp99/P0030</guid>
        
        
        <category>tutorial</category>
        
        <category>WP99</category>
        
      </item>
    
      <item>
        <title>Controllability to steady states of a reaction diffusion system and emergence of barriers.</title>
        <description>asda
</description>
        <pubDate>Mon, 08 Jul 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp99/WP99-P0028</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp99/WP99-P0028</guid>
        
        
        <category>tutorial</category>
        
        <category>WP99</category>
        
      </item>
    
      <item>
        <title>Analysis, numerics and control for the obstacle problem</title>
        <description>In this tutorial, we present some aspects of the obstacle problem in the stationary (elliptic) and the evolutionary (parabolic) setting by means of numerical simulations, performed in FEniCS and Matlab.




 Figure 1. The solution (blue) of the one-dimensional elliptic obstacle problem is superharmonic (concave down) and matches derivatives with the obstacle (black). 


1. Mathematical formulation

1.1. Elliptic problem
Let $\Omega \subset \mathbb{R}^d$ be open, regular and bounded, let $\psi \in H^2(\Omega)$ (the obstacle) and $g \in H^1(\Omega)$ (boundary data) with $\psi \leq g$ on $\partial \Omega$ be given. We consider the functional



where $f \in L^2(\Omega)$ is given, and the closed convex set



The classical obstacle problem can be written as



and admits a unique minimizer $\phi \in \mathcal{K}^g_{\text{OB}}$, which is also a solution of the variational inequality



The terminology “obstacle problem” comes from the physical origin of the model $-$ the solution $\phi$ represents the equilibrium position of an elastic membrane, which is constrained to lie above an obstacle (given by the graph of the function $\psi$).

Notation:  The notation



designates the Gateaux derivative ($L^2$-gradient) of the functional ${F}_{\text{OB}}$ at the point $\phi$ in the direction $\zeta$; in occurence,



Most of the regularity theory for the obstacle problem has been developed for the model case $\Delta \psi=-1$. We refer to [3,4] for a detailed presentation. 
For simplicity of the presentation, let us thus assume for a moment that $f \equiv -1$ and $\psi \equiv 0$. Choosing appropriate test functions bring us from the variational inequality for $\phi$ to



The set ${ \phi=0 }$ is called contact set, while $\partial { \phi&amp;gt;0 }$ is the free boundary.



 Figure 2. The solution $\phi$ to the elliptic obstacle problem just above. The obstacle is equal to 0 (gray plane), and the free boundary (boundary of the contact set between the membrane and the obstacle) is displayed in red, projected just below. 


Remark: At this point, we may see why solving the elliptic obstacle problem is different to just solving the Poisson equation for $\phi$ in a subdomain of $\Omega$ and setting $\phi=\psi$ elsewhere. Indeed, solving the latter problem does not guarantee that we would have



along the boundary.

1.2. Parabolic problem

The parabolic counterpart onsists in finding $u \in H^1(]0, +\infty[; L^2(\Omega))\cap L^2(]0,+\infty[; H^2(\Omega) \cap \mathcal{K}^g_{\text{OB}})$ satisfying



The name “parabolic obstacle problem” for the above system may be misleading. 
While the mathematical formulation is similar, the physical interpretation of the elliptic and parabolic problem is different. In the parabolic setting, seeing $\psi$ as a physical obstacle in space is not correct. Rather, one should interpret $\psi$ as a barrier for the temperature $u$. More accurately, we are dealing with a parabolic variational inequality with an obstacle-type constraint.

The function $\psi$ may be time-dependent, but for simplicity we omit this case.











 Figure 3. Here $\psi(x)=3*\sqrt{(1-x^2)}$ in $(-1,1)$ (black) is a barrier for the evolving solution (blue). The initial data coincides with $\psi$ on (-1.5, 1.5). We observe that the contact set $\{ u = \psi \}$ changes (gets smaller) as time grows. There is also matching of normal derivatives on the contact points (i.e. the free boundary). 
Finally, the solution of the parabolic problem (left, blue) converges to that of the elliptic problem (right, red) for large time, as seen below.   











Figure 4. The solution to the parabolic problem with Dirichlet boundary condition =1. The obstacle is given by the &quot;three towers&quot;. The free boundary (boundary of the contact set with the obstacle) is projected on a plane below in red. It is seen that the source term $-1$ acts as a gravity force pushing the membrane towards the obstacle. We moreover observe a convergence to the equilibrium position (see also the section below). 

For $\psi \equiv 0$ and $f\equiv-1$ (or more generally $f + \Delta \psi = -1$), a manifestation of the parabolic variational inequality above is as a “weak form” of the one-phase Stefan problem (see [3,4]). In this setting, we may pass from the variational inequality to the equivalent problem with two boundary conditions at the free boundary



This is an overdetermined problem, which means that the free boundary/the contact set ${u=0}$ has to change with time to ensure that both BC hold for every time. The evolution of the contact set may be observed in the numerics of Figures 3,4,5.

Remark: $\quad$
As for the elliptic problem, solving the above parabolic problem is different to just solving the heat equation for $u$ in a subdomain of $\Omega$ and setting $u$ elsewhere, with a Dirichlet boundary condition. Indeed, solving the latter problem does not guarantee that we would have $|\nabla u|= 0$ along the boundary.

Remark: $\quad$
While in Figure 3 the contact set is shrinking does not disappear, it may happen that the contact set vanishes (depending on the magnitude of the boundary data $g$, the support of the initial data $u_0$, and if present, the source term $f$).

1.3. Relationship between both problems

In the recent paper [5], it is shown that the solution $u(t)$ to the parabolic variational inequality converges to the solution $\phi$ of the elliptic probelem in $H^1(\Omega)$ norm as $t\rightarrow \infty$ with exponential rate. This is done by means of new techniques including a so-called constrained Lojaseiwicz inequality.

We may observe this convergence numerically in Figures 4 and 5. In both cases, we work in the ball $B_2$, with boundary data $g\equiv 0.8$, barrier $\psi\equiv 0$ and 
initial datum supported outside $B_{1.5}$.

The one-dimensional case:










Figure 5. The solution of the parabolic problem (left, blue) converges to that of the elliptic problem (right, red) as time increases. We again see that the contact set $\{ u = 0\}$ changes with time. 

The two-dimensional case:












Figure 6. We see that as time increases, the contact region $\{ u = 0\}$ (white patch) gets smaller. In terms of the Stefan problem, the white patch represents the ice temperature. Ice is melting as time increases, but does not fully melt, because the support of the initial data is too small and the temperature on the boundary is not high enough.  










Figure 7. In this figure, the obstacle is set to zero, and we consider Dirichlet boundary conditions $=0.8$ on the fixed boundary. We see that even when considering initial data which are not an equilibrum membrane position (but do satisfy the obstacle condition), as time increases, the solution of the parabolic problem (left) converges to that of the elliptic problem (right)  

2. Numerical implementation

2.1. Possible strategies

A common approach for proving the well-posedness and $W^{2,p}$ regularity for variational inequalities is a technique called penalization, which consists of approximating by a sequence of semilinear equations [8]. For any $\epsilon&amp;gt;0$, we will consider



to approximate the parabolic problem, and remove $u’$ and time dependence for the elliptic one. As $\epsilon\rightarrow0$, the solution $u_\epsilon$ of the above equation converges to the variational inequality.  Here



may be replaced by another penalty function, for instance $\beta_\epsilon(u) = -\exp(-u/\epsilon)$.

The approximated problem is relatively simple to implement numerically. We may use finite elements (say $P1$ elements) to discretize in space, and time-stepping (Euler implicit for instance) to discretize in time.

There are other ways to solve the variational inequality after discretizing in space and time (using one’s favorite metood). One which is popular in the literature for the elliptic problem is the primal dual active set method, which we will present in a future post.

2.2. Code

i). Elliptic problem.

Let us start by solving the two-dimensional elliptic problem from Figure 4.

The fenics module [1] contains all of the necessary finite element tools for the space discretization of PDEs.

from fenics import *
from obstacles import dome
from mshr import mesh

It is advantageous to regularize the “kink” appearing in penalty function (the negative part of a function) in view of using a Newton method for solving the nonlinear problem

def smoothmax(r, eps=1e-4): 
	return conditional(gt(r, eps), r-eps/2, conditional(lt(r, 0), 0, r**2/(2*eps)))

We mesh the domain $B_2$.

mesh = generate_mesh(Circle(Point(0, 0), 2), 25)


$P1$ elements are used for the FEM spaces (but one may easily choose higher order in the definition of V).

V = FunctionSpace(mesh, &quot;CG&quot;, 1)
w = Function(V)
v = TestFunction(V)
psi = Constant(0.)
f = Constant(-1)		
eps = Constant(pow(10, -8))


Using the symbolic expressions of FEniCS, we define the variational formulation of the penalized PDE, which we write in the form $F(w)=0$:

bc = DirichletBC(V, 0.8, &quot;on_boundary&quot;)
F = dot(grad(w), grad(v))*dx - 1/eps*inner(smoothmax(-w+psi), v)*dx - f*v*dx


We solve the nonliner problem with the command solve, which makes use of a Newton method to solve the nonlinear equation $F(w)=0$.

solve(F == 0, w, bcs=bc)


We visualise in ParaView by storing the solution in .vtk format using:
vtkel = File(&quot;output/el.pvd&quot;)
vtkel &amp;lt;&amp;lt; w


ii). Parabolic problem.

We set $T=2$, use $100$ subdivisions and set

dt = T/100


This part only differs by the presence of a time-loop. We define the initial datum in a separate class:

class indata(Expression):
	def eval(self, value, x):
		if sqrt(x[0]*x[0]+x[1]*x[1]) &amp;lt;= 1.95:
			value[0] = 0
		elif 1.95 &amp;lt;= sqrt(x[0]*x[0] + x[1]*x[1]) &amp;lt;= 2:
			value[0] = (sqrt(x[0]*x[0]+x[1]*x[1])-1.95)/(2-1.95)*0.8
		else:
			value[0] = 0.8


Repeating as in the elliptic case up to defining the variational form, we now set the inital datum (interpolating the expression onto the FEM space) and time:

un = interpolate(indata(degree=2), V)
t = 0

We set up the time loop:

vtksol = File(&quot;output/popsol.pvd&quot;)
for n in range(num_steps):
    t+=dt
    solve(F==0, u, bcs=bc)
    vtksol &amp;lt;&amp;lt; (u, t)
    un.assign(u) 


Optimal Control

We will now briefly present the implementation of an optimal control strategy for the elliptic and parabolic problems. We will make use of the penalized problems where $\epsilon&amp;gt;0$ is small (of the order $10^{-8}$). This strategy has been succesfully applied in [8].

3.1. Elliptic problem

Given a target $\phi_d$ and a regularization parameter $\delta&amp;gt;0$ (we usually use $\delta = 10^{-2}$), we seek to minimize



subject to



For simplicity of the presentation, we will consider







on the domain $\Omega = [-2,2]^2$. The target is the function $\phi_d(x) = \frac12(\cos(x_1)+\cos(x_2))$.

The uncontrolled solution of the elliptic obstacle problem in this case is:



	

	
	

	



Figure 8. For comparison with the controlled problem, we show also the solution to the free elliptic problem with the &quot;dome&quot; like obstacle given on the right. 

The FEniCS optimization code will have the effect of obtaining the following results.



	

	
	
	
	















Figure 9. The optimal state (top left) can be seen to be above the obstacle (top right), and is very much alike the shape of the target (bottom left). Finally, the optimal control is given (bottom right). 

Let us present the numerical implementation. After importing the dolfin_adjoint optimization module [2], we make use of the code for simulating the uncontrolled problem. A “tape” of the forward model is built. This tape is used to drive the optimization by repeatedly solving the forward model and the adjoint model for varying control inputs.

Defining the objective functional to be minimized:

delta = Constant(1e-2)
j = assemble(0.5*inner(w-wd, w-wd)*dx + delta/2*inner(u, u)*dx)	
m = Control(u)


Writing the state $y$ as the output of the solution map corresponding to the input $m$, one sees that the objective functional depends only on the control:

J = ReducedFunctional(j, m)									


We run the optimization, based on a conjugate-gradient method:

u_opt = minimize(J, options={&quot;method&quot;=&quot;CG&quot;})


The tape is modified such that the initial guess for y (to be used in the Newton solver in the forward problem) is set to y_opt.

y_opt = y.block_variable.saved_output
Control(y).update(y_opt)


The algorithm converged after 8 iterations, and the value of the functional at the final iteration is $0.0967$.

3.2. The parabolic problem

Given a final time $T&amp;gt;0$, a target $u_d$ and a regularization parameter $\delta&amp;gt;0$, we seek to minimize



subject to



For simplicity of the presentation, we will consider the same setting as in Figure 3, namely $f\equiv-1\,$,  $g\equiv0.8\,$, on the domain $\Omega = [-2,2]$. The target temperature is $u_d(x) = 0.8$. This would correspond to “melting”, as this target is never equal to zero, thus has no contact set. We would thus expect the action of the control $v$ to lift-up $u$ from $0$ and thus the contact set (and free boundary) to vanish.

We appeal to DyCon-Toolbox [9] for the numerical implementation of this problem. DyCon-Toolbox is an efficient and easy to use solver for nonlinear control problems.



Figure 10. The optimal state (left, red), starting from an initial datum with a non-empty contact set (left, blue) and the optimal control (red, right). 


The action and magnitude of the control ensures that the solution to the parabolic problem will lift-off from the initial contact set. 
For comparison, we recall that the uncontroled free dynamics have a very different behavior:





Figure 11. The uncontrolled free dynamics of the parabolic obstacle problem in 1 dimension, where the obstacle is zero. 

We begin by symbolically defining the time variable:
syms t


We discretize the interval $[-2, 2]$:
N = 70;
xi = -2; xf = 2;
xline = linspace(xi,xf,N+2);
xline = xline(2:end-1);

We define symbolically (as in FEniCS) the state vector $Y$ and control vector $U$:
Y = SymsVector('y',N);
U = SymsVector('u',N);

We discretize by finite differences, and define the semilinear penalty term:
alpha = 1e-3;
epsilon = 1e-2;
A = FDLaplacian(xline);

F  = @(Y) NonLinearTerm(Y,alpha);
dF = @(Y) DiffNonLinearTerm(Y,alpha);


We may define the parabolic problem to be solved.
dx = xline(2) - xline(1);
Y_t = @(t,Y,U,Params) A*Y + 1/epsilon*F(-Y) + U + (1/dx^2)*[0.8;zeros(N-2,1);0.8];
Dyn = pde(Y_t,Y,U);

We set the mesh, and other relevant parameters such as the final time and initial data. We consider the same data as in the free problem.

Dyn.mesh   = xline;
Dyn.Solver = @ode23;
Dyn.Nt     = 150;
Dyn.FinalTime        = 0.3;
Dyn.InitialCondition = InitialConditionFcn(xline);


We now compute the necessary Jacobian matrices.

Dyn.Derivatives.Control.Num = @(t,Y,U,Params) eye(N);
Dyn.Derivatives.State.Num   = @(t,Y,U,Params) A + 1/epsilon*dF(-Y);


We may solve the free problem over the given time interval. We recall that we have used finite differences to discretize in space (but finite elements can be used as well in more complicated settings), and the underlying solve command solves the system of ODEs by means of some Runge-Kutta scheme.

[tspan,Ysolution] = solve(Dyn);
figure
surf(Ysolution)

Having solved the free dynamics, we are now in a position to solve the optimal control problem:

YT = 0.8 + 0*xline';
beta = dx^4;
Psi  = @(T,Y) dx*(YT - Y).'*(YT - Y);
L    = @(t,Y,U)  dx*(YT - Y).'*(YT - Y) + ...
                 beta*dx*0.5*alpha*(U.'*U);
OCP = Pontryagin(Dyn,Psi,L);
U0 = -ones(Dyn.Nt,Dyn.ControlDimension);
U0 = GradientMethod(OCP,U0,'Graphs',true,'EachIter',2,'DescentAlgorithm',@AdaptativeDescent)


References:

[1] The FEniCS Project Version 1.5
M. S. Alnaes, J. Blechta, J. Hake, A. Johansson, B. Kehlet, A. Logg, C. Richardson, J. Ring, M. E. Rognes and G. N. Wells
Archive of Numerical Software, vol. 3, 2015.

[2] Patrick E. Farrell, David A. Ham, Simon W. Funke and Marie E. Rognes (2013). Automated derivation of the adjoint of high-level transient finite element programs, SIAM Journal on Scientific Computing 35.4, pp. C369-C393. doi:10.1137/120873558. arXiv:1204.5577

[3] Figalli A. (2018), Free boundary regularity in obstacle problems 
Journées EDP 2018, to appear.

[4] Figalli, A. (2018), Regularity of interfaces in phase transitions via obstacle problems 
Proceedings ICM 2018, to appear.

[5] Colombo M. and Spolaor, L. and Velichkov, B. (2018), On the asymptotic behavior of the solutions to parabolic variational inequalities, ArXiV preprint.

[6] Borjan Geshkovski (2018). Obstacle Problems: Theory and Applications. Master Thesis.

[7] Hintermüller M. and Kopacka I., A smooth penalty approach and a nonlinear multigrid algorithm for elliptic MPECs. Computational Optimization and Applications, 50(1):111–145, 2011.

[8] D. Kinderlehrer and G. Stampacchia. An introduction to variational inequalities and their applications. Volume 31 of Classics in Applied Mathematics. SIAM, 2000.

[9] DyCon-Toolbox, https://deustotech.github.io/dycon-toolbox-documentation/.

</description>
        <pubDate>Mon, 08 Jul 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp03/WP03-P0031</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp03/WP03-P0031</guid>
        
        
        <category>tutorial</category>
        
        <category>WP03</category>
        
      </item>
    
      <item>
        <title>Rotors imbalance suppression by optimal control</title>
        <description>Consider a rotor rotating around a fixed axis. Because of wear and damage, the mass distribution is not homogeneous. This leads to dangerous vibrations in the rotation. A prototypical example can be a wind turbine, affected by misalignment of the blades and/or mass imbalance of the hub and blades [2].

In order to compensate the imbalance, two balancing heads are mounted at the endpoints of the axle, as in figure 1. Each balancing head is made of two masses free to rotate.

Our goal is to determine the optimal movement of the balancing masses to minimize the vibrations. Control theoretical techniques are employed. For further details, see [6].

 Figure 1: representation of the rotor and the balancing. The balancing heads are located at the endpoints of the spindle. The four balancing masses (two for each balancing head) are drawn in red. 

Consider a rotor-fixed reference frame $(O;(x,y,z))$. Figures 1 and 2 show a front view of the device and a scheme of the balancing heads. 

Figure 2: front view of the system made of rotor and balancing device. 

Figure 3: scheme of one balancing head. The balancing masses $(m_i,P_{i,1})$ and $(m_i,P_{i,2})$ are drawn in red. The bisector of the angle generated by $\overset{\longrightarrow}{OP_{i,1}}$ and $\overset{\longrightarrow}{OP_{i,2}}$ is the dashed line. The *intermediate* angle $\alpha_i$ is represented in 3(A), while the *gap* angle $\gamma_i$ is depicted in 3(B). The angles $\alpha_i$ and $\gamma_i$ give the position of the balancing masses in each balancing head. 

We consider two planes



orthogonal to the rotation axis $z$. The balancing device (see figures 1 and 2) is made of two heads lying in each of these planes.

The heads are fixed to the rotor and rotate with it. In particular, $\alpha_i$ and $\gamma_i$ are defined with respect to the rotor-fixed reference frame $(O;(x,y,z))$.

Each head is made of a pair of balancing masses, which are free to rotate orthogonally to the rotation axis $z$.

Namely, we have

  two mass-points $(m_1,P_{1,1})$ and $(m_1,P_{1,2})$ lying on $\pi_1$ at distance $r_1$ from the axis $z$, i.e., in the reference frame $(O;(x,y,z))$





  two mass-points $(m_2,P_{2,1})$ and $(m_2,P_{2,2})$ lying on $\pi_2$ at distance $r_2$ from the axis $z$, namely, in the reference frame $(O;(x,y,z))$




For any $i=1,2$, let $b_i$ be the bisector of the angle generated by $\overset{\longrightarrow}{OP_{i,1}}$ and $\overset{\longrightarrow}{OP_{i,2}}$ (see figure 3). The intermediate angle $\alpha_i$ is the angle between the $x$-axis and the bisector $b_i$, while the gap angle $\gamma_i$ is the angle between $\overset{\longrightarrow}{OP_{i,1}}$ and the bisector $b_i$.

The imbalance is modelled by a resulting force $F$ and a momentum $N$ orthogonal to the rotation axis. In the rotor-fixed reference frame $(O;(x,y,z))$, set $P_1 :=  (0,0,-a)$, $P_2 :=  (0,0,b)$, $F :=  (F_x,F_y,0)$ and $N :=  (N_x,N_y,0)$. By imposing the equilibrium condition on forces and momenta, the force $F$ and the momentum $N$ can be decomposed into a force $F_1$ exerted at $P_1$ contained in plane $\pi_1$ and a force $F_2$ exerted at $P_2$ contained in $\pi_2$.

In each plane, we generate a force to balance the system, by moving the balancing masses:

  in the plane $\pi_1$, we compensate $F_1$ by the centrifugal force:





  in the plane $\pi_2$, we compensate $F_2$ by the centrifugal force:




The overall imbalance of the system is then given by the resulting forces in $\pi_1$ and $\pi_2$,



and



respectively.

The overall imbalance on the system made of rotor and balancing device is measured by the imbalance indicator



Our task is to find a control strategy such that

  the balancing masses move from their initial configuration $\Phi_0$ to a final configuration $\overline{\Phi}$, where the imbalance is compensated;
  the imbalance and velocities should be kept small during the correction process.


We address the minimization problem



where $Q(\Phi) :=  G(\Phi)-\inf G$ and



with .

The theoretical analysis of the above problem is in [6]. The existence of the optimum is proved. The stabilization of the optimal trajectories towards steady optima is proved in any condition.

Simulation

In order to perform some numerical simulations, we firstly discretize our cost functional and then we run AMPL-IPOpt to minimize the resulting discretized functional.

For the purpose of the numerical simulations, it is convenient to rewrite the cost functional as



subject to the state equation



Discretization
Choose $T$ sufficiently large and $Nt \in \mathbb{N} \setminus {0,1}$. Set



The discretized state is  , whereas the discretized control (velocity) is $(\psi_{i})_{i=0, … ,Nt-2}$. The discretized functional then reads as



subject to the state equation



Execution

The discretized minimization problem is



We address the above minimization problem by employing the interior-point optimization routine IPOpt (see [3,4]) coupled with AMPL [1], which serves as modelling language and performs the automatic differentiation. The interested reader is referred to [8, Chapter 9] and [7] for a survey on existing numerical methods to solve an optimal control problem.

In figures 4, 5, 6 and 7, we plot the computed optimal trajectory for \eqref{functional}, with initial datum $\Phi_0=\left(\alpha_{0,1},\gamma_{0,1};\alpha_{0,2},\gamma_{0,2}\right) :=  \left(2.6,0.6, 2.5,1.5\right)$. We choose $F$, $N$ and $m_i$. The exponential stabilization proved in [6] emerges. 

Figure 4: intermediate angle $\alpha_1$ versus time. 

Figure 5: gap angle $\gamma_1$ versus time. 

 Figure 6: intermediate angle $\alpha_2$ versus time. 
In figure 8, we depict the imbalance indicator versus time along the computed trajectories. As expected, it decays to zero exponentially.

Figure 7: gap angle $\gamma_2$ versus time. 

 Figure 8: system response. 



 Figure 9: We represent the evolution in time of the rotor vibrations.
In the uncontrolled case, the balancing device does not act. In the controlled case, our balancing strategy suppress the vibrations. 

References:
[1] Robert Fourer, David M Gay, and Brian W Kernighan. A modeling language for mathematical programming. Management Science, 36(5):519{554, 1990.

[2] Mike Jeffrey, Michael Melsheimer, and Jan Liersch. Method and system for determining an imbalance of a wind turbine rotor, September 11 2012. US Patent 8,261,599.

[3] Andreas Waechter, Carl Laird, F Margot, and Y Kawajir. Introduction to ipopt: A tutorial for downloading, installing, and using ipopt. Revision, 2009.

[4] Andreas Waechter and Lorenz T Biegler. On the implementation of an interior-point lterline-search algorithm for large-scale nonlinear programming. Mathematical programming, 106(1):25{57, 2006.

[6] Matteo Gnuffi, Dario Pighin and Noboru Sakamoto. Rotors imbalance suppression by optimal control. Preprint.

[7] Noboru Sakamoto and Arjan J van der Schaft. Analytical approximation methods for the stabilizing solution of the Hamilton-Jacobi equation. IEEE Transactions on Automatic Control, 53(10):2335{2350, 2008.

[8] Emmanuel Trélat. Contrôle optimal : théorie et applications
Mathématiques Concrètes. Vuibert, Paris, 2005. available
online:
https://www.ljll.math.upmc.fr/trelat/chiers/livreopt2.pdf.
</description>
        <pubDate>Mon, 08 Jul 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp02/P0005</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp02/P0005</guid>
        
        
        <category>tutorial</category>
        
        <category>WP02</category>
        
      </item>
    
      <item>
        <title>Propagation of one and two-dimensional discrete waves under finite difference approximation</title>
        <description>In [1], we discuss several aspects of wave propagation in a computational
framework. In particular, we consider the following one-dimensional wave equation



and we discuss the propagation properties of its finite-difference solutions on uniform and non-uniform grids, by establishing comparisons with the usual behavior of the continuos waves.

Our approach is based on the study of the propagation of high-frequency Gaussian beam solutions (that is, solutions originated from highly concentrated and oscillating initial data), both in continuous and
discrete media.

Roughly speaking, the idea at the basis of this techniques is that the energy of Gaussian beam solutions propagates along bi-characteristic rays, which are obtained from the Hamiltonian system associated to the
symbol of the operator under consideration.

At the continuous level of equation \eqref{main_eq_1d}, these mentioned rays are straight lines and travel with a uniform velocity.

On the other hand, the finite difference space semi-discretization of \eqref{main_eq_1d} may introduce different dynamics, with a series of unexpected propagation properties at high frequencies. For instance, one can generate spurious solutions traveling at arbitrarily small velocities which, therefore, show lack of propagation in space.

In addition, the introduction of a non-uniform mesh for the discretization may generate further pathologies such as internal
reflections, meaning that the waves change direction without hitting the boundary.

In order to illustrate these mentioned pathological phenomena, we perform simulations on a uniform space-mesh of $N$ points and mesh-size $h=2/(N+1)$



and on two non-uniform ones produced by applying to $\mathcal G_h$ the transformations



Moreover, the initial data are constructed starting from the following Gaussian profile



Low frequency simulations

We present in Figure 1 our simulations for low-frequency solutions of \eqref{main_eq_1d}. In particular, we considered initial position and frequency $(x_0,\xi_0)=(0,\pi/4)$ and a time horizon $T=5s$.

In this case, the numerical solutions behave basically like the continuous ones: they start traveling to the left along the straight characteristic line $x+t$ and, after having hit the boundary, they reflect following the Descartes-Snell’s law and continue propagating, this time to the right along the other branch of the characteristic ($x−t$).


Figure 1. Numerical solutions of \eqref{main_eq_1d} with $(x_0,\xi_0)=(0,\pi/4)$ on uniform and non-uniform meshes.

High frequency simulations

When increasing the frequency, the situation changes and we encounter several interesting phenomena and pathologies:

• The so-called umklapp or U-process, also known as internal reflection, consisting in the reflection of waves without touching only one or both the endpoints of the space interval. This phenomenon is typical for the semi-discretization of high-frequecncy solutions of \eqref{main_eq_1d} on non-uniform meshes, which may produce waves oscillating in the interior of the computational domain and reflecting without touching the boundary (see Figure 2 - middle) or touching the boundary only at one of the endpoints (see Figure 2 - right).


Figure 2. Numerical solutions of \eqref{main_eq_1d} with $(x_0,\xi_0)=(1/2,\pi)$ on uniform and non-uniform meshes.

• Non-propagating waves, corresponding to equilibrium (fixed) points on the phase diagram (see Figure 3).


Figure 3. Numerical solutions of \eqref{main_eq_1d} with $(x_0,\xi_0)=(0,\pi)$ on uniform and non-uniform meshes.

These phenomena are related with the particular nature of the discrete group velocity which, in the finite difference setting, is given by



and vanishes for $\xi = (2k+1)\pi$, $k\in\mathbb{Z}$. Moreover, they can be understood by looking at the phase portrait of the hamiltonian system associated to the finite difference semi-discretization of \eqref{main_eq_1d} (see Figure 4).


Figure 4. Phase portrait of the Hamiltonian system for the numerical wave equation corresponding to the and the grid transformation $g_1$ (left) and $g_2$ (right).

In particular, the solutions displayed in Figure 2 correspond to
trajectories which remain always in the red area of these phase portraits, while Figure 3 shows solutions starting from the equilibrium point $(0,\pi)$ (the green one in Figure 4)

Notice that, for the grid transformation $g_1$, this equilibrium point is a center (stable) while it is a saddle (unstable) for the grid transformation $g_2$. For this reason, in the first case the non-propagating wave remains concentrated along the vertical ray, while in the second case the wave presents more very dispersive features.

Two-dimensional simulations

Analogous phenomena can be detected also in the case of the finite-difference semi-discretization of the two-dimensional wave equation



Also in this case, we perform simulations on  a uniform mesh of $N$ points both in the $x$ and $y$ direction and mesh-sizes $h_x=2/(N+1)=h_y$



and on two non-uniform ones produced by applying to $\mathcal G_h$ the transformations $g_1$ and $g_2$ above introduced.

Moreover, the initial data are constructed once again starting from a Gaussian profile:



Then, it can be observed in Video 1 that, as for the one-dimensional case, at low frequencies the solution remains concentrated and propagates along straight characteristics which reach the boundary, where there is reflection according to the Descartes-Snell’s law. This independently on whether we use a uniform or a non-uniform mesh.







Video 1. Numerical solutions of \eqref{main_eq_2d} with $(x_0,y_0,\xi_0,\eta_0)=(0,0,\pi/4,\pi/4)$ on uniform and non-uniform meshes.

Nevertheless, increasing the frequencies similar phenomena as in the one-dimensional case show up. For instance, on the non-uniform mesh corresponding to the transformation $g_1$, we observe the so-called
rodeo effect, according to which, waves that should propagate along straight lines are trapped along closed circles (Video 2 - middle).




Video 2. Numerical solutions of \eqref{main_eq_2d} with $(x_0,y_0,\xi_0,\eta_0)=(0,\tan(\arccos(\sqrt[4]{1/2},\pi/2,\pi)$ on uniform and non-uniform meshes.

Finally, waves starting from the point $(x_0,y_0,\xi_0,\eta_0)=(0,0,\pi,\pi)$, which is an equilibrium for the phase Hamiltonian system, cannot move, and remain trapped around the point $(0,0)$ in the physical plane for any time (see Video 3).




Video 3. Numerical solutions of \eqref{main_eq_2d} with $(x_0,y_0,\xi_0,\eta_0)=(0,0,\pi,\pi)$ on uniform and non-uniform meshes.

Conclusions

Summarizing, our analysis shows that the finite-difference semi-discretization of one and two-dimensional waves may modify the dynamics of the continuous model. In particular, as a result of the accumulation of the local effects introduced by the heterogeneity of the employed grid, numerical high-frequency solutions can bend in a singular and unexpected manner. Moreover, this phenomenon has to be added to the well known numerical dispersion effect, producing the high-frequency discrete group velocity to vanish, even in uniform grids.
Our results constitute a warning both for adaptivity and for the treating of control and inverse problems. In broad terms, the goal of adaptivity is to refine a mesh on the support of the solution, keeping it coarse where the solution has little oscillations and energy. Our analysis shows that, in this context, adaptivity has to be performed with some attention. Indeed, if one is not careful enough when refining the mesh, they can be produced spurious effects due to the fact that waves feel the fictitious numerical boundaries that are generated when the grid passes from fine to coarse. Finally, our results are also a signal that the dangers of uniform meshes in the study of numerical control and inverse problems (already observed in [2]) may be enhanced when the mesh is non-uniform. In more details, the heterogeneity of the grid may introduce added trapping effects, which need to be avoided in
order to prove convergence in the context of controllability, stabilization or inversion algorithms.

References

[1] U. Biccari, A. Marica and E. Zuazua, Propagation of one and two-dimensional discrete waves under finite difference approximation. Submitted

[2] E. Zuazua, Propagation, observation, control and numerical approximation of waves. SIAM Rev. 47, 2 (2005), 197-243.

</description>
        <pubDate>Wed, 26 Jun 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp99/WP99-P0023</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp99/WP99-P0023</guid>
        
        
        <category>tutorial</category>
        
        <category>WP99</category>
        
      </item>
    
      <item>
        <title>Sparse sources identification through adjoint localization algorithm</title>
        <description>Model Problem

We consider the numerical approximation of the inverse problem for the linear advection-diffusion equation,



with $d$ being the diffusivity of the material and $v$ the direction of the advection.

Given a final time $T&amp;gt;0$ and a target function $u^\ast$ the aim is to identify the initial condition $u_0$ such that the solution, at time $t=T$, reaches the target $u^*$ or gets as close as possible to it. We assume that the initial condition $u_0$ is characterized as a combined set of sparse sources. This means that $u_0$ is a linear combination of unitary deltas with certain and possibly different weights, i.e:



We formulate the inverse problem using optimal control techniques. In particular, we consider the minimization of the following functional:



Space and time discretization

Letting $\textbf{u} : [0, T] \rightarrow \mathbb{R}^s$ where $s$ is the number of grid points on $\Omega$, we can write a general finite element (FE) discretization of the diffusion–advection equation in \eqref{modeleq} in a compact form as:



In order to get a time discretized version of the previous equation, we apply implicit Euler method with stepsize $\Delta t := T/N$ where $N$ is the total number of time steps. The numerical approximations to the solution are given by the vectors $\textbf{u}^n \approx \textbf{u}(t_n) \in \mathbb{R}^s$ with respect to the index $n=i\Delta t$ for $i=1,2,..,N$. Therefore, the fully discrete version of model equation is as follows,



Adjoint Algorithm for sparse source identification

The algorithm to be presented in this work for the sparse source identification of the linear diffusion-advection equation based on the adjoint methodology consists of two steps. Firstly, we use the adjoint methodology to identify the locations of the sources. Secondly, a least squares fitting is applied to find the corresponding intensities of the sources.

We have considered here a two-dimensional example with several sources to be identified in a multi-model environment. This means that the left half ($\Omega_1 = [0,1] \times [0,1]$) and the right half ($\Omega_2 = [1,2] \times [0,1]$) of the domain are modelled with different equations. In particular, the heat equation is used on $\Omega_1$ and the diffusion–advection equation is used on $\Omega_2$.

The initialization parameters look as follows:

N=30; %% space discretization points in y-direction
dx=1/(N+1); %% mesh size
t0=0; %% initial time
tf=0.1; %% final time
n=5; %% time discretization points
dt=(tf-t0)/n; %% stepsize
TOL=1e-5; %% stopping tolerance

d1=0.05; %% diffusivity of the material on the left sudomain
d2=0.05; %% diffusivity of the material on the left sudomain

%% advection components
vx=0;
vy=-3;

tau=dx^4; %% regularization parameter
epsilon=0.1; %% stepsize of the gradient descent method


We now compute the FE discretization matrices $M$, $A$ and $V$ that are respectively the mass matrix, the stiffness matrix and the advection matrix. For the FE discretization we assume equidistant structured meshes. In particular we use triangular elements and the classical pyramidal test functions are employed.

[M,A,V] = computeFEmatrices(N,d1,d2,vx,vy); %% Compute FE discretization matrices


A reference initial condition is chosen and we compute using the FE discretization specified above and implicit Euler in time its corresponding final state at time $T$. This final state will be considered the initial data of the inverse problem to be solved and we name it the target function $u^*$ as mentioned previously.

U0_ref = initial_deltas(N); %% computes reference initial condition

[U_target,u_target] = compute_target(U0_ref,N,n,dt,M,A,V); %% Compute target distribution


We now call the algorithm that estimates the initial condition $u_0$ using as a initial data the target function $u^*$. As mentioned before, this algorithm consists of two steps.

Firstly, the classical adjoint methodology that minimizes the functional $J(u_0)$ subject to the diffusion-advection equation is used. The iterative optimization algorithm employed is the classical gradient descent method. However, although this iterative procedure finds quite accurately the locations of the sources, it does not recover the sparse character of the initial condition. This is not suprising because the recovered initial data comes from solving the adjoint problem which is basically a diffusive process that smoothes out its state. Consequently, a second procedure is needed to project the obtained non sparse initial condition into the set of admissible sparse solutions.

As the initial condition $u_0$ is assumed to be a linear combination between the locations and the intensities, once we have fixed the locations using the adjoint methodology we can solve a least squares problem to get the remaining intensities. We assemble a matrix $\textbf{L} \in \mathbb{R}^{s \times l}$ where at each column we have the forward solution for a single unitary delta placed at each of the locations already identified. We then solve the following linear system of equations for the vector of unknowns $\alpha = (\alpha_1, \alpha_2, …, \alpha_l)^T$:



to find the intensities vector $\alpha$.

Adjoint algorithm for sparse source identification (algorithm 4)

U0 = SparseIdentification(u_target,TOL,dt,n,N,M,A,V,epsilon,tau);


Finally, the final state at $T$ is computed using as a initial condition the estimated sparse sources identified with our algorithm.

Compute final state with the recovered initial condition

[UF,u_final] = compute_target(U0,N,n,dt,M,A,V);


We can see the evolution recovered



We now visualize the numerical results. Plots on the left side show the reference initial solution and the given target. Similarly, plots on the right side show the recovered initial condition and the distribution at the final time $T$ produced by the recovered initial sources.

One can observe the difference between the two models (the heat equation on $\Omega_1$ and the diffusion-advection on $\Omega_2$) in the two figures at the bottom where the initial sources on $\Omega_2$ move downwards at the same time as they dissipate while the initial sources on $\Omega_1$ only dissipate without displacement.

xplot = linspace(0,2,2*N+3); %% space grid w.r.t component x
yplot = linspace(0,1,N+2); %% space grid w.r.t component y
%%
figure('unit','norm','pos',[0.25 0.1 0.5 0.8])
subplot(3,2,1)
surf(xplot,yplot,U0_ref)
shading interp;colorbar;colormap jet
title('Reference initial state (front view)')
subplot(3,2,2)
surf(xplot,yplot,U0)
shading interp;colorbar;colormap jet
title('Recovered initial state (front view)')
subplot(3,2,3)
pcolor(xplot,yplot,U0_ref)
shading interp;colorbar;colormap jet
title('Reference initial state (above view)')
subplot(3,2,4)
pcolor(xplot,yplot,U0)
shading interp;colorbar;colormap jet
title('Recovered initial state (above view)')
subplot(3,2,5)
pcolor(xplot,yplot,U_target)
shading interp;colorbar;colormap jet
title('Given target u^*')
subplot(3,2,6)
pcolor(xplot,yplot,UF)
shading interp;colorbar;colormap jet
title('Recovered final state')



</description>
        <pubDate>Fri, 14 Jun 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0014</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0014</guid>
        
        
        <category>tutorial</category>
        
        <category>WP04</category>
        
      </item>
    
      <item>
        <title>Controllability of a Class of Infinite Dimensional Systems with Age Structure</title>
        <description>
 Infinite dimensional dynamical systems coupling age structuring with diffusion appear naturally in
 population dynamics, medicine or epidemiology. A by now classical example is the Lotka-Mckendrick system with spatial diffusion. The
 aim of this blog is to study contaollability properties of such age structured models in an unified manner. 
 


Let $A : \mathcal{D}(A) \to X$  be the generator of a $C^0$ semigroup $\mathbb{S}$ on the Hilbert space $X$ and let $U$ be
another Hilbert space. Both $X$ and $U$ will be identified with their duals. Let $B$ be a (possibly unbounded) linear operator
from $U$ to $X$, which is supposed to be an admissible control operator for  $\mathbb{S}$.
In the examples we have in mind, the above spaces and operators describe the dynamics of a system without age structure.
In particular, $X$ is the state space and $U$ is the control space.
The corresponding age structured system is obtained by first extending these spaces to
\begin{equation} \label{input-space}
\mathcal{X} = L^{2}(0,a_{\dagger}; X), \quad \mathcal{U} = L^{2}(0,a_\dagger;U).
\end{equation} 
where $a_\dagger&amp;gt;0$ denotes the maximal age individuals can attain. Let $p(t) \in \chi$ be the distribution density of the
individuals with respect to age $a\geqslant 0$ and at some time $t \geqslant 0.$  Then the abstract version of the Lotka-McKendrick
system to be considered in this paper writes:
 \begin{equation} \label{eq:main}
\begin{cases}
\displaystyle \frac{\partial p}{\partial t} + \frac{\partial p}{\partial a} - A p  + \mu(a) p =  \mathcal{\chi}_{(a_{1},a_{2})} B u, &amp;amp; t \geqslant 0, a \in (0,a_\dagger), \\
\displaystyle p(t,0)  = \displaystyle \int_{0}^{a_\dagger} \beta(s) p(t,s) \; {\rm d}s, &amp;amp; t \geqslant 0, \\
\displaystyle p(0,a) = p_{0},
\end{cases}
\end{equation} 
where $\mathcal{\chi}$ is the characteristic function of the interval $(a_{1}, a_{2})$ with
$0 \leqslant a_{1} &amp;lt; a_{2} \leqslant a_\dagger$ and $p_{0}$ is the initial population density.
In the above system, the positive function $\mu:[0,a_\dagger] \to \mathbb{R}_{+}$ denotes the
natural mortality rate of individuals of age $a.$ We denote by $\beta: [0,a_\dagger] \to \mathbb{R}_{+}$
the positive function describing the fertility rate at age $a.$ We assume that the fertility rate $\beta$ a
nd the mortality rate $\mu$ satisfy the conditions

(H1) $\beta \in L^\infty[0, a_\dagger], \; \beta \geqslant 0$ for almost every $a \in [0,a_\dagger].$
(H2) $\mu \in L^1_{loc}[0, a_\dagger], \; \mu \geqslant 0$ for almost every $a \in [0,a_\dagger].$
(H3) $\displaystyle \int_0^{a_\dagger} \mu(a) \ {\rm d} a = \infty.$


Before we state our result, let us introduce the notion of null controllability of the pair $(A,B).$

We say that a pair $(A,B)$ is null-controllable in time $\tau,$ if for every $z_{0} \in X$ there exists a control $u \in L^{2}(0,\tau,U)$ such that, the solution of the system
\begin{equation*}
\dot z(t) = A z(t) + Bu(t) \quad t \in [0,\tau], \qquad z(0) = z_{0},
\end{equation*}
satisfies $z(\tau) = 0.$


We prove the following result

Assume that  $\beta$ and $\mu$ satisfy the conditions (H1)-(H3) above. Moreover,
suppose that the fertility rate $\beta$ is such that
\begin{equation} \label{eq:beta}
\beta(a)= 0 \mbox{ for all } a \in (0,a_{b}),
\end{equation}
for some $a_b\in (0,a_\dagger)$ and that $a_1 &amp;lt; a_b$.
Let us assume that the pair $(A,B)$ is null controllable in arbitrary time. 
Then for every $\tau &amp;gt; a_{1} + a_{\dagger}-a_{2}$ and for every $p_{0} \in \chi$ there exists  a control $v \in L^{2}(0,\tau;\mathcal{U})$ such that the solution $p$ of \eqref{eq:main} satisfies
\begin{equation}
p(\tau,a) = 0 \mbox{ for all }  a \in (0,a_\dagger).
\end{equation} 
 

It is well known that null controllability is equivalent to final state observability of the adjoint system
(see for instance [4, Section 11.2]). The adjoint of the above system reads as
\begin{equation} \label{eq:adj}
\begin{cases}
\displaystyle \frac{\partial q}{\partial t} - \frac{\partial p}{\partial a} - A^* p  + \mu(a) p - \beta(a) q(t,0) =  0, &amp;amp; t \geqslant 0, a \in (0,a_\dagger), \\
\displaystyle q(t,a_\dagger)  = 0, &amp;amp; t \geqslant 0, \\
\displaystyle q(0,a) = q_{0},
\end{cases}
\end{equation} 
where $A^*$ is the adjoint of  $A.$  In order to prove Theorem 1, it is enough to prove the following

 Let us assume the hypothesis of Theorem 1. Then for every $\tau &amp;gt; a_1 + a_\dagger - a_2$ there exists $k_\tau &amp;gt; 0$ such that
 the solution $q$ of \eqref{eq:adj} satishfies
 \begin{equation} \label{eq:est-adj}
 \int_0^{a_\dagger} \|q(\tau, a)\|^2_X \ da  \leqslant k_\tau^2 \int_0^\tau \int_{a_1}^{a_2} \left\|B^*q(t,a) \right\|^2_U \ da dt, \qquad \qquad (q_0 \in \mathcal{X}).
 \end{equation}
 
Let us give an idea of the proof. We combine characteristics method with final state observability of the pair $(A^*, B^*).$ For siplicity
in the presentation, let us assume that $\mu = 0,$ $a_1 = 0,$ $a_2 &amp;lt; a_b$ and $\tau = 2a_\dagger.$
The detail proof of Theorem 1, in a slightly more general case, can be found in [3]. Integrating along the characteristic lines,
it is not very difficult to see that
\begin{equation*}
q(t,a) = \int_{t + a - a_\dagger}^{t} e^{(t-s)A^*} \beta(a+ \tau - s) q(s, 0) \ ds, \qquad t &amp;gt; a_\dagger. 
\end{equation*}
Therefore,
\begin{equation} \label{est0}
\int_0^{a_\dagger} \|q(\tau, a)\|^2_X \ da \leqslant C \int_{a_\dagger}^\tau \left\|q(t,0) \right\|^2_X \ dt. 
\end{equation}
Thus to prove \eqref{eq:est-adj}, we need to estimate the right hand side of the above estimate. We now make use of the condition
\eqref{eq:beta}.  Note that, due to this condition, $q$ satisfies
\begin{equation} \label{eq:adj-2}
\displaystyle \frac{\partial q}{\partial t} - \frac{\partial q}{\partial a} - A^* p  + \mu(a) p =  0,  \quad  t \geqslant 0, a \in (0,a_2).
\end{equation}
For a.e. $t \in (a_\dagger, \tau),$ we define $w(s) = q(s, t- s),$ $s \in (t - a_2, t).$ Then $w$ solves
\begin{equation*}
\displaystyle \frac{\partial w}{\partial s} - A^* w = 0, \quad s \in (t - a_2, t). 
\end{equation*}
Since the pair $(A, B)$ is null controllable in any time, or equivalently the pair $(A^*, B^*)$ is final-state observable in any
time, there exists a constant
$C &amp;gt; 0,$ depending only on $a_2, A$ and $B$  such that
\begin{equation*}
\left\| w(t) \right\|_{X}^2 \leqslant C \int_{t-a_2}^t \left\| B^* w(s) \right\|^2_U \ ds. 
\end{equation*}
Coming back to $q$ and integrating over $[a_\dagger, \tau]$ with respect to $t,$ we have that
\begin{equation}
\int_{a_\dagger}^\tau \|q(t,0)\|^2_X \ dt \leqslant \int_{0}^\tau \int_{0}^{a_2} \left\|B^* q(t,a)\right\|^2  \ da dt. 
\end{equation}
Combining the above estimate together with \eqref{est0}, we get \eqref{eq:est-adj}. In the two figures below, we ilustrate the
estimate of $q(t,0)$ and the final-state observability of $q$ in the general case.



  
   
     
  
   Fig.1 - An ilustration of the estimate of $q(t,0).$ We apply final state observability of $(A^*, B^*)$ along
  the characteristics. Since we want to estimate $q(t,0),$ we consider the trajectory $\gamma(s) = (t-s,s),$
  $s \leqslant t \leqslant \tau$ (or equivalently the backward characteristics starting from $(t,0).$) 

    

    
  
   Fig.2 - An illustration of the final-state observability at time $\tau &amp;gt; a_1 + a_\dagger - a_2$: For
  $ a \in (0,a_2 - \varepsilon)$ (blue region) the backward characteristics starting from $t = \tau$ enters the
  observation domain.  For $a \in (a_2 - \varepsilon, a_\dagger)$, the backward characteristics (green region)
  hits the line $a = a_\dagger$, gets renewed by the renewal condition $\beta(a)q(t,0)$ and then enters the observation
  domain (purple region).
 
 
 
 

Examples
We consider two examples :  1) The classical Lotka-McKendrick system and 2) Lotka-Mckendrick system with diffusion.

 1)  The classical Lotka-McKendrick system : Let us choose $X = \mathbb{R},$ $A=0$ and $B=1.$ Then the system \eqref{eq:main} reduces
 to  classical Lotka-McKendrick system. By Theorem 1, this system is null controllable in time $\tau &amp;gt; a_1 + a_\dagger - a_2.$ A
 similar result was obtained in [1]. 



 2)  The Lotka-McKendrick system with spatial diffusion  : Let $\Omega$ be a smooth bounded domain in $\mathbb{R}^3$ and
 $\omega \subseteq \Omega.$ Let us consider 
 \begin{equation*}
 X = L^2(\Omega), \quad A = \Delta , \quad \mathcal{D}(A) =
 \left\{ f \in H^2(\Omega) \mid \displaystyle \frac{\partial f}{\partial n} = 0\right\}, \quad B = \chi_{\omega}.
 \end{equation*}
 This corresponds to the Lotka-McKendrick model with spatial diffusion ([2]). It is known that, the pair $(A, B)$ or equivalently
 the heat equation with localized interior control, is null controllable in any time. Thus we can apply Theorem 1 to conclude that
 the system is null controllable in time $\tau &amp;gt; a_1 + a_\dagger - a_2.$
 
 Several other applications can be found in [3].

Numerical Simulations
 We now present some numerical siimulations of the controlled trajectory.  We shall consider the case  $A = 0$
and $B = 1,$ i.e the classical Lotka-McKendrick system. We also consider the case $a_1 = 0$ and $a_2 = a_{\dagger} =4,$ i.e., the control
acts everywhere with respect to age variable. We present numerical simulations in the following two scenarios : 1) null controllabilty,
2) controllability to a steady state.
Null Controllability
By Theorem 1 we know that system (1) is null controllable in any time. In the following
video, we see the evolution of unctrolled trajectory and controlled trajectories to zero  with controllability time $t=2$ and $t =5.$ 

   
  


 We now plot the control functions. In the following two videoes, we see the evolution of the control
 functions at controllability time $t = 2$ and $t =5$
 respectively.



  
   
    
   
  
    
   &amp;nbsp; &amp;nbsp; 

   
   
  
 
 
 
Controllability to a steady state
 If we choose $\beta$ and $\mu$ such that $R = \int_0^{a_\dagger} \beta(a) e^{-\int_0^r \mu(r) \ dr } da = 1,$ then
$p_s(a) = \alpha e^{-\int_0^r \mu(r) \ dr }$ with $\alpha \in (0,\infty)$ is a steady state to the system (1) with zero steady control.
Thus we can control to these trajectories also. In the following example, we have chosen $a_{\dagger} = 4,$ $a_b = 1,$ $\alpha = 1.25$ and
\begin{equation*}
\beta(a) = \begin{cases}
0 &amp;amp; \mbox{ if } a \in [0,1), \\
\frac{1}{3}e^{.05 a^2} &amp;amp;  \mbox{ if } a \in [1,4],
\end{cases}
\qquad
\mu(a) = .1 a.
\end{equation*}
In this case we can verify that $R=1$ and $p_s(a) = 1.25 e^{-.05 a^2}.$ As before we take control everywhere, thus the system is
controllable to the trajectory $p_s(a)$ in any time.  In the following video,
we see the evolution of unctrolled trajectory and controlled trajectories to the steady state $p_s(a)$ with
controllability time $t=2$ and $t =5.$


   
  
 As before, in the following two videos we see the evolution of the control functions at time $t =2$ and $t=5$ respectively. 



  
   
    
   
  
    
   &amp;nbsp; &amp;nbsp; 

   
   
  
 
 
 



References
 [1] D. Maity,  On the Null Controllability of the Lotka-Mckendrick System, Submitted. 
 [2] D. Maity, M. Tucsnak and E. Zuazua,  Controllability and positivity constraints in population dynamics with age
structuring and diffusion , Journal de Math&amp;eacute;matiques Pures et Appliqu&amp;eacute;s. In press, 10.1016/j.matpur.2018.12.006.

 [3] D. Maity, M. Tucsnak and E. Zuazua,  Controllability of a Class of Infinite Dimensional
Systems with Age Structure. Submitted.

 [4] M. Tucsnak and G. Weiss,  Observation and control for operator semigroups,   
Birkh&amp;auml;user Advanced Texts: Basler Lehrbu&amp;uuml;cher. [Birkh&amp;auml;user Advanced Texts: Basel Textbooks], Birkh&amp;auml;user Verlag, Basel, 2009. 

</description>
        <pubDate>Fri, 07 Jun 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp05/WP99-P0030</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp05/WP99-P0030</guid>
        
        
        <category>tutorial</category>
        
        <category>WP05</category>
        
      </item>
    
      <item>
        <title>Finite Element approximation of the one-dimensional fractional Laplacian</title>
        <description>We describe here a Finite Element algorithm for the approximation of the one-dimensional fractional Laplacian $(-d_x^2)^s$ on the interval $(-L,L)$, $L&amp;gt;0$ and for the numerical resolution of the following fractional Poisson equation



This algorithm has also been emploied in [2,3] for the numerical controllability of fractional parabolic problems (see our previous entries in the DyCon Blog, WP3_P0001 and WP3_P0022).

We recall that the fractional Laplacian is defined, for all $s\in(0,1)$ and any function $u$ regular enough, as the following singular integral



with $c_s$ an explicit normalization constant given by



$\Gamma$ being the usual Euler Gamma function.

The starting point of the FE method is the definition of a bilinear form associated to the fractional Laplace operator $(-d_x^2)^s$ and the introduction of the variational formulation corresponding to the elliptic problem \eqref{Fl_Poisson}.

This variational formulation is given as follows: find $u\in H_0^s(-L,L)$ such that the identity



is satisfied for any function $v\in H_0^s(-L,L)$. In \eqref{variational}, with $H_0^s(-L,L)$ we indicate the space



$H^s(\mathbb{R})$ being the usual fractional Sobolev space. Moreover, the bilinear form



is defined as



Let us describe how, starting from the above bilinear form, we can obtain our FE approximation. For simplicity, in what follows we will take $L=1$, although the methodology remains the same for any real $L&amp;gt;0$.

Let us take a uniform partition of the interval $(-1,1)$ as follows:



with $x_{i+1}=x_i+h$, $i=0,\ldots N$. We call $\mathfrak{M}$ the mesh composed by the points ${x_i\,:\, i=1,\ldots,N}$, while the set of the boundary points is denoted $\partial\mathfrak{M}:={x_0,x_{N+1}}$.

Now, define $K_i:=[x_i,x_{i+1}]$ and consider the discrete space



where $\mathcal{P}^1$ is the space of the continuous and piece-wise linear functions. Hence, we approximate \eqref{variational} with the following discrete problem: find $u_h\in V_h$ such that



for all $v_h\in V_h$. If now we indicate with



a basis of $V_h$, it will be sufficient that the above equality is satisfied for all the functions of the basis, since any element of $V_h$ is a linear combination of them. Therefore the problem takes the following form



Clearly, since $u_h\in V_h$, we have $u_h(x) = \sum_{j=1}^N u_j\phi_j(x)$, where the coefficients $u_j$ are, a priori, unknown. In this way, \eqref{WFD} is reduced to solve the linear system $\mathcal A_h u=F$, where the stiffness matrix $\mathcal A_h\in \mathbb{R}^{N\times N}$ has components



while the vector $F\in\mathbb{R}^N$ is given by $F=(F_1,\ldots,F_N)$ with



Moreover, the basis



that we will employ is the classical one in which each $\phi_i$ is the tent function with $supp(\phi_i)=(x_{i-1},x_{i+1})$ and verifying $\phi_i(x_j)=\delta_{i,j}$. In particular, for $x\in{x_{i-1},x_i,x_{i+1}}$ the $i^{th}$ function of the basis is explicitly defined as (see Figure 1)




Figure 1. Basis function $\phi_i$ on its support $(x_{i-1},x_{i+1})$.

We now start building the stiffness matrix $\mathcal A_h$ approximating the fractional Laplacian. This will be done in three steps, since the values of the matrix can be computed differentiating among three well defined regions: the upper triangle, corresponding to $j\geq i+2$, the upper diagonal corresponding to $j=i+1$ and the diagonal, corresponding to $j=i$. In each of these regions the intersections among the support of the basis functions are different, thus generating different values of the bilinear form.

We present below an abridged explanation of how to compute the entries of $\mathcal{A}_h$. Complete details may be found in [2].

Step 1: $j\geq i+2$

In this case we have $supp(\phi_i)\cap supp(\phi_j) =\emptyset$ (see also Figure 2). Hence, \eqref{stiffness_nc} is reduced to computing only the integral




Figure 2. Basis functions $\phi_i(x)$ and $\phi_j(x)$ for $j\geq i + 1$. The supports are disjoint.

Taking into account the definition of the basis function \eqref{basis_fun}, the integral \eqref{elem_noint_app} becomes



Let us introduce the following change of variables:



Then, rewriting (with some abuse of notations since there is no possibility of confusion) $\hat{x}=x$ and $\hat{y}=y$, we get



This integral can be computed explicitly, and we obtain



Notice that, when $s=1/2$, both the numerator and the denominator of the expression above are zero. In this case, the value of $a_{i,j}$ can be obtained by taking the limit $s\to 1/2$ and we get



if $k\neq 2$ and



Step 2: $j= i+1$
This is the most cumbersome case, since it is the one with the most interactions between the basis functions (see Figure 3).


Figure 3. Basis functions $\phi_i(x)$ and $\phi_{i+1}(x)$. In this case, the intersection of the supports is the interval $[x_i,x_{i+1}]$.

Using the symmetry of the integral with respect to the bisector $y=x$, we have



Also in this case, the terms $Q_i$, $i=1,\ldots,6$, can be computed explicitely, by noticing also the following facts:


  $Q_1=Q_6=0$ since $\phi_i = 0$ on the domain of integration.
  $Q_2=Q_5$ due to symmetry.


Then, the elements $a_{i,i+1}$ are given by the sum $2Q_2+Q_3+Q_4$ and we have



Step 3: $j= i$
As a last step, we fill the diagonal of the matrix $\mathcal A_h$, which collects the values corresponding to $\phi_i(x)=\phi_j(x)$ (see Figure 4).


Figure 4. Basis functions $\phi_i(x)=\phi_j(x)$.

In this case, we have



Once again, the terms $R_i$, $i=1,\ldots,7$ can be computed explicitely, by taking also into account that $R_1=R_3=R_6=R_7=0$ because the corresponding integration domains are all away from the support of the basis functions.

The result of these computations gives the values



Step 4: building of $\mathcal A_h$

Summarizing, we have the following values for the elements of the stiffness matrix $\mathcal{A}_h$: for $s\neq 1/2$



For $s=1/2$, instead, we have



Numerical results

We present the numerical simulations corresponding to the algorithm previously described.

First of all, we test numerically the accuracy of our method for the resolution of the elliptic equation \eqref{Fl_Poisson} by applying it to the following problem



In this particular case, the unique solution $u$ can be computed exactly and it is given by



where $\chi_{(-1,1)}$ indicates the characteristic function of the interval $(-1,1)$.

The solution of \eqref{poisson} in the case  $s=0.1$ and $N=50$ is computed with the following script which emploies the function FEFractionalLaplacian.m ginving the FE discretization of $(-d_x^2)^s$.

s = 0.1;
N = 50;
L = 1;

x = linspace(-L,L,N+2);
x = x(2:end-1);
h = x(2)-x(1);

f = @(x) 1+0*x;
Phi = @(x) 1-abs(x);

F = zeros(N,1);

for i=1:N
    xx = linspace(x(i)-h,x(i)+h,N+1);
    xx = 0.5*(xx(2:end)+xx(1:end-1));
    B1 = f(xx).*(Phi((xx-x(i))/h));
    F(i) = ((2*h)/N)*sum(B1); 
end

A = FEFractionalLaplacian(s,L,N);
sol = A\F;


In Figure 5, we show a comparison for different values of $s$ between the exact solution \eqref{real_sol} and the computed numerical approximation.


Figure 5. Real and numerical solution.

One can notice that when $s=0.1$ (and also for other small values of s), the computed solution is to a certain extent different from the exact solution. Notwithstanding, it is well-known that the notion of trace is not defined for the spaces $H^s(-1,1)$ with $s\leq 1/2$. Hence, it is somehow natural that we cannot expect a point-wise convergence in this case.

Furthermore, the accuracy of our algorithm is validated by a simple error analysis.

The computation of the error in the space $H_0^s(-1,1)$ can be readily done by using the definition of the bilinear form, namely



where have used the orthogonality condition $a(v_h,u-u_h)=0$ for all $v_h \in V_h$.

For this particular test, since $f\equiv 1$ in $(-1,1)$, the problem is therefore reduced to



where the right-hand side can be easily computed, since we have the closed formula



and the term corresponding to $\int_{-1}^1u_h$ can be carried out numerically. Moreover, we have the following convergence result.

Theorem [2] For the solution $u$ of \eqref{WFD} and its FE approximation $u_h$ given by \eqref{WFD}, if $h$ is sufficiently small, the following estimates hold



where $\mathcal{C}$ is a positive constant not depending on $h$.

In Figure 6, we present the computational errors evaluated for different values of $s$ and $h$, which can be obtained with the following function.

function [h,e] = error_fl(s,N)

x = linspace(-1,1,N+2);
x = x(2:end-1);
h = x(2)-x(1);

f = @(x) 1+0*x;
Phi = @(x) 1-abs(x);

F = zeros(N,1);

for i = 1:N
    xx = linspace(x(i)-h,x(i)+h,N+1);
    xx = 0.5*(xx(2:end)+xx(1:end-1));
    B1 = f(xx).*(Phi((xx-x(i))/h));
    F(i) = ((2*h)/N)*sum(B1); 
end

A = FEFractionalLaplacian(s,1,N);
sol = A\F;
val = pi/(2^(2*s)*gamma(s+0.5)*gamma(s+1.5));
valnum = h*sum(sol);
e = sqrt(val-valnum);



Figure 6. Computational error in logarithmic scale in terms for different values of $s$.

The rates of convergence shown are of order (in $h$) of $1/2$, and this convergence rate is maintained also for small values of $s$. This confirms the accuracy of our approximation. In particular, the behavior shown in Figure 6 is not in contrast with the known theoretical results.

References

[1] G. Acosta, F. Bersetche and J. P. Borthagaray, A short FE implementation for a 2d homogeneous Dirichlet problem of a Fractional Laplacian. Comput. Math. Appl., Vol. 74, No. 4 (2017), pp. 784-816.

[2] U. Biccari and V. Hernández-Santamarı́a, Controllability of a one-dimensional fractional heat equation: theoretical and
numerical aspects. IMA J. Math. Control. Inf, to appear, 2018.

[3] U. Biccari, M. Warma and E. Zuazua, Controllability of the one-dimensional fractional heat equation under positivity constraints. Submitted.
</description>
        <pubDate>Fri, 07 Jun 2019 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp05/WP99-P0024</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp05/WP99-P0024</guid>
        
        
        <category>tutorial</category>
        
        <category>WP05</category>
        
      </item>
    
  </channel>
</rss>
