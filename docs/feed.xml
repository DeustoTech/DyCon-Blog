<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DyCon Blog</title>
    <description>Welcome to the web interface of DyCon Toolbox, the computational platform developed within the &lt;a href='https://cmc.deusto.eus/dycon/' target='_blank'&gt;ERC DyCon - Dynamic Control&lt;/a&gt; project.</description>
    <link>https://deustotech.github.io/DyCon-Blog/</link>
    <atom:link href="https://deustotech.github.io/DyCon-Blog/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 02 Apr 2020 11:11:37 +0200</pubDate>
    <lastBuildDate>Thu, 02 Apr 2020 11:11:37 +0200</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Inverse problem for Hamilton-Jacobi equations</title>
        <description>Background and motivation

We consider the initial-value problem for a Hamilton-Jacobi equation of the form



where $u_0\in C^{0,1}(\mathbb{R}^n)$ and the Hamiltonian $H: \mathbb{R}^n\rightarrow\mathbb{R}$ is assumed to satisfy the following hypotheses:



Here, the unknown $u$ is a function $[0,T]\times\mathbb{R}^n\longrightarrow\mathbb{R}$,
and the inequality $H_{pp}(p)&amp;gt;0$ means that the Hessian matrix of $H$ at $p$ is positive definite.
The study of Hamilton-Jacobi equations
arises in the context of optimal control theory and calculus of variations,
where the value function satisfies, in a weak sense, a Hamilton-Jacobi equation.
In this context, Hamilton-Jacobi equations have applications in a wide range of fields such as
economics, physics, mathematical finance, traffic flow and geometrical optics.

Our goal in this tutorial is to study the inverse design problem associated to (HJ).
More precisely, for a given target function $u_T$ and a time horizon $T&amp;gt;0$,
we want to construct all the initial conditions $u_0$ such that the viscosity solution of (HJ) coincides with $u_T$ at time $T$.

The study of this problem can also be motivated by considering the following question:

Given an observation of the solution to (HJ) at time $T&amp;gt;0$, can we construct all the possible initial data that agree we the observation at time $T$ ?

For this purpose, for a fixed $T&amp;gt;0$, 
we define the following nonlinear operator, which associates to any initial condition $u_0$, the function
$u(T,\cdot)$, where $u$ is the viscosity solution of (HJ):



The inverse design problem that we are considering is then reduced to, for $u_T$ and $T&amp;gt;0$ given,
characterize all the initial conditions $u_0$ satisfying $S_T^+ u_0 =u_T$.

Reachability of the target

The first thing one notices when addressing this problem is that not all the Lipschitz targets are reachable.
Indeed, as it is well known, the viscosity solution to (HJ) is always a semiconcave function (see [2,4]). 
Therefore, an obvious necessary (but not sufficient) condition for the reachability of $u_T$ is that it must be a semiconcave function.
See Figures 1 and 2 for some examples of unreachable targets in dimension 1 and 2 respectively. Observe that, in particular, they are not semiconcave functions.



  




 Figure 1:  Two unreachable target functions in dimension 1.


We then start our study by characterizing the set of targets that are reachable.
In other words, for a given $u_T$ and $T&amp;gt;0$, we aim to determine whether or not there exists at least one initial condition $u_0$ satisfying $S_T^+ u_0=u_T$.
The natural candidate is the one obtained by reversing the time in the equation, considering $u_T$ as terminal condition. Here, we have to consider the class of backward viscosity solutions, for which the terminal-value problem associated to (HJ) is well-posed (see for example [1]).

We then define the backward operator as follows:



which associates to any terminal condition $u_T$, the function $w(0,\cdot)$,
i.e. the unique backward viscosity solution at time $0$, with terminal condition $u_T$.



  




 Figure 2:  Two unreachable target functions in dimension 2.


Let us introduce, for a target $u_T$ and a time horizon $T&amp;gt;0$, the set



of initial conditions $u_0$ satisfying $S_T^+ u_0 = u_T$.
We say that a target $u_T$ is reachable for the time horizon $T$ if $I_T(u_T)\neq \emptyset$.
We can equivalently say that $u_T$ is an admissible observation at time $T&amp;gt;0$.

Here we give a result that identifies the set of reachable targets (or admissible observations) at time $T$,
with the fix-points of the composition operator $S_T^+\circ S_T^-$.



Theorem 1: Let $H$ satisfy (2), $u_T\in C^{0,1}(\mathbb{R}^n)$ and $T&amp;gt;0$. Then, the set $I_T(u_T)$ is nonempty 
if and only if  $S_T^+ \left( S_T^- u_T\right) = u_T.$



In other words, a target is reachable if and only if the initial condition $\tilde{u}_0:= S_T^- u_T$, obtained by applying the backward operator, satisfies $\tilde{u}_0\in I_T(u_T)$.
Or equivalently, an observation $u_T$ at time $T$ is admissible if and only if $S_T^+ \left( S_T^- u_T\right) = u_T.$

Projection on the set of reachable targets

In the case the observation $u_T$ at time $T$ is not admissible, maybe due to noise effects or to errors in the measurements, we can actually project it on the set of admissible observations by applying the operator $S_T^+\circ S_T^-$. For a given $u_T\in C^{0,1}(\mathbb{R}^n)$, we define this projection as follows:



Observe that $I_T(u_T^\ast)\neq \emptyset$ for any $u_T\in C^{0,1}(\mathbb{R}^n)$. 
Indeed, $\tilde{u}_0:=S_T^- u_T\in I_T (u_T^\ast)$ by definition.



  

  




 Video 1:  Projection of the examples in Figure 1 on the set of admissible observations at times $T=0.3$ and $T=0.8$ respectively.


In the Videos 1 and 2, we observe how the examples in Figures 1 and 2 are projected on the set of reachable targets.
We recall that the projection is obtained by solving the problem (HJ) backward in time and the forward.
Observe that, when the time goes backward, the solution is semiconvex, while in the forward resolution, it becomes semiconcave. See [1,4] for the classical results concerning this regularizing effect and [2] for the definition and properties of semiconcave and semiconvex functions.



  

  




 Video 2:  Projection of the examples in Figure 2 on the set of admissible observations at time $T=1$.


Construction of initial data

Once we have projected the observation $u_T$ on the set of reachable targets at time $T$, we proceed to the construction of all the initial data $u_0$ satisfying $S_T^+ u_0 = u_T^\ast$.
This construction is contained in the recent work [3], and reads as follows:


Theorem 2: Let $H$ satisfy (2) and $T&amp;gt;0$.
Let $u_T\in C^{0,1}(\mathbb{R}^n)$ and define the functions



Then, for any $u_0\in C^{0,1} (\mathbb{R}^n)$, the two following statements are equivalent:

  $u_0\in I_T(u_T^\ast)$;
  $u_0(x)\geq \tilde{u}_0 (x), \ \forall x\in \mathbb{R}^n \quad \text{and} \quad u_0(x) = \tilde{u}_0(x), \ \forall x\in X_T(u_T^\ast),$


where $X_T(u_T^\ast)$ is the subset of $\mathbb{R}^n$ given by





In view of this result, we observe that, for a given admissible observation $u_T^\ast$ at time $T$,
the construction of the possible initial data can be carried out after obtaining the two following ingredients:

  the function $\tilde{u}_0 = S_T^- u_T^\ast$, obtained as the backward viscosity solution to (HJ) with terminal condition $u_T^\ast$;
  and the set $X_T(u_T^\ast)\subset \mathbb{R}^n$, that can be deduced from the differentiability points of $u_T^\ast$.


In Figures 3 and 4 below, we can see the scheme of the construction of all the initial data for the projection $u_T^\ast = S_T^+ (S_T^- u_T)$ of the non-admissible observations in Figure 1.
Observe that all the initial data in $I_T(u_T^\ast)$ must coincide with $\tilde{u}_0$ on the red region, while on the black region, they must be greater or equal than $\tilde{u}_0$.



  




 Figure 3:  Reconstruction scheme for the admissible target obtained in Video 1 at the left.




  




 Figure 4:  Reconstruction scheme for the admissible target obtained in Video 1 at the right.


An important consequence of Theorem 2 is that the initial data in $I_T(u_T^\ast)$ are not unique whenever $X_T(u_T^\ast) \neq \mathbb{R}^n$.
Indeed, the set $I_T(u_T^\ast)$ can be given in the following way:



In the case of the examples illustrated in Figures 3 and 4,
we can see in the videos 3 below, examples of different initial conditions whose solution coincides with $u_T^\ast$ at time $T$.
These exmaples have been generated randomly by adding to $\tilde{u}_0$ a nonegative a Lipschitz function vanishing in $X_T(u_T^\ast)$.



  
  




 Video 3:  Examples of initial data $u_0\in I_T(u_T^\ast)$ for the reconstruction schemes depicted in Figures 3 and 4.


A possible interpretation of this reconstruction result is that the initial datum can only be reconstructed in the region $X_T(u_T^\ast)$, where it is uniquely determined by $\tilde{u}_0$,
while in the region $\mathbb{R}^n\setminus X_T(u_T^\ast)$, only a lower bound can be deduced.
In this last region, we can say that the information of the initial data has been partially lost after the time-interval $[0,T]$.
In the case of smooth solutions (when they exists) we have backward uniqueness, i.e. $I_T(u_T)$ is either empty or a singleton (see [1]).
In this case it holds that $X_T(u_T)=\mathbb{R}^n$.

Next, we see the reconstruction schemes for the examples depicted in Video 2, that correspond to the projection on the set of admissible observations of $u_T$ from Figure 2.
Observe the different structure of the set $\mathbb{R}^n\setminus X_T(u_T^\ast)$ (white region in the plot at the right)
in the surrounding area of bumps and wells of $u_T^\ast$.

  Near a bump, the set where $u_T^\ast$ is not differentiable is an isolated point, and then, it produces a ball for the set $\mathbb{R}^n\setminus X_T(u_T^\ast)$.
  Near a well, $u_T^\ast$ is non-differentiable on a circumference. This produces an annulus for the set $\mathbb{R}^n\setminus X_T(u_T^\ast)$.




  




 Figure 5:  Reconstruction scheme for the admissible target obtained in Video 2 at the left.




  




 Figure 6:  Reconstruction scheme for the admissible target obtained in Video 2 at the right.


Evolution of $X_T(u_T)$

As we have seen in the previous section, given an admissible observation $u_T$ of the solution at time $T$,
the initial datum can only be reconstructed in a region $X_T(u_T)$, while in its complementary, only a lower bound can be obtained.
Here, we are interested in the  the role that $T$ plays in this reconstruction issue.
For this purpose, we fix an initial datum $u_0$ and then, for each $T&amp;gt;0$,
we construct the set $X_T(u_T)$, where $u_T := S_T^+ u_0$.

Our goal is to observe how $X_T(u_T)$ evolves as we increase $T$,
or in other words, we want to know what information from $u_0$ can be recoverd when the solution is observed at time $T$.
As we see in the following videos, $X_T(u_T)$ becomes smaller as we increase $T$, meaning that less information from the initial datum can be recovered if the observation is made after a long time interval.

In Figure 7 we fix two initial data in dimension 1.



  




 Figure 7:  Two initial data in dimension 1.


Next, we can see in in Video 4, how the reconstruction scheme for $u_0$ (at the left in Figure 7) evolves as we increase the observation time.
For $T$ small, very little information is lost, however, for $T$ large, $X_T(u_T)$ only intersects the nonpositive region of $u_0$, and therefore, the two bumps can no longer be reconstructed.



  




 Video 4:  Evolution of the target and the reconstruction scheme for the initial datum at the left in Figure 7.


In Video 5, we see the evolution of the reconstruction scheme for $u_0$ at the right in Figure 7.
In this case, $u_0$ is constituted of two wells, however, if the observation is made at time $T$ large enough, only the deepest well can be identified.



  




 Video 5:  Evolution of the target and the reconstruction scheme for the initial datum at the right in Figure 7.


We end this blog post with three examples in the two-dimensional case.
Here we have the three examples of initial data that we have considered.



  




 Figure 8:  Two initial data in dimension 2.


In Video 6, we see the evolution of the reconstruction scheme for our first 2-dimensional example.
In this case, $u_0$ has a well and a bump. We observe that, as we increase the observation time $T$,
the white region, which corresponds to $\mathbb{R}^n\setminus X_T(u_T)$ becomes bigger and bigger.
This corresponds to the fact that, when the observation is made after a long time-interval, less information from the initial datum can be recovered. In addition, if we look at $u_T^\ast$ and $\tilde{u}_0$ in the two first plots, we see that the bump disappears for $T$ large enough.
This implies that after a long time-interval, the bump cannot be reconstructed.



  




 Video 6:  Evolution of the reconstruction scheme for the first initial datum in Figure 8.


In Video 7, we see the evolution of the reconstruction scheme for the second example in Figure 8.
Here, the initial datum has two bumps of different height but same shape and size in their support.
Observe that, when $T$ is large enough, both bumps look the same, and therefore we cannot guess which one was taller at $t=0$. In fact, only the shape of their support can be reconstructed.



  




 Video 7:  Evolution of the reconstruction scheme for the second initial datum in Figure 8.


In Video 8, we see the evolution of the reconstruction scheme for the third example in Figure 8.
Here, the initial datum has two wells of different depth.
We observe the same behaviour as in Video 5, where only the information of the deepest well is preserved for $T$ large enough.



  




 Video 8:  Evolution of the reconstruction scheme for the third initial datum in Figure 8.


References

[1] E. Barron, P. Cannarsa, R. Jensen, C. Sinestrari,  Regularity of Hamilton-Jacobi equations when forward is backward. Indiana University mathematics journal, pages 385-409, 1999.

[2] P. Cannarsa, C. Sinestrari,  Semiconcave functions, Hamilton-Jacobi equations, and optmial control. volume 58. Springer Science &amp;amp; Business Media, 2004.

[3] C. Esteve, E. Zuazua,  The inverse problem for Hamilton-Jacobi equations and semiconcave envelopes. Preprint &amp;lt;https://arxiv.org/abs/2003.06914

[4] P.L. Lions,  Generalized solutions of Hamilton-Jacobi equations. volume 69. London Pitman, 1982.
</description>
        <pubDate>Fri, 03 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0010</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0010</guid>
        
        
        <category>tutorial</category>
        
        <category>WP04</category>
        
      </item>
    
      <item>
        <title>The Optimal control on the Kuramoto adaptative coupling model with DyCon Toolbox</title>
        <description>In this tutorial, we present how to use Pontryagin environment to control a consensus system that models the complex emergent dynamics over a given network. The control basically minimize the cost functional which contains the running cost and desired final state.

Setup

In this tutorial we need DyCon Toolbox, to install it we will have to write the following in our MATLAB console:

unzip('https://github.com/DeustoTech/DyCon-Computational-Platform/archive/master.zip')
addpath(genpath(fullfile(cd,'DyCon-toolbox-master')))

Model

The Kuramoto model describes the phases $\theta_i$ of active oscillators, which is described by the following dynamics:



Here, the first constant terms $\omega_i$ denote the natural oscillatory behaviors, and the interactions are nonlinearly affected by the relative phases. The amplitude and connections of interactions are determined by the coupling strength, $K_{i,j}$.

Control strategy

The control interface is on the coupling strength as follows:



This is a nonlinear version of bi-linear control problem for the Kuramoto interactions. The idea is as follows;


  
    There are $N$ number of oscillators, oscillating with their own natural frequencies.
  
  
    We want to make a collective behavior using their own decision process. The interaction is given by the Kuramoto model, or may follow other interaction rules. The network can be given or flexible with control.
  
  
    The cost of control will be related to the collective dynamics we want, such as the variance of frequencies or phases.
  


Numerical simulation

Here, we consider a simple problem: we control the all-to-all network system to get gathered phases at final time $T$. We first need to define the system of ODEs in terms of symbolic variables.

clear all
m = 50;  %% [m]: number of oscillators.

import casadi.*

t     = SX.sym('t');
symTh = SX.sym('y', [m,1]);   %% [y] :  phases of oscillators, $\theta_i$.
symOm = SX.sym('om', [m,1]);  %% [om]:  natural frequencies of osc., $\omega_i$.
symK  = SX.sym('K',[m,m]);    %% [K] :  the coupling network matrix, $K_{i,j}$.
symU  = SX.sym('u',[1,1]);    %% [u] :  the control function along time, $u(t)$.


syms Vsys;    %% [Vsys]: the vector fields of ODEs.
symThth = repmat(symTh,[1 m]);


Vsys = casadi.Function('V',{symOm,symK,symTh,symU}, ...
{ symOm + (symU./m)*sum(symK.*sin(repmat(symTh,[1 m]).' - repmat(symTh,[1 m])),2) }); 


The parameter $\omega_i$ and $K_{i,j}$ should be specified for the calculations. Practically, $K_{i,j}u(t) &amp;gt; \vert \max\Omega - \min\Omega \vert$ leads to the synchronization of frequencies. We normalize the coupling strength to 1, and give random values for the natural frequencies from the normal distribution $N(0,0.1)$. We also choose initial data from $N(0,\pi/4)$.

rng('default');
rng(1,'twister');
Om_init = normrnd(0,0.2,m,1);
Om_init = Om_init - mean(Om_init);  %% Mean zero frequencies
Th_init = normrnd(0,pi/8,m,1);


K_init = ones(m,m);                 %% Constant coupling strength, 1.
T = 5;                              %% We give enough time for the frequency synchronization.


symF = casadi.Function('F',{t,symTh,symU},{Vsys(Om_init,K_init,symTh,symU)});

tspan = linspace(0,T,110);
odeEqn = ode(symF,symTh,symU,tspan);
SetIntegrator(odeEqn,'RK4')
odeEqn.InitialCondition = Th_init;


We next construct cost functional for the control problem.

PathCost  = casadi.Function('L'  ,{t,symTh,symU},{ (1/2)*(symU'*symU)           });
FinalCost = casadi.Function('Psi',{symTh}      ,{  1e5*(1/m^2)*sum(sum((symThth.' - symThth).^2))  });

iCP_1 = ocp(odeEqn,PathCost,FinalCost);


Solve Gradient descent

tic
ControlGuess = ZerosControl(odeEqn);
[OptControl_1 ,OptThetaVector_1] =  ArmijoGradient(iCP_1,ControlGuess);
toc


Length Step has been change: LenghtStep = 0.00025
iter: 001 | error: 2.215e+02 | LengthStep: 5.00e-04 | J: 1.16893e+03 | Distance2Target: NaN 
iter: 002 | error: 2.214e+02 | LengthStep: 1.00e-03 | J: 1.16780e+03 | Distance2Target: NaN 
iter: 003 | error: 2.212e+02 | LengthStep: 2.00e-03 | J: 1.16554e+03 | Distance2Target: NaN 
iter: 004 | error: 2.207e+02 | LengthStep: 4.00e-03 | J: 1.16103e+03 | Distance2Target: NaN 
iter: 005 | error: 2.198e+02 | LengthStep: 8.00e-03 | J: 1.15206e+03 | Distance2Target: NaN 
iter: 006 | error: 2.180e+02 | LengthStep: 1.60e-02 | J: 1.13430e+03 | Distance2Target: NaN 
iter: 007 | error: 2.144e+02 | LengthStep: 3.20e-02 | J: 1.09950e+03 | Distance2Target: NaN 
iter: 008 | error: 2.074e+02 | LengthStep: 6.40e-02 | J: 1.03270e+03 | Distance2Target: NaN 
iter: 009 | error: 1.938e+02 | LengthStep: 1.28e-01 | J: 9.09692e+02 | Distance2Target: NaN 
iter: 010 | error: 1.682e+02 | LengthStep: 2.56e-01 | J: 7.01968e+02 | Distance2Target: NaN 
iter: 011 | error: 1.235e+02 | LengthStep: 5.12e-01 | J: 4.10547e+02 | Distance2Target: NaN 
iter: 012 | error: 6.257e+01 | LengthStep: 1.02e+00 | J: 1.47399e+02 | Distance2Target: NaN 
Length Step has been change: LenghtStep = 0.256
iter: 013 | error: 4.113e+01 | LengthStep: 5.12e-01 | J: 1.20852e+02 | Distance2Target: NaN 
iter: 014 | error: 2.215e+01 | LengthStep: 1.02e+00 | J: 9.58348e+01 | Distance2Target: NaN 
Length Step has been change: LenghtStep = 0.256
Length Step has been change: LenghtStep = 0.064
Length Step has been change: LenghtStep = 0.016
Length Step has been change: LenghtStep = 0.004
Length Step has been change: LenghtStep = 0.001
Length Step has been change: LenghtStep = 0.00025
Length Step has been change: LenghtStep = 6.25e-05
Length Step has been change: LenghtStep = 1.5625e-05
Length Step has been change: LenghtStep = 3.9063e-06
Length Step has been change: LenghtStep = 9.7656e-07
Length Step has been change: LenghtStep = 2.4414e-07
Length Step has been change: LenghtStep = 6.1035e-08
Length Step has been change: LenghtStep = 1.5259e-08
Length Step has been change: LenghtStep = 3.8147e-09

    Mininum Length Step have been achive !! 

Elapsed time is 2.131655 seconds.



Visualization

First, we present the dynamics without control,

FreeThetaVector = solve(odeEqn,ControlGuess);
FreeThetaVector = full(FreeThetaVector);
figure
plot(FreeThetaVector')
ylabel('Phases [rad]')
xlabel('Time [sec]')
title('The dynamics without control (incoherence)')




and see the controled dynamics.

figure
plot(OptThetaVector_1')
ylabel('Phases [rad]')
xlabel('Time [sec]')
title(&quot;The dynamics under control L^2 | N_{osc}=&quot;+m)




We also can plot the control function along time.

figure
plot(OptControl_1)
legend(&quot;norm(u(t)) = &quot;+norm(OptControl_1))
ylabel('u(t)')
xlabel('Time [sec]')
title('The control function')




The problem with different regularization

In this part, we change the regularization into $L^1$-norm and see the difference.

PathCost  = casadi.Function('L'  ,{t,symTh,symU},{ sqrt(symU.^2+1e-3)});
iCP_2 = ocp(odeEqn,PathCost,FinalCost);


tic
[OptControl_2 ,OptThetaVector_2] =  ArmijoGradient(iCP_2,ControlGuess);
toc


Length Step has been change: LenghtStep = 0.00025
iter: 001 | error: 2.363e+01 | LengthStep: 5.00e-04 | J: 1.11735e+02 | Distance2Target: NaN 
iter: 002 | error: 2.363e+01 | LengthStep: 1.00e-03 | J: 1.11730e+02 | Distance2Target: NaN 
iter: 003 | error: 2.361e+01 | LengthStep: 2.00e-03 | J: 1.11720e+02 | Distance2Target: NaN 
iter: 004 | error: 2.359e+01 | LengthStep: 4.00e-03 | J: 1.11699e+02 | Distance2Target: NaN 
iter: 005 | error: 2.353e+01 | LengthStep: 8.00e-03 | J: 1.11659e+02 | Distance2Target: NaN 
iter: 006 | error: 2.343e+01 | LengthStep: 1.60e-02 | J: 1.11579e+02 | Distance2Target: NaN 
iter: 007 | error: 2.322e+01 | LengthStep: 3.20e-02 | J: 1.11420e+02 | Distance2Target: NaN 
iter: 008 | error: 2.282e+01 | LengthStep: 6.40e-02 | J: 1.11109e+02 | Distance2Target: NaN 
iter: 009 | error: 2.208e+01 | LengthStep: 1.28e-01 | J: 1.10509e+02 | Distance2Target: NaN 
iter: 010 | error: 2.081e+01 | LengthStep: 2.56e-01 | J: 1.09388e+02 | Distance2Target: NaN 
iter: 011 | error: 1.890e+01 | LengthStep: 5.12e-01 | J: 1.07391e+02 | Distance2Target: NaN 
iter: 012 | error: 1.674e+01 | LengthStep: 1.02e+00 | J: 1.04033e+02 | Distance2Target: NaN 
iter: 013 | error: 1.607e+01 | LengthStep: 2.05e+00 | J: 9.86861e+01 | Distance2Target: NaN 
iter: 014 | error: 4.081e+01 | LengthStep: 4.10e+00 | J: 9.24707e+01 | Distance2Target: NaN 
Length Step has been change: LenghtStep = 1.024
Length Step has been change: LenghtStep = 0.256
Length Step has been change: LenghtStep = 0.064
Length Step has been change: LenghtStep = 0.016
Length Step has been change: LenghtStep = 0.004
Length Step has been change: LenghtStep = 0.001
Length Step has been change: LenghtStep = 0.00025
Length Step has been change: LenghtStep = 6.25e-05
Length Step has been change: LenghtStep = 1.5625e-05
Length Step has been change: LenghtStep = 3.9063e-06
Length Step has been change: LenghtStep = 9.7656e-07
Length Step has been change: LenghtStep = 2.4414e-07
Length Step has been change: LenghtStep = 6.1035e-08
Length Step has been change: LenghtStep = 1.5259e-08
Length Step has been change: LenghtStep = 3.8147e-09

    Mininum Length Step have been achive !! 

Elapsed time is 2.494788 seconds.



figure

line(tspan,OptThetaVector_2')
ylabel('Phases [rad]')
xlabel('Time [sec]')
title(&quot;The dynamics under control L^1 | N_{osc}=&quot;+m)




figure
plot(OptControl_1)
line(1:length(OptControl_2),OptControl_2,'Color','red')

Psi_1 = norm(sin(OptThetaVector_1(:,end).' - OptThetaVector_1(:,end)),'fro');
Psi_2 = norm(sin(OptThetaVector_2(:,end).' - OptThetaVector_2(:,end)),'fro');

legend(&quot;u(t) with L^2-norm; Terminal cost = &quot;+Psi_1,&quot;u(t) with L^1-norm; Terminal cost = &quot;+Psi_2)
ylabel('The coupling strength (u(t))')
xlabel('Time [sec]')
title('The comparison between two different control cost functionals')




As one can expected from the regularization functions, the control function from $L^2$-norm acting more smoothly from $0$ to the largest value. The function from $L^2$-norm draws much stiff lines.

YFr = FreeThetaVector';
YL1 = OptThetaVector_1';
YL2 = OptThetaVector_2';
%%
animationpendulums({YFr,YL1,YL2},tspan,{'Free','L^2 Control','L^1 Control'})


Finally, we can see the behavior of the two control types against the evolution of free dynamics.



</description>
        <pubDate>Wed, 01 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp06/P0007</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp06/P0007</guid>
        
        
        <category>tutorial</category>
        
        <category>WP06</category>
        
      </item>
    
      <item>
        <title>Simultaneous Control with DyCon Toolbox</title>
        <description>In this tutorial we will present a simultaneous control problem in a linear system dependent on parameters. We will use the MATLAb DyCon Toolbox library.

Setup

In this tutorial we need DyCon Toolbox, to install it we will have to write the following in our MATLAB console:

unzip('https://github.com/DeustoTech/DyCon-Computational-Platform/archive/master.zip')
addpath(genpath(fullfile(cd,'DyCon-toolbox-master')))

Problem definition

The simultaneous control problem is defined as:



subject to:



where:



Numerical Implementation

First we import the CasAdi library in order to create symbolic variables

import casadi.*
%% In this case $\nu_i$ are
M = 50;
nu = linspace(1,6,M);

Then we create the matrices of linear dynamics.



[A,B] = GenMatSim(nu);


With these matrices we create the ode object

Nt = 500;T  = 0.8;
tspan = linspace(0,T,Nt);
%% create linear dynamic
iode = linearode(A,B,tspan);
%% set initial condition
Y0 = ones(2, 1);
iode.InitialCondition = repmat(Y0,M,1);


then we create the optimal control problem
% Get Symbolical variable
Ys  = iode.State.sym;
Us  = iode.Control.sym;
ts  = SX.sym('t'); %% &amp;lt;= Create a symbolical time
% Set Target
YT = zeros(2, 1);YT = repmat(YT,M,1);
%
PathCost  = casadi.Function('L'  ,{ts,Ys,Us},{ (1/2)*(Us'*Us)           });
FinalCost = casadi.Function('Psi',{Ys}      ,{  1e7*((Ys-YT).'*(Ys-YT)) });
% Create the optimal control
iocp = ocp(iode,PathCost,FinalCost);


Solve Optimal Control Problem with ipopt solver

U0 = ZerosControl(iode);
[Uopt ,Yopt] =  IpoptSolver(iocp,U0);


This is Ipopt version 3.12.3, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation).

Number of nonzeros in equality constraint Jacobian...:   199700
Number of nonzeros in inequality constraint Jacobian.:        0
Number of nonzeros in Lagrangian Hessian.............:      600

Total number of variables............................:    50500
                     variables with only lower bounds:        0
                variables with lower and upper bounds:        0
                     variables with only upper bounds:        0
Total number of equality constraints.................:    50000
Total number of inequality constraints...............:        0
        inequality constraints with only lower bounds:        0
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0

iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
   0  1.0000000e+09 1.12e-02 7.88e+00  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0
   1  4.0934345e+06 1.26e-13 5.63e-09  -1.0 1.66e+02    -  1.00e+00 1.00e+00f  1

Number of Iterations....: 1

                                   (scaled)                 (unscaled)
Objective...............:   2.0467172272726700e+01    4.0934344545453396e+06
Dual infeasibility......:   5.6305735629536002e-09    1.1261147125907198e-03
Constraint violation....:   1.2612133559741778e-13    1.2612133559741778e-13
Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00
Overall NLP error.......:   5.6305735629536002e-09    1.1261147125907198e-03


Number of objective function evaluations             = 2
Number of objective gradient evaluations             = 2
Number of equality constraint evaluations            = 2
Number of inequality constraint evaluations          = 0
Number of equality constraint Jacobian evaluations   = 2
Number of inequality constraint Jacobian evaluations = 0
Number of Lagrangian Hessian evaluations             = 1
Total CPU secs in IPOPT (w/o function evaluations)   =      1.391
Total CPU secs in NLP function evaluations           =      0.027

EXIT: Optimal Solution Found.
               t_proc [s]   t_wall [s]    n_eval
       nlp_f     0.000381      0.00038         2
       nlp_g       0.0042       0.0042         2
  nlp_grad_f      0.00167      0.00167         3
  nlp_hess_l      0.00331      0.00332         1
   nlp_jac_g        0.026        0.026         3
      solver         1.48         1.36         1
Elapsed time is 3.853462 seconds.



Compute Free solution

Yfree = solve(iode,U0*0);
Yfree = full(Yfree);


Visualization

fig = figure;
fig.Units = 'norm';fig.Position = [0.1 0.1 0.6 0.5];
%%
plotSimu(tspan,Yfree,Yopt,Uopt,M)




Figure 1. The different colors represent the dynamic system under different parameters. It can be seen how the same control is obtained acting for all the dynamic systems is capable of driving the systems to the target.
</description>
        <pubDate>Wed, 01 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0009</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0009</guid>
        
        
        <category>tutorial</category>
        
        <category>WP01</category>
        
      </item>
    
      <item>
        <title>Control of reaction-diffusion under state constraints - Barriers</title>
        <description>This tutorial is focused on the understanding of of the problem



The solutions for $a=0$ constitute an intrinsic obstruction to the controllability with state-constraints due to the comparison principle [1]. Indeed, for any control function $0\leq a(x,t)\leq 1$
the solution of the problem:



where $u$ is an elliptic nontrivial solution satisfies that $v(x,t)\geq u(x)$. Therefore, we cannot expect to control to any function below $u$.

This is why the understanding of the existence or non-existence of nontrivial solutions is of main importance for the controllability under state-constraints.

Non-trivial solutions around the boundary value $a=\theta$ do not constitute an intrinsic obstruction to the controllability because the boundary value $\theta$ is not in the border of the admissible set. However, the existence of such solutions create a technical difficulty for achieving the controllability to the steady state $w\equiv \theta$ which can be solved by constructing paths of steady states (See this blog entry ).

Elliptic nontrivial solutions with boundary value 1 do not exist. This can be related to the fact that the traveling waves for the Cauchy problem in the real line are approaching the steady state $w\equiv 1$ [2]

The existence of non-trivial solutions can be done either by comparison principles or by understanding the critical points of a functional (see [3]).

Any critical point of the functional:





is a weak solution of the main equation.

In order to guarantee that the solution will be between $0$ and $1$, one can extend $f$ by $0$ outside of the interval $[0,1]$.

Restricting ourselves in the one dimensional case (the argument also holds in several dimensions) and making the spatial change fo variables $x\to \frac{x}{L}$, where $L$ is the length of the domain the functional reads:



In the expression above we see that if $L$ is small the convex part dominates while if $L$ is large and $\int_0^1f(s)ds&amp;gt;0$ it might not be convex.

In the videos below one can see the evolution of the functional depending on the parameter $\lambda=L^2$. The representation is the evaluation of the functional along the first and the third eigenfunction, i.e. $e_1=\sin(\pi x)$ and $e_3=\sin(3\pi x)$,



Here, one can see the evolution of the functional for $a=0$ and for $a=\theta=1/3$ for values of $\alpha$ and $\beta$.


    
        
            
            
                
        
         
            
            
            
               
    

Figure 1. Animation of the functional $J$ for diferent values of $\lambda=L^2$. At the left the boundary value is $0$ while at the right the boundary value is $\theta$.

In the next figure a qualitative bifurcation diagram is represented, the red curve represents the nontrivial solutions with boundary value $\theta$ and the blue one the boundary value $0$. $\lambda^\ast$ is the minimum $\lambda=L^2$ for which a nontrivial solution around $0$ exists, analogously $\lambda^*_\theta$.




Figure 2. Bifurcation diagram . The blue line represents the maximum of the nontrivial solutions with boundary value $0$. The red curve represents some nontrivial solutions with boundary value $\theta$, the red line is the maximum of the nontrivial solution whenever the line is above $\theta$ and represents the minimum when the red line is below $\theta$.

The value $\frac{\lambda_1}{f’(\theta)}$ is the critical value for which the stationary solution $\theta$ becomes unstable. After this situation we have a nontrivial solution above and below $\theta$.

There will be further bifurcations around the boundary value $\theta$ when we increase $\lambda$. These bifuractions will lead to oscillatory nontrivial solutions that can be well understood in the phase-plane, see [2].

Here one can see different non-trivial solutions for different boundary values. The green curve corresponds to a section of the nontrivial solution in the whole $\mathbb{R}$:






Figure 3. Several nontrivial solutions, in blue the ones with boundary value $0$, the red ones have boundary value $\theta$ and the green one is the solution in the whole $\mathbb{R}$. $\theta_1$ is the number such that $\int_0^{\theta_1} f(s)ds = 0 $.

References:

[1] M.H. Protter and H.F. Weinberger, Maximum principles in differential equations, Springer Science &amp;amp; Business Media, 2012.

[2] D.  Ruiz-Balet  and  E.  Zuazua. Control of certain parabolic models from biology and social sciences.   Preprint  available  at https://cmc.deusto.eus/domenec-ruiz-balet/.

[3]  P.L. Lions, On the existence of positive solutions of semilinear elliptic equations, SIAM Rev. 24 (1982), no. 4, 441–467.

</description>
        <pubDate>Tue, 31 Mar 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0006</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0006</guid>
        
        
        <category>tutorial</category>
        
        <category>WP03</category>
        
      </item>
    
      <item>
        <title>Control of reaction-diffusion under state constraints - Application of the staircase method</title>
        <description>In this tutorial, we will present how to generate admissible paths of steady states for the homogeneous reaction-diffusion equation:



This a key argument for ensuring controllability that has been studied in [2,3,4,5].

Stair-case method
The stair-case method (see [1]) guarantees the following:

If we have an admissible continuous path of steady states, for any initial datum in the path, any target function in the path and for $T&amp;gt;0$ sufficiently large, there exists a function $a$ such that drives the system from the initial datum to the target fulfilling the state-constraints along the trajectory.

Extension to ball, and the phase-plane analysis

We will restrict ourselves to the construction of paths that connect the steady state $w\equiv 0$ with the steady state $w\equiv \theta$. For doing such example with our model bistable equation.

In order to construct the paths we will use phase-plane techniques for which we use radial coordinates. Our original domain might not be a ball, for this reason we extend it to a ball and firstly construct the path there.




Figure 1. Extension of our domain to a ball.

Remind that the important issue is to be able to guarantee that for every domain $w\equiv0$ and $w\equiv\theta$ are connected in an admissable way and this is seen in the phase plane representation of the elliptic equation.



Now, considering the energy


where $F(u)=\int_0^u f(s)ds$, one can see that the radial ODE dissipates.

Define the following region:



Let $\theta_1$ be defined as:



Note that the region defined by



Note that $\Gamma\subset D$.

Take $(u_0,0)\in \Gamma$, then the solution of radial equation with initial
datum $(u_0,0)$ satisfies:



So $(u,v)\in\Gamma$ for all $r&amp;gt;0$.

We have that the blue line (the border of $\Gamma$) in the following figure determines a positively invariant region.

Then, making $a$ change continuously from $0$ to $\theta$ we generate a continuous path (by the Gromwall inequality) that is admissible since the invariant region $\Gamma$ is inside the admissible set.




Figure 2. Invariant region and construction of the path.

Animation of the path



  


Figure 3. Path of steady states.

If our domain is not a ball we restrict our path to the original domain to obtain the desired path.

Other features

General existence of admisible paths is not true due to the comparison principle. Non-trivial steady states can exist that block any possibility to control (See this blog entry ).

Here we restrict ourselves in the one dimensional case. In the following figure one can see how can we connect the steady state $w\equiv 0$ with the first nontrivial solution with boundary value $0$.




    
        
            
        
         
            
                           
    



Figure 4. Path of steady states.

However, since this path touches the boundary of the admissible set controllability cannot be guaranteed due to the comparison principle.

Another important remark to be mentioned is that if we forget about the state constraints, more paths can be generated, provided that the ODE representation of the elliptic equation does not blow up. The following figure is an example of a path connecting $w\equiv 0$ with $w\equiv 1$ that violates the constraints:



    
        
            
        
         
            
                           
    


Figure 5. Path of steady states violating constraints.

References:

[1] D. Pighin, E. Zuazua, Controllability under positivity constraints of multi-d wave equations, in:
Trends in Control Theory and Partial Differential Equations, Springer, 2019, pp. 195–232.

[2] J.-M. Coron, E. Trélat, Global steady-state controllability of one-dimensional semilinear heat equa-
tions, SIAM J. Control. Optim. 43 (2) (2004) 549–569.

[3] C. Pouchol, E. Trélat, E. Zuazua, Phase portrait control for 1d monostable and bistable reac-
635
tion–diffusion equations, Nonlinearity 32 (3) (2019) 884–909.

[4]   D.  Ruiz-Balet  and  E.  Zuazua. Controllability  under  constraints  for  reaction-diffusionequations:   The  multi-dimensional  case.   Preprint  available  athttps://cmc.deusto.eus/domenec-ruiz-balet/.

[5] D.  Ruiz-Balet  and  E.  Zuazua. Control of certain parabolic models from biology and social sciences.   Preprint  available  at https://cmc.deusto.eus/domenec-ruiz-balet/.

</description>
        <pubDate>Mon, 30 Mar 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0005</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0005</guid>
        
        
        <category>tutorial</category>
        
        <category>WP03</category>
        
      </item>
    
      <item>
        <title>Control for a semilinear heat equation and analogies with a collective behavior model</title>
        <description>In this tutorial we will apply the DyCon toolbox to find a control to the semi-discrete semi-linear heat equation.



where $N^2A$ is the discretization of the Laplacian in 1d in $N$ nodes. We are looking for a control that after time $T$ steers the system near zero.

Setup

In this tutorial we need DyCon Toolbox, to install it we will have to write the following in our MATLAB console:

unzip('https://github.com/DeustoTech/DyCon-Computational-Platform/archive/master.zip')
addpath(genpath(fullfile(cd,'DyCon-toolbox-master')))

Methodology
In order to do so we will frame the problem as a minimization of a functional and we will apply gradient descent to find it. Note that the convexity of the functional is not proven, therefore, we will obtain a local minima for the functional. The functional considered will be:



where by $| \cdot |_{L^2}$ we understand the discrete $L^2$ norm.

Once this control is computed for a certain N, we will think on a dynamical system that models an opinion dynamics with $N$ agents communicating through a chain.

\begin{equation}\label{m1}y_t-\frac{1}{N}Ay=G(y)+Bv\end{equation}

The goal will be to compute also the control $v$ thinking model \eqref{m1} as if it was a semidiscretization of a heat equation with diffusivity $\frac{1}{N^3}$. Furthermore we will also compute the control for model \eqref{m1} with the non-linearity being non-homogeneous on $N$ and a time horizon being $T_N=N^3T$.

clear all
import casadi.*


Definition of the time

ts = SX.sym('t');
%% Discretization of the space
N = 30;
xi = 0; xf = 1;
xline = linspace(xi,xf,N);


Interior Control region between 0.5 and 0.8

w1 = 0.5; w2 = 0.8;
D = 1;


[A,B,count] = GetABmatrix(N,w1,w2,D);


we define symbolically the vectors of the state and the control

Ys = SX.sym('y',[N 1]);
Us = SX.sym('u',[count 1]);


We create the ODE object Our ODE object will have the semi-discretization of the semilinear heat equation. We set also initial conditions, define the non linearity and the interaction of the control to the dynamics.

Initial condition

Y0 = 2*sin(pi*xline');


Diffusion part: the discretization of the 1d Laplacian

Definition of the non-linearity 

G = casadi.Function('NLT',{Ys},{10*Ys.*exp(-Ys.^2)});%%


and we define the part of the dynamics corresponding to the nonlinearity

Putting all the things together

F = casadi.Function('F',{ts,Ys,Us},{A*Ys + B*Us + G(Ys)});
%% Creation of the ODE object
%% Time horizon
T = 1;


We create the ODE-object and we change the resolution to $dt=0.01$ in order to see the variation in a small time scale. We will get the values of the solution in steps of size odeEqn.dt, if we do not care about modifying this parameter in the object, we might get the solution in certain time steps that will hide part of the dynamics.

Nt = 100;
tspan = linspace(0,T,Nt);
ipde = semilinearpde1d(Ys,Us,A,B,G,tspan,xline);
ipde.InitialCondition = Y0;


We solve the equation and we plot the free solution applying solve to odeEqn and we plot the free solution.

Yfree = solve(ipde,ZerosControl(ipde));
Yfree = full(Yfree);


figure;
surf(Yfree','EdgeColor','none');
title('Free Dynamics')
xlabel('space discretization')
ylabel('Time')
yticks([1 Nt])
yticklabels([tspan(1) tspan(end)])




We create the object that collects the formulation of an optimal control problem  by means of the object that describes the dynamics odeEqn, the functional to minimize Jfun and the time horizon T

L   = Function('L'  ,{ts,Ys,Us},{ Us.'*Us  });
Psi = Function('Psi',{Ys}      ,{ 1e6*(Ys.'*Ys) });

iocp = ocp(ipde,L,Psi);


We apply the steepest descent method to obtain a local minimum (our functional might not be convex).

U0 =ZerosControl(ipde);
[OptControl ,OptState]  = IpoptSolver(iocp,U0);


This is Ipopt version 3.12.3, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation).

Number of nonzeros in equality constraint Jacobian...:    19236
Number of nonzeros in inequality constraint Jacobian.:        0
Number of nonzeros in Lagrangian Hessian.............:     3900

Total number of variables............................:     3900
                     variables with only lower bounds:        0
                variables with lower and upper bounds:        0
                     variables with only upper bounds:        0
Total number of equality constraints.................:     3000
Total number of inequality constraints...............:        0
        inequality constraints with only lower bounds:        0
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0

iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
   0  3.0000000e+07 9.05e+00 9.67e-01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0
   1  2.4544321e+05 1.22e-01 7.92e+00  -1.0 3.10e+01  -2.0 1.00e+00 1.00e+00f  1
   2  1.8219937e+06 4.24e-03 1.73e+00  -1.0 1.31e+01  -1.6 1.00e+00 1.00e+00h  1
   3  7.1220860e+04 1.67e-02 2.92e+00  -1.0 9.12e+00  -2.1 1.00e+00 1.00e+00f  1
   4  8.9356500e+03 3.97e-04 8.51e-02  -1.0 2.41e+00  -2.5 1.00e+00 1.00e+00f  1
   5  3.1075917e+03 1.06e-04 3.19e-03  -2.5 2.96e+00  -3.0 1.00e+00 1.00e+00f  1
   6  1.3016546e+03 5.20e-05 1.19e-03  -3.8 3.60e+00  -3.5 1.00e+00 1.00e+00h  1
   7  4.5871538e+02 5.64e-05 4.84e-04  -5.7 4.41e+00  -4.0 1.00e+00 1.00e+00h  1
   8  1.6346578e+02 6.55e-05 1.92e-04  -5.7 5.26e+00  -4.4 1.00e+00 1.00e+00h  1
   9  9.7049320e+01 1.20e-04 3.78e-05  -5.7 3.10e+00  -4.9 1.00e+00 1.00e+00h  1
iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
  10  4.0069187e+01 9.26e-03 1.98e-05  -5.7 1.48e+01    -  1.00e+00 5.00e-01h  2
  11  1.6748754e+01 4.28e-04 2.92e-06  -5.7 4.34e+00    -  1.00e+00 1.00e+00h  1
  12  1.6371256e+01 4.58e-06 9.87e-09  -5.7 1.88e-01    -  1.00e+00 1.00e+00h  1
  13  1.6367177e+01 1.03e-11 1.11e-13  -8.6 7.84e-04    -  1.00e+00 1.00e+00h  1

Number of Iterations....: 13

                                   (scaled)                 (unscaled)
Objective...............:   8.1835886066017175e-04    1.6367177213203433e+01
Dual infeasibility......:   1.1096830919432588e-13    2.2193661838865175e-09
Constraint violation....:   1.0345974077452524e-11    1.0345974077452524e-11
Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00
Overall NLP error.......:   1.0345974077452524e-11    2.2193661838865175e-09


Number of objective function evaluations             = 19
Number of objective gradient evaluations             = 14
Number of equality constraint evaluations            = 19
Number of inequality constraint evaluations          = 0
Number of equality constraint Jacobian evaluations   = 14
Number of inequality constraint Jacobian evaluations = 0
Number of Lagrangian Hessian evaluations             = 13
Total CPU secs in IPOPT (w/o function evaluations)   =      0.436
Total CPU secs in NLP function evaluations           =      0.057

EXIT: Optimal Solution Found.
               t_proc [s]   t_wall [s]    n_eval
       nlp_f     0.000826     0.000828        19
       nlp_g      0.00916      0.00917        19
  nlp_grad_f      0.00139      0.00139        15
  nlp_hess_l       0.0175       0.0174        13
   nlp_jac_g       0.0333       0.0333        15
      solver        0.507        0.501         1
Elapsed time is 0.636870 seconds.



plotSHE(OptState,OptControl,Yfree,YT,xline,tspan,Nt)



    
        
            
        
         
            
               
    


Collective behavior dynamics

Now we apply the same procedure for the collective behavior dynamics. We will employ a function that does the algorithm explained before for the semilinear heat equation having the chance to set a diffusivity constant.

For the simulation of the model in collective behavior we will employ a diffusivity $D=\frac{1}{N^3}$.

D = 1/(N^3);
[A,B,count] = GetABmatrix(N,w1,w2,D);
%%
G = casadi.Function('NLT',{Ys},{10*Ys.*exp(-Ys.^2)});%%
%%
T = 1;  Nt = 100;
tspan = linspace(0,T,Nt);
%%
ipde = semilinearpde1d(Ys,Us,A,B,G,tspan,xline);
ipde.InitialCondition = Y0;
%%
iocp = ocp(ipde,L,Psi);



    
        
            
        
         
            
               
    


U0 =ZerosControl(ipde);
%%[OptControl ,OptState]  = ArmijoGradient(iocp,U0,'MaxIter',200);
[OptControl ,OptState]  = IpoptSolver(iocp,U0);


This is Ipopt version 3.12.3, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation).

Number of nonzeros in equality constraint Jacobian...:    19236
Number of nonzeros in inequality constraint Jacobian.:        0
Number of nonzeros in Lagrangian Hessian.............:     3900

Total number of variables............................:     3900
                     variables with only lower bounds:        0
                variables with lower and upper bounds:        0
                     variables with only upper bounds:        0
Total number of equality constraints.................:     3000
Total number of inequality constraints...............:        0
        inequality constraints with only lower bounds:        0
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0

iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
   0  3.0000000e+07 1.00e+00 9.72e-01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0
   1  1.1211823e+08 6.93e-02 1.07e+01  -1.7 1.75e+00   0.0 1.00e+00 1.00e+00h  1
   2  1.0201471e+08 2.53e-02 2.23e+00  -1.7 3.50e+00  -0.5 1.00e+00 1.00e+00f  1
   3  8.8642262e+07 4.18e-02 5.31e-01  -1.7 2.13e+02    -  1.00e+00 1.00e+00f  1
   4  8.2505158e+07 1.39e+00 3.13e+00  -1.7 2.72e+03    -  1.00e+00 1.00e+00F  1
   5  7.9212105e+07 9.68e+00 5.20e+00  -1.7 1.38e+03    -  1.00e+00 1.00e+00f  1
   6  7.8291279e+07 6.94e-02 2.84e+00  -1.7 2.05e+03    -  1.00e+00 1.00e+00f  1
   7  7.8090072e+07 1.57e+00 4.98e-01  -1.7 7.87e+02    -  1.00e+00 1.00e+00f  1
   8  7.8026943e+07 1.54e-01 8.25e-02  -1.7 3.33e+02    -  1.00e+00 1.00e+00f  1
   9  7.8491521e+07 4.41e-02 7.64e+00  -2.5 2.10e+00  -1.0 1.00e+00 1.00e+00h  1
iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
...
...
  54  7.8022238e+07 4.03e-07 8.33e-10  -5.7 2.15e-02    -  1.00e+00 1.00e+00h  1
  55  7.8022238e+07 9.83e-12 1.82e-12  -8.6 1.02e-04    -  1.00e+00 1.00e+00h  1

Number of Iterations....: 55

                                   (scaled)                 (unscaled)
Objective...............:   3.9011119130960064e+03    7.8022238261920124e+07
Dual infeasibility......:   1.8189894035458565e-12    3.6379788070917130e-08
Constraint violation....:   9.8296371042749797e-12    9.8296371042749797e-12
Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00
Overall NLP error.......:   9.8296371042749797e-12    3.6379788070917130e-08


Number of objective function evaluations             = 111
Number of objective gradient evaluations             = 56
Number of equality constraint evaluations            = 111
Number of inequality constraint evaluations          = 0
Number of equality constraint Jacobian evaluations   = 56
Number of inequality constraint Jacobian evaluations = 0
Number of Lagrangian Hessian evaluations             = 55
Total CPU secs in IPOPT (w/o function evaluations)   =      1.374
Total CPU secs in NLP function evaluations           =      0.308

EXIT: Optimal Solution Found.
               t_proc [s]   t_wall [s]    n_eval
       nlp_f      0.00624      0.00627       111
       nlp_g       0.0634       0.0633       111
  nlp_grad_f      0.00762      0.00768        57
  nlp_hess_l       0.0868       0.0846        55
   nlp_jac_g        0.149        0.149        57
      solver         1.74         1.73         1
Elapsed time is 1.874814 seconds.



plotSHE(OptState,OptControl,Yfree,YT,xline,tspan,Nt)



    
        
            
        
         
            
               
    


Now we will change also the time horizon and we will incorporate a non-homogeneous non-linearity, we will just divide the non-linearity $G$ by $N^3$

D = 1/(N^2);
[A,B,count] = GetABmatrix(N,w1,w2,D);
%%
G = casadi.Function('NLT',{Ys},{10*Ys.*exp(-Ys.^2)/N^2} );%%
%%
T = N^2;  Nt = 100;
tspan = linspace(0,T,Nt);
%%
ipde = semilinearpde1d(Ys,Us,A,B,G,tspan,xline);
ipde.InitialCondition = Y0;
%%
iocp = ocp(ipde,L,Psi);


U0 =ZerosControl(ipde);
[OptControl ,OptState]  = IpoptSolver(iocp,U0);


This is Ipopt version 3.12.3, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation).

Number of nonzeros in equality constraint Jacobian...:    19236
Number of nonzeros in inequality constraint Jacobian.:        0
Number of nonzeros in Lagrangian Hessian.............:     3900

Total number of variables............................:     3900
                     variables with only lower bounds:        0
                variables with lower and upper bounds:        0
                     variables with only upper bounds:        0
Total number of equality constraints.................:     3000
Total number of inequality constraints...............:        0
        inequality constraints with only lower bounds:        0
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0

iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
   0  3.0000000e+07 9.05e+00 1.17e+02  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0
   1  2.8754020e+04 1.48e-01 4.01e+00  -1.0 2.73e+00   0.0 1.00e+00 1.00e+00f  1
   2  3.0839145e+03 1.91e-02 3.08e-01  -1.0 9.19e-01  -0.5 1.00e+00 1.00e+00h  1
   3  9.2703794e+02 1.17e-03 2.31e-02  -1.7 2.08e-01  -1.0 1.00e+00 1.00e+00h  1
   4  4.2707593e+02 2.49e-03 9.65e-03  -3.8 2.61e-01  -1.4 1.00e+00 1.00e+00h  1
   5  2.2851845e+02 1.36e-03 2.60e-03  -3.8 2.10e-01  -1.9 1.00e+00 1.00e+00h  1
   6  1.5158663e+02 1.25e-03 9.80e-04  -3.8 2.38e-01  -2.4 1.00e+00 1.00e+00h  1
   7  1.0229631e+02 2.37e-03 4.26e-04  -5.7 3.11e-01  -2.9 1.00e+00 1.00e+00h  1
   8  8.6220145e+01 1.84e-01 3.73e-04  -5.7 2.84e+01    -  1.00e+00 1.25e-01h  4
   9  1.6663658e+00 6.98e-02 4.71e-05  -5.7 2.20e+00    -  1.00e+00 1.00e+00h  1
iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
  10  1.1303218e-01 1.61e-02 9.28e-07  -5.7 4.53e-01    -  1.00e+00 1.00e+00h  1
  11  3.7959122e-02 7.79e-03 2.54e-08  -5.7 2.84e-01    -  1.00e+00 1.00e+00h  1
  12  2.1316195e-02 1.77e-03 4.98e-09  -5.7 1.39e-01    -  1.00e+00 1.00e+00h  1
  13  1.8447375e-02 6.30e-06 5.26e-11  -5.7 8.15e-03    -  1.00e+00 1.00e+00h  1
  14  1.8440851e-02 2.26e-09 9.54e-15  -8.6 1.61e-04    -  1.00e+00 1.00e+00h  1

Number of Iterations....: 14

                                   (scaled)                 (unscaled)
Objective...............:   9.2204254694796125e-07    1.8440850938959225e-02
Dual infeasibility......:   9.5381182640930876e-15    1.9076236528186175e-10
Constraint violation....:   2.2559639156760625e-09    2.2559639156760625e-09
Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00
Overall NLP error.......:   2.2559639156760625e-09    2.2559639156760625e-09


Number of objective function evaluations             = 19
Number of objective gradient evaluations             = 15
Number of equality constraint evaluations            = 19
Number of inequality constraint evaluations          = 0
Number of equality constraint Jacobian evaluations   = 15
Number of inequality constraint Jacobian evaluations = 0
Number of Lagrangian Hessian evaluations             = 14
Total CPU secs in IPOPT (w/o function evaluations)   =      0.415
Total CPU secs in NLP function evaluations           =      0.065

EXIT: Optimal Solution Found.
               t_proc [s]   t_wall [s]    n_eval
       nlp_f     0.000898     0.000903        19
       nlp_g      0.00904      0.00905        19
  nlp_grad_f       0.0017      0.00169        16
  nlp_hess_l       0.0214       0.0214        14
   nlp_jac_g       0.0354        0.035        16
      solver        0.489        0.487         1
Elapsed time is 0.616536 seconds.



plotSHE(OptState,OptControl,Yfree,YT,xline,tspan,Nt)



    
        
            
        
         
            
               
    

</description>
        <pubDate>Sun, 15 Mar 2020 00:00:00 +0100</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp06/P0008</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp06/P0008</guid>
        
        
        <category>tutorial</category>
        
        <category>WP06</category>
        
      </item>
    
      <item>
        <title>POD and DMD Reduced Order Models for a 2D Burgers Equation</title>
        <description>Burgers equation

For a given viscosity parameter $\nu$ and for time $t&amp;gt;0$, we consider the 2D Burgers equation on the unit square



with zero Neumann boundary conditions and initial condition



and its numerical approximation using


  Finite Elements in space and Runge-Kutta in time,
  a POD reduction of the FEM model, and
  a DMD reduced order model.


We present the basic ideas and how they are realized in a basic Python script.

0. The Setup

Please download and run the python file burgers.py to check the
implementation, to reproduce the presented results, and to the behavior of the
routines for different parameters.

Here is the start of the script that lists the modules on which the
implementation bases on.

import dolfin

import scipy.linalg as spla
import matplotlib.pyplot as plt
import numpy as np

from scipy.integrate import solve_ivp
from spacetime_galerkin_pod.ldfnp_ext_cholmod import SparseFactorMassmat


The code uses dolfin which is the python interface to
FEniCS while the other modules scipy, numpy,
and matplotlib are standard in python, I would say. The module
ldfnp_ext_cholmod is a little wrapper for the sparsity optimizing Cholesky
decomposition of sksparse.
It is available from my github
repository and falls back
to numpy routines, in the case that sksparse is not available.

1. The FEM Discretization

The spatial discretization in FEniCS

First, the mesh is defined – here a uniform triangulation of the unit square
controlled by the parameter N. For the value N=80 and for globally smooth
and piecewise quadratic ansatz functions, the resulting dimension of the system
is 51842.

# The mesh

pone = dolfin.Point(-1, -1)
ptwo = dolfin.Point(1, 1)
mesh = dolfin.RectangleMesh(pone, ptwo, N, N)

V = dolfin.VectorFunctionSpace(mesh, 'CG', 2)

### The FENICS FEM Discretization

v = dolfin.TestFunction(V)
u = dolfin.TrialFunction(V)


Next, the discrete linear operators are defined and exported as a SciPy sparse
matrix. Also, we factorize the mass matrix mmat for later use and define the
norm that is weighted with the mass matrix as this is the discrete $L^2$ norm.

# ## the mass matrix

mform = dolfin.inner(v, u)*dolfin.dx
massm = dolfin.assemble(mform)
mmat = dolfin.as_backend_type(massm).sparray()
mmat.eliminate_zeros()

# factorize it for later


mfac = SparseFactorMassmat(mmat)

# norm induced by the mass matrix == discrete L2-norm

def mnorm(uvec):
    return np.sqrt(np.inner(uvec, mmat.dot(uvec)))

# ## the stiffness matrix
# as a form in FEniCS

aform = nu*dolfin.inner(dolfin.grad(v), dolfin.grad(u))*dolfin.dx
aassm = dolfin.assemble(aform)

# as a sparse matrix

amat = dolfin.as_backend_type(aassm).sparray()
amat.eliminate_zeros()


Then we define the convective term as function of the velocity u both in terms
of an FE-function and as a function of the associated coefficient vector.

# ## the convective term

# as a function in FEniCS

def burgers_nonl_func(ufun):
    cform = dolfin.inner(dolfin.grad(ufun)*ufun, v)*dolfin.dx
    cass = dolfin.assemble(cform)
    return cass

# as a vector to form map

def burgers_nonl_vec(uvec):
    ufun = dolfin.Function(V)
    ufun.vector().set_local(uvec)
    bnlform = burgers_nonl_func(ufun)
    bnlvec = bnlform.get_local()
    return bnlvec


The Runge-Kutta Time Integration

To apply standard time integration schemes (here, RK23 turned out to be most
efficient), we define the right hand side $f_h$ of the spatially discretized problem



In this case the right hand side is an application of the discrete diffusion
and convection operator and the inverse of the mass matrix that, simply
speaking, maps a (discrete) form onto a discrete function. Note that there is no
explicit time dependency in the Burgers equation, but SciPy’s solve_ivp
requires this parameter. Also, we define the initial value here. The time grid
is used to store the solution for the snapshots needed later, but the time
integrator uses his internal time grid.

inivstrg = 'exp(-3.*(x[0]*x[0]+x[1]*x[1]))'
inivexpr = dolfin.Expression((inivstrg, inivstrg), degree=2)
inivfunc = dolfin.interpolate(inivexpr, V)
inivvec = inivfunc.vector().get_local()

burgsol = solve_ivp(brhs, (t0, tE), inivvec, t_eval=timegrid, method='RK23')
fullsol = burgsol.y


Here is the result. Note the sharp front that develops towards the end of the time
integration.


Solution snapshots of the full FEM model 

2. POD Reduced Model

If one has snapshots of the solution $v _ h$ at some time instances $t _ i$ one
may well think that the span of the matrix of snapshots



is a good candidate for a space in which the solution evolves in. One may even
go further and look for a low-dimensional basis of this space. The span of a
matrix is best approximated by its dominant singular vectors. And this is the idea of
Proper Orthogonal Decomposition (POD) – use the leading singular vectors as a
basis for the solution space.

We use the Nts=101 snapshots of the FEM solutions to setup the matrix of
measurements $X$ and to compute the POD modes as $M^{-1/2}v _ k$, where $v _ k$
is the $k$-th leading left singular vector of $M^{1/2}X$. This procedure gives a
low-dimensional orthogonal (in the discrete $L^2$ inner product) basis that
optimally parametrizes the subspace of $L^2$ that is spanned by the solution
snapshots1. In this example, we use the poddim=25 leading singular vectors
to define the reduced model.

snapshotmat = mfac.Ft.dot(burgsol.y)
podmodes, svals, _ = spla.svd(snapshotmat, full_matrices=False)
selected_podmodes = podmodes[:, :poddim]
podvecs = mfac.solve_Ft(selected_podmodes)

In this implementation we use a sparse factor of the mass matrix instead of the
square root. The singular values (in particular those that correspond to the
discarded directions) give an indication of how good the approximation is.


The singular values of the snapshot matrix 

Here, the decay is comparatively slow, so that a one should not expect a good
low dimensional approximation by POD.

For the simulation, the state is parametrized by $u_h (t) \approx V \tilde
u_h(t)$ where $V$ is the matrix of the POD modes (in the code $V$ denoted by
podvecs), which gives a system in $\tilde u _ h$ with 25 degrees of freedom
(as opposed to the 51842 of the full order model).

redamat = podvecs.T.dot(amat.dot(podvecs))  # the projected stiffness

def redbrhs(time, redvec):
    inflatedv = podvecs.dot(redvec)
    redconv = podvecs.T.dot(burgers_nonl_vec(inflatedv))
    return -redamat.dot(redvec) - redconv.flatten()


Here we define the projected stiffness matrix and the reduced nonlinearity
through


  inflating the reduced state to full dimension
  applying the nonlinearity
  projecting down the result.


This means that our model is not completely independent of the full dimension.
For this problem there are hyperreduction techiques like DEIM.

Thus, the right hand side is readily defined the more that the projected mass
matrix is the identity. Why?

Finally, the initial value is projected into the reduced coordinates and the
reduced system is integrated in time.

redburgsol = solve_ivp(redbrhs, (t0, tE), prjinivvec,
                       t_eval=timegrid, method='RK23')
podredsol = redburgsol.y


In the solution we see that the reduced order model gives a decent approximation
in the smooth regime in the beginning and has its troubles approximating the front as can be seen in the error (log) plot.




Snapshots of the solution of the reduced system 


Snapshots of the log of the error between the full and the POD solution

3. DMD Reduced Model

POD is partially data driven – it uses data to create a basis but still
uses (a projection of) the model. If only snapshots but no model is given, one
may use the method of Dynamic Mode Decomposition2 (DMD) that tries to identify
a matrix $A$ that evolves the state like



In practice, one uses a set of snapshots and the two measurement matrices



and



Note that $X’$ is basically $X$ shifted by one time step.

Then the DMD matrix can be found by solving the linear regression problem



For the reduced DMD model we use the same snapshot matrix as for the POD. The
regression problem is solved via SVD to compute the needed pseudo inverse since
this naturally allows for a rank reduction and a factored representation of the
DMD matrix.

# ### dmd using truncated svd inverse

fburgsol = burgsol.y
Xmat = fburgsol[:, :-1]
Xdsh = fburgsol[:, 1:]
ux, sx, vxh = spla.svd(Xmat, full_matrices=False)
uxr, sxr, vxhr = ux[:, :poddim], sx[:poddim], vxh[:poddim, :]

# compute the dmd matrix in factored form: `dmda = dmdaone * dmdatwo`

dmdaone = Xdsh.dot(vxhr.T)
dmdatwo = np.linalg.solve(np.diag(sxr), uxr.T)


Once the DMD matrix $A$ is determined, the simulation of the DMD reduced model
is only a repeated multiplication by $A$.

# simulation of the dmd reduced model

dmdxo = inivvec
dmdsol = [dmdxo]
for k in np.arange(Nts):
    dmdsol.append(dmdaone.dot(dmdatwo.dot(dmdsol[-1])))

dmdsol = np.array(dmdsol).T


As can be seen from the results and the error plots, DMD does a good job in the
initial phase but fails in the region with the sharp front.


Snapshots of the DMD solution


4. Remarks

It is commonly accepted that POD does not work well for transport dominated
problems – like the current case with the low viscosity parameter nu=1e-4.

So, I think that the results for POD are quite good noting that the reduced
order model has 25 degrees of freedom whereas the full model has 51842.
Nonetheless, in my tests, increasing the number of basis functions did not help
much. One can use a larger nu to get better POD approximations.

The DMD approach shows a similar performance. If compared to POD, the
qualitative approximation looks less good but the numbers are slightly better.
All in all, the DMD approximation seems less reliable as for some parameter
choices, the performance severely deteriorated.

Differential Equations.* arXiv:1611.04050

Decomposition: Theory and Applications* arXiv:1312.0041

  
    
      See, e.g., Lemma 2.5 of Baumann, Benner, and Heiland (2018): *Space-Time Galerkin POD with Application in Optimal Control of Semi-linear Parabolic Partial &amp;#8617;
    
    
      See, e.g., Tu, Rowley, Luchtenburg, Brunton, Kutz (2013): *On Dynamic Mode &amp;#8617;
    
  

</description>
        <pubDate>Sat, 07 Mar 2020 00:00:00 +0100</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0009</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0009</guid>
        
        
        <category>tutorial</category>
        
        <category>WP04</category>
        
      </item>
    
      <item>
        <title>Inverse design for the one-dimensional Burgers equation</title>
        <description>The problem

We consider the following one-dimensional Burgers equation



where $u$ is the state, $u_0$ is the initial state and  the flux function $f$ is defined by $f(u)=\frac{u^2}{2}$. Kruzkov’s theory  provides existence and uniqueness of a  solution  of \eqref{eq} with initial datum $u_0 \in L^{\infty}(\mathbb R)$. This solution is called a weak-entropy solution, denoted by $(t,x) \to S_t^+(u_0)(x)$. For a given target function $u^T$, we introduce the backward entropy  solution $(t,x) \to S^-_{t}(u^T)(x)$  as follows: 
for every $t\in [0,T]$, for a.e $x\in \mathbb R$,



We study the problem of inverse design for \eqref{eq}. This problem consists in  identifying the set of initial data evolving to a given target at a final time.

Due to the time-irreversibility of the Burgers equation, some target functions are unattainable from weak-entropy solutions of this equation, making the inverse problem under consideration ill-posed. To get around this issue, we introduce the following  optimal control problem



where $u^T$ is a given target function and the class of admissible initial data $\mathcal{U}^0_{\text{ad}}$ in \eqref{opt2} is defined by



Above,  stands for functions of bounded variation and  $C&amp;gt;0$ is a constant large enough. The study of \eqref{opt2} is motivated by the minimization of the sonic boom effects generated by supersonic aircrafts [2].

To solve the optimal control problem \eqref{opt2}, some difficulties arise from a  theoretical  and  numerical point of view.

  Since the  entropy solution $u$ of \eqref{eq} may contain shocks even if the initial datum is a smooth function, this generates important added difficulties that have been the object of intensive study in the past, see  [3,4] and the references therein. In particular, the authors make sense of the derivative of $J_0$ in \eqref{opt2} in a weak way by requiring  strong conditions on the set of initial data. This leads to require that entropy solutions of \eqref{eq} have a finite number of non-interacting jumps.
  When $J_0$ is weakly differentiable,  gradient descent  methods have been implemented in [1,5,6] to solve numerically the optimal problem \eqref{opt2}. In the cases where it was applied successfully, only one possible initial datum emerges, namely the backward entropy solution $S_T^-(u^T)$. This is mainly due to the numerical viscosity that numerical schemes introduce to gain stability. To find some multiple minimizers, the authors in  [8]  use a filtering step in the backward adjoint solution.


Dans [9], we fully characterize the set of minimizers of the optimal control problem \eqref{opt2}.

Theorem 1.  Let $u^T\in BV(\mathbb R)$. The optimal control problem \eqref{opt2}  admits multiple optimal solutions. Moreover, for a.e $T&amp;gt;0$, the initial datum $u_0\in BV(\mathbb R)$ is an optimal solution of \eqref{opt2} if and only if $u_0 \in BV(\mathbb R)$ verifies $S_T^+(u_0)=S_T^+ (S_T^-(u^T))$.

A characterisation of the set  is given in [7]. An illustration of Theorem 1. is given in Figure 1.


Figure 1: The backward-forward  solution $S_T^+(S_T^-(u^T))$ is the projection of $u^T$ onto the set of attainable target functions. The shaded area in red at time $t=0$ represents the set of minimizers of \eqref{opt2} 

The proof of Theorem 1 is structured as follows. From [7, Theorem 3.1, Corollary 3.2] or [8, Corollary 1], there exists $u_0\in BV(\mathbb R)$ such that  if and only if  satisfies  the one-sided Lipschitz condition, i.e 

 Thus, the optimal problem \eqref{opt2} can be rewritten as  follows.



where the admissible set $\mathcal{U}^T_{\text{ad}}$ is defined by



Above, $K_1$ an open bounded interval large enough.  Note that the optimal problem \eqref{opt5} is not related to the PDE model \eqref{eq}. We prove that $q=S_T^+ (S_T^-(u^T))$ is a critical point of \eqref{opt5} using the first-order optimality conditions applied to \eqref{opt5} and the full characterization of the set  given in [9, Theorem A.2].

Numerical simulations

In [9,Section 3], we  implement a wave-front tracking algorithm to construct numerically the set of  minimizers of \eqref{opt2}.  We consider for instance, a target function   defined by



From Theorem 1,  the backward solution  is an  optimal solution of \eqref{opt2} and   is an optimal solution of \eqref{opt2} if and only if  . In Figure 2, the target function , the backward solution  and  the backward-forward solution  are plotted.



    
        
        
        
        
        
        
    
    
        
        $x\to S^{-}_T(u^T)(x)$ 
        
        
        $(t,x)\to S^{+}_t(S^{-}_T(u^T))(x)$ 
         
    




$u^T$  and $\color{red}{x\to S^{+}_T(S^{-}_T(u^T))(x)}$ 
$ $
Figure 2. Plotting of the target function $u^T$ defined in \eqref{uT}, the optimal solution $S_T^-(u^T)$ and the backward-forward solution $\color{red}{S^{+}_T(S^{-}_T(u^T))}$.

Note that  has four different shocks located at $x=1.1$, $x=3.1$, $x=5.3$ and $x=7.2$. If we use a conservative numerical method as Godunov scheme,  the approximate solution of  doesn’t have shocks because of numerical viscosity that numerical schemes introduced, see Video 1.



    
        
            
        
        
            
        
    


Video 1. Approximate solution of $S_T^+(u_0)$ with $u_0$ an $N$-wave constructed with $ \color{red}{\text{a wave-front tracking algorithm}}$ and $\color{blue}{\text{a Godunov scheme}}$ 

This implies that only one minimizer of \eqref{opt2} can be constructed using a Godunov scheme, which is the backward entropy solution $S_T^-(u^T)$. When a wave-front tracking algorithm is implemented, the approximate solution of $S_T^+(S_T^-(u^T))$ has shocks since we track the possible discontinuities from $u^T$ to  $S_T^+(S_T^-(u^T))$. This implies that all initial data $u_0$ that coincide with the approximate solution of $S_T^+ (S_T^-(u^T))$ can be recovered, see [9,Section 3].

In Video 2, we show that the weak-entropy solution of \eqref{eq}  with initial data $S_T^-(u^T)$ coincides with $S_T^+ (S_T^-(u^T))$ at time $T$.


Video 2. Approximate solution of $(t,x) \to S_t^+(S_T^-(u^T))(x)$  using a wave-front tracking algorithm. 

In Video 3, three other approximate optimal solutions $u_0$ of \eqref{opt2} are constructed. In particular, we show that  $S_T^+ (u_0)=S_T^+ (S_T^-(u^T))$.



    
        
            
        
        
            
        
    

    
                
            
    
    



Video 3. Three approximate optimal solutions of \eqref{opt2} constructed using a wave-front tracking algorithm 

[1] Navid Allahverdi,  Alejandro Pozo and Enrique Zuazua. Numerical aspects of large-time optimal control of Burgers equation. ESAIM: Mathematical Modelling and Numerical Analysis, 50 (5):1371-1401,2016.

[2]  Navid Allahverdi,  Alejandro Pozo and Enrique Zuazua. Numerical aspects of sonic-boom minimization. A panorama of Mathematics: Pure and Applied, 658:267,2016.

[3] François Bouchut and François James. One-dimensional transport equations with discontinuous coefficients. Nonlinear Analysis, 32(7):891,1998.

[4] Alberto Bressan and Andrea Marson. A maximum principle for optimally controlled systems of conservation laws. Rendiconti del Seminario Matematico della Universita di Padova, 94:79-94, 1995.

[5] Carlos Castro, Francisco Palacios and Enrique Zuazua. An alternating descent method for the optimal control of the inviscid Burgers equation in the presence of shocks. Mathematical Models and Methods in Applied Sciences, 18(03):369-416,2008.

[6] Carlos Castro, Francisco Palacios and Enrique Zuazua. Optimal control and vanishing viscosity for the Burgers equation. In Integral Methods in Science and Engineering, Volume 2, pages 65-90. Springer, 2010.

[7] Rinaldo Colombo and Vincent Perrollaz. Initial data identification in conservation laws and Hamilton-Jacobi equations. arXiv preprint arXiv:1903.06448,2019.

[8] Laurent Gosse and Enrique Zuazua. Filtered gradient algorithms for inverse design problems of one-dimensional Burgers equation. In Innovative algorithm and analysis, pages 197-227. Springer, 2017.

[9] Thibault Liard and Enrique Zuazua. Inverse design for the one-dimensional Burgers equation. Submitted (2019).
</description>
        <pubDate>Mon, 02 Mar 2020 00:00:00 +0100</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0007</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0007</guid>
        
        
        <category>tutorial</category>
        
        <category>WP04</category>
        
      </item>
    
      <item>
        <title>An eulerian-lagrangian scheme for the problem of the inverse design of hyperbolic transport equations</title>
        <description>The inverse design of hyperbolic transport equations can be addressed by using gradient-adjoint methodologies. Recently, Morales-Hernandez and Zuazua [1] investigated the convenience of using low order numerical schemes for the adjoint resolution in the gradient-adjoint method. They focused on hyperbolic transport scalar equations with an heterogeneous time-independent vector field.

Morales-Hernandez and Zuazua analysed the numerical resolution of the adjoint equation by means of two methods: a First Order Upwind $(FOU)$ scheme and a Second Order Upwind $(SOU)$ scheme. They concluded that the $FOU$ scheme is the best choice when dealing with smooth functions once the $SOU$ scheme introduces some spurious oscillations. In addition, the $FOU$ scheme requires shorter computational time than the $SOU$ scheme. The non-linear transport problems in which sharp interfaces or discontinuities may appear in the functions has remained to be studied.

The numerical scheme for the adjoint equation determines the direction of descent in the iteration procedure, and consequently the $CPU$ time consumed by the solver. As the $CPU$ time is a decisive factor, we propose an eulerian-lagrangian scheme that uses the Modified Method of Characteristics ($MMOC$) [2] for solving the transport and adjoint equations. The $MMOC$ method is based on the characteristic curves, and so very computationally competitive for linear hyperbolic transport equations. The $MMOC$ was initially developed performing the resolution in a forward sense. In this work, the eulerian-lagrangian scheme is used for the transport and adjoint resolution, employing it in both forward and backward senses.

We perform numerical tests in order to investigate the accuracy and efficiency of the eulerian-lagrangian scheme to solve the problem of inverse design of linear and non-linear hyperbolic transport scalar equations.

The gradient-adjoint method

Consider the following hyperbolic transport scalar equation with a vector flux function ${\bf f} (u)$:

\begin{equation}\label{eq1}
\frac{\partial u}{\partial t} + \nabla \cdot {\bf f} (u) = 0, ~ ~ u({\bf x},0) = u_0.
\end{equation}

Given a target function $u^* \in \bf H$, $\bf H$ being a Hilbert space, the problem of inverse design consists of achieving $u_0 \in \bf H$ such that the solution of the former evolution equation satisfies $u(T) = u^*$. This problem is solved via the minimization of the functional

\begin{equation}\label{eq2}
J(u_0) = \frac{1}{2} \int_{0}^{T} \int_{\Omega} (u(T) - u^*)^2 d\Omega dt
\end{equation}

by means of the gradient descent method:

\begin{equation}\label{eq3}
{u_0}^{k+1} = {u_0}^k -  \epsilon {\nabla J}^k,
\end{equation}

where $\epsilon$ is the step size, and the gradient $\nabla J = \sigma (0)$, in which $\sigma$ is the adjoint variable . The gradient $\sigma (0)$ is achieved by solving the adjoint equation:



The gradient-adjoint method is based on iterating a loop, where the transport equation (\ref{eq1}) is solved in a forward sense, while the adjoint equation (\ref{eq4}), which is of hyperbolic nature as well, is solved backwards in time.

The $MMOC$ in forward sense

Eulerian-lagrangian approaches provide computationally efficient techniques for approximating the solution of hyperbolic transport equations. Douglas and Russel [2] introduced an eulerian-lagrangian method based on the characteristic curve - the $MMOC$ - to develop fast solvers for hyperbolic transport equations. Despite the $MMOC$ does not provide a local conservation of the identity associated with the function, it present good performance for linear hyperbolic transport equations.

We use the $MMOC$ method for solving the transport and adjoint equations in the context of the problem of inverse design of hyperbolic transport equations. In this problem, the adjoint equation is always a linear differencial equation, worthing the use of the eulerian-lagrangian scheme. The gradient-adjoint iteration requires the adjoint equation to be solved in backward sense. We employ an eulerian-lagrangian scheme that uses the $MMOC$, considering procedures in forward and backward senses.

Consider a scalar hyperbolic transport equation determining the initial value problem

\begin{equation}\label{eq5}
\frac{\partial u}{\partial t} + \nabla \cdot {\bf f} (u) = 0, ~ ~ u({\bf x},0) = u_0 ,
\end{equation}

where ${\bf f} (u)$ is a vector flux function. Rewriting the Eq. (\ref{eq5}) in nondivergence form,

\begin{equation}\label{eq6}
\frac{\partial u}{\partial t} + {\bf f}^\prime (u) \cdot \nabla u  = 0 .
\end{equation}

The solution of Eq. (\ref{eq6}) is essentially along the characteristic curves of the transport operator $\partial / \partial t + {\bf f}^\prime (u) \cdot \nabla$, so that it is appropriate to introduce differentiation in this characteristic direction. Let

\begin{equation}\label{eq7}
\partial / \partial t + {\bf f}^\prime (u) \cdot \nabla = \psi \frac{\partial}{\partial \tau} , ~~\psi (u) = \sqrt {1 + {\left \lVert {\bf f}^\prime (u) \right \rVert}^2} , 
\end{equation}

in which the direction $\tau$ depends on $\bf x$.

Let us consider the discretization of  Eq. (\ref{eq6}) in time. Denote the time step $\Delta t &amp;gt; 0$ and consider the approximation of the solution at times $t^n = n \Delta t$. In the standard $MMOC$, the characteristic derivative is approximated by

\begin{equation}\label{eq8}
\psi \frac{\partial u}{\partial \tau} \approx \psi (u({\bf x}, t^{n-1})) \frac {u({\bf x}, t^{n}) - u(\overline {\bf x}^n, t^{n-1})}{\sqrt {\left \lVert {\bf x} - \overline {\bf x}^n \right \rVert^2 + (\Delta t)^2 }} =  \frac {u({\bf x}, t^{n}) - u(\overline {\bf x}^n, t^{n-1})}{\Delta t} ,
\end{equation}

where

\begin{equation}\label{eq9}
\overline {\bf x}^n =  \overline {\bf x}^n ({\bf x}) = {\bf x} - {\bf f}^\prime (u({\bf x}, t^{n-1}) \Delta t .
\end{equation}

Let $U^n ({\bf x})$ denote the approximations to $u({\bf x},t^n)$. For a predecessor position

\begin{equation}\label{eq10}
\overline {\bf X}^n ({\bf x}) = {\bf x} - {\bf f}^\prime (U^{n-1} ({\bf x})) \Delta t,
\end{equation}

the transported function is defined as

\begin{equation}\label{eq11}
\overline {U}^n ({\bf x}) =  u (\overline {\bf X}^n ({\bf x}), t^{n-1}) .
\end{equation}

As the continuous in space approximation for Eq. (\ref{eq6}) is

\begin{equation}\label{eq12}
\frac{U^n ({\bf x}) - \overline U^{n} ({\bf x})}{\Delta t} = 0 ,
\end{equation}

the $MMOC$ scheme is given by



Preliminary results

The performance of the $MMOC$ scheme is checked in numerical tests, where two hyperbolic transport problems are considered. The results by $MMOC$ are compared with the ones by the Lax-Wendroff ($LW$), which is a second-order central scheme.

2D Doswell frontogenesis

The first test case symbolizes the presence of horizontal temperature gradients and fronts in the context of meteorological dynamics. In this linear problem the flux function is

\begin{equation}
{\bf f} (u) =  {\bf v} (x,y)~ u ,
\end{equation}

where ${\bf v} (x,y)= g~(-y,~x)$, $g = \frac{1}{r}2.59807~sech^2(r)~tanh(r)$, and $r = \sqrt{x^2 + y^2}$. The exact solution of this problem is given by

\begin{equation}
u (x,y,t) = tanh(y~cos(gt) - x~sin(gt)).
\end{equation}

The domain $\Omega = [-5,5]^2$ is discretized in 40000 square cells, a step size $\epsilon = 0.5$ is selected and the Courant number is set to $CFL = 1.0$. The results are shown in Fig.1 for the initial condition and target function at $T = 4s$. The target function in a vortex-type profile is presented by both the $LW$ and $MMOC$ schemes.



    
        
            
        
        
            
        
    
    
        
            
        
        
            
        
    



 Figure 1. The initial condition and target function by $LW$ (top) and MMOC (bottom).   


Results of $CPU$ time and $L_2$ norm of the numerical initial condition with respect to the exact initial condition are displayed in Table [1]. The results are quite the same for the $LW$ and $MMOC$, since we have a linear transport equation.




    
		  $Scheme$ 
    
    
         $CPU~ time~ (s)$ 
    
    
        $L_2~ norm$ 
    




        
		  $LW$ 
    
    
         49.0
    
    
        4.07
    


        
		  $MMOC$ 
    
    
         80.8
    
    
        3.96
    




Table 1. Performance for initial condition in the Doswell frontogenesis.

Inviscid Burgers equation

We consider the Burgers equation with zero viscosity term which is called the inviscid Burgers equation. This equation is one of the most useful formulation of the behaviour of the shock waves in which nonlinear advection can be observed. In this nonlinear problem the flux function is

\begin{equation}
f (u) =  \frac{1}{2} u^2 .
\end{equation}

The exact solution of this problem is given by

\begin{equation}
u(x,t) = sin(x - ut) .
\end{equation}

The domain $\Omega = [0,32]^2$ is discretized in 4096 square cells, a step size $\epsilon = 0.01$ is selected and the Courant number is set to $CFL = 0.1$. The exact and $MMOC$ initial conditions are shown in Fig 2 considering zero values as a guess initial condition. Only with the $MMOC$ scheme the numerical solver was able to converge to the initial condition for the selected step size.


    
    
        
    
    
        
    
    

     Figure 2.The exact (left) and $MMOC$ (right) initial conditions.


In order to observe the convergence to the initial condition, we set $u_0(x) = \beta sin(x)$ as a guess initial condition, where $\beta$ is a constant. As $\beta$ is next to one, the guess initial conditions is closer to the exact initial condition. Table 2 displays the $CPU$ time and $L_2$ norm of the numerical initial condition with respect to the exact initial condition for several $\beta$ values. The numerical solver using $MMOC$ scheme converged for any $\beta$ value, showing that the norm is smaller as the $\beta$ is increased.



    
        $\beta$    $LW$  $MMOC$
    
    
        0.1    3.68  3.39
    
    
        0.5    2.82  2.70
        
    
        0.9    0.95  1.19
    




Table 2. Performance for initial condition by $MMOC$ in the Burgers problem.


Conclusions

The results achieved and presented here are preliminary ones. The $MMOC$ is a promising eulerian-lagrangian scheme for the problem of inverse design of nonlinear transport equations. More complex problems and another eulerian-lagrangian scheme are to be tested. The next steps are the follows:


  simulate and investigate the Buckley-Leverett problem;
  simulate and investigate the miscible tracer flow; and
  use the $LCELM$ eulerian-lagrangian scheme [3].


References

[1] M. Morales-Hernandez, E. Zuazua, Adjoint computational methods for 2d inverse design of linear transport equations on unstructured grids, Computational and Applied Mathematics (2019)

[2] J. Douglas, T. F. Russell, Numerical methods for convection dominated diffusion problems based on combining the method of characteristics with finite element or finite difference procedures, SIAM J.
Numer. Anal. (1982)

[3] J. Douglas, F. Pereira, L.-M. Yeh, A locally conservative euleriana lagrangian numerical method and its application to nonlinear transport in porous media, Computational Geosciences (2000)
</description>
        <pubDate>Tue, 18 Feb 2020 00:00:00 +0100</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0008</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0008</guid>
        
        
        <category>tutorial</category>
        
        <category>WP04</category>
        
      </item>
    
      <item>
        <title>Stabilization of a double pendulum on a cart with DyCon Toolbox</title>
        <description>The finite-dimensional dynamical system given by a pendulum on a cart (cartpole) is widely considered as a benchmark for comparing the performance of different control strategies. In this blog post, we consider a double pendulum on a cart and we solve the problem of swinging up the pendulum from the downward position to the upward position using optimal control techniques.

Setup

In this tutorial we need DyCon Toolbox, to install it we will have to write the following in our MATLAB console:

unzip('https://github.com/DeustoTech/DyCon-Computational-Platform/archive/master.zip')
addpath(genpath(fullfile(cd,'DyCon-toolbox-master')))


Equations of motion



  Figure 1. Scheme of system dynamics


The physical system defined by the double pendulum in a cart can be modelled as a system of six ordinary differential equations, whose state at each instant of time $t \geq 0$ is the vector $y(t) = \left( x(t), \theta_{1}(t), \theta_{2}(t), v(t), \omega_{1}(t), \omega_{2}(t) \right)$, where:


  $x(t) \in \mathbb{R}$ is the position of the cart.
  $\theta_{i}(t) \in [0, 2\pi]$ is the angle of the $i$-th pendulum.
  $v(t) \in \mathbb{R}$ is the velocity of the cart.
  $\omega_{i}(t) \in \mathbb{R}$ is the angular velocity of the $i$-th pendulum.


The equations of motion of the system can be derived using the Lagrangian formalism of classical mechanics and are given by:



where, for each $t \geq 0$, we define:



and



We have the fixed parameters:


  $d_{i} \in [0,1]$ are damping coefficients.
  $m &amp;gt; 0$ is the mass of the cart and $m_{i} &amp;gt; 0$ is the mass of the $i$-th pendulum.
  $l_{i} &amp;gt; 0$ is the length of the $i$-th pendulum.


The function $u \in L^{\infty}(0,\infty; \mathbb{R})$ is a time-dependent control that acts on the acceleration term $\dot{v}(t)$ of the cart, and can thus be interpreted as a force that is exerted on the cart.

This dynamical system shows chaotic behaviour. The following animation displays the evolution of the double pendulum on a cart with control $u = 0$ and starting at the two different initial conditions










  Figure 2. Chaotics behaviour


  Optimal Control Problem 

We will solve the problem of steering the state of the pendulum from the downward position $y_{0} = (0, \pi, \pi, 0, 0, 0)$ to the upward position $z = (0, 0, 0, 0, 0, 0)$ in a fixed time horizon $T &amp;gt; 0$ using a control $u(t)$.

One way to look for solutions to this problem is to minimize the following functional:



Where $\alpha &amp;gt;0$ is a fixed parameter, the state vector $y(t)$ is subject to the system of differential equations (1) and $y(0) = y_0$.

  Numerical simulation 

We start by implementing the dynamical system defined in (1) as a MATLAB function. We fix the lengths $l_{i} = 1$, and the mass of the cart $M = 50$. The damping coefficients are set to $d_{i} = 0.1$.

  &amp;gt;&amp;gt; type cartpole_dynamics.m

function ds = cartpole_dynamics(t,s,u,params)
    L1 = 1;    L2 = 1;    g = 9.8;
    m1 = params.m1;    m2 = params.m2;
    M = 50;
    d1 = 1e-1;    d2 = 1e-1;    d3 = 1e-1;
    %%%%%%%%%%%%%%%%%%%%%5%%%
    ds = zeros(6,1,class(s));
    %
    q      = s(1);    theta1 = s(2);
    theta2 = s(3);    dq     = s(4);
    dtheta1= s(5);    dtheta2= s(6); 
    % q
    ds(1) = dq -q;
    % theta 1
    ds(2) = dtheta1;
    % theta 2
    ds(3) = dtheta2;
    %
    Mt = [ M+m1+m2                  L1*(m1+m2)*cos(theta1)     m2*L2*cos(theta2) ; ...
          L1*(m1+m2)*cos(theta1)     L1^2*(m1+m2)             L1*L2*m2*cos(theta1-theta2) ; ...
          L2*m2*cos(theta2)          L1*L2*m2*cos(theta1-theta2)  L2^2*m2        ];
    % v
    f1 = L1*(m1+m2)*dtheta1^2*sin(theta1) + m2*L2*theta2^2*sin(theta2)      - d1*dq + u  ;
    f2 = -L1*L2*m2*dtheta2^2*sin(theta1-theta2) + g*(m1+m2)*L1*sin(theta1)  - d2*dtheta1;
    f3 =  L1*L2*m2*dtheta1^2*sin(theta1-theta2) + g*L2*m2*sin(theta2)       - d3*dtheta2; 

    ds(4:6) = Mt\[f1;f2;f3];
end


This optimal control problem is solved using CasADi with the IPOPT solver. MATLAB’s symbolic toolbox has been used to implement the system dynamics in CasADi.

The time horizon is set to $T = 10$ and the system (1) has been discretized using a Crank-Nicolson scheme. The use of an implicit discretization method with good behaviour with respect to the number of mesh points has been crucial to speed up the optimization process.

import casadi.*
% define optimal 

Ss = SX.sym('x',6,1);As = SX.sym('u',1,1);ts = SX.sym('t');
%
EvolutionFcn = Function('f',{ts,Ss,As},{ cartpole_dynamics(ts,Ss,As,params) });
%
dynamics_obj = ode(EvolutionFcn,Ss,As,tspan);
dynamics_obj.InitialCondition = s0;
%
PathCost  = Function('L'  ,{ts,Ss,As},{ (  (Ss.'*Ss) + 1e-3*(As.'*As) ) });
FinalCost = Function('Psi',{Ss}      ,{ (  (Ss.'*Ss) ) });

ocp_obj = ocp(dynamics_obj,PathCost,FinalCost);
%
U0 =0*tspan;
[OptControl ,OptState] = IpoptSolver(ocp_obj,U0,'integrator','CrankNicolson');



The following animation displays the resulting evolution of the system’s state:






  Figure 3. Optimal solution 


Interestingly, due to the chaotic character of system (1), the IPOPT routine yields different controlled trajectories if we change the initial guess of the control:






  Figure 4. Several initial guess of optimization

</description>
        <pubDate>Thu, 13 Feb 2020 00:00:00 +0100</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0008</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0008</guid>
        
        
        <category>tutorial</category>
        
        <category>WP01</category>
        
      </item>
    
  </channel>
</rss>
