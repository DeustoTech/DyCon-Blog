<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DyCon Blog</title>
    <description>Welcome to the web interface of DyCon Toolbox, the computational platform developed within the &lt;a href='https://cmc.deusto.eus/dycon/' target='_blank'&gt;ERC DyCon - Dynamic Control&lt;/a&gt; project.</description>
    <link>http://localhost:4000/DyCon-Blog/</link>
    <atom:link href="http://localhost:4000/DyCon-Blog/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 31 Mar 2021 09:44:33 +0200</pubDate>
    <lastBuildDate>Wed, 31 Mar 2021 09:44:33 +0200</lastBuildDate>
    <generator>Jekyll v3.5.0</generator>
    
      <item>
        <title>Multilevel Selective Harmonic Modulation via Optimal Control</title>
        <description>Selective harmonic Modulation (SHM) [1,2] is a well-known methodology in power electronics engineering, employed to improve the performance of a converter by controlling the phase and amplitude of the harmonics in its output voltage. As a matter of fact, this technique allows to increase the power of the converter and, at the same time, to reduce its losses.

In broad terms, SHM consists in generating a control signal with a desired harmonic spectrum by modulating some specific lower-order Fourier coefficients. In practice, the signal is constructed as a step function with a finite number of switches, taking values only in a given finite set. Such a signal can be fully characterized by two features (see Fig. 1):


	The waveform, i.e. the sequence of values that the function takes in its domain.
	The switching angles, i.e. the sequence of points where the signal switches from one value to following one. 


In mathematical terms, the SHM problem can be formulated as follows. Let



be a given set of $L\geq 2$ real numbers satisfying $u_1 = -1$, $u_L = 1$ and $u_k&amp;lt;u_{k+1}$ for all $k\in {1,\ldots, L}$.

The goal is to construct a step function $u(t):[0,2\pi)\to\mathcal U$ with a finite number of switches, such that some of its lower-order Fourier coefficients take specific values prescribed a priori.

Due to applications in power converters, it is typical to only consider functions with half-wave symmetry, i.e. satisfying $u(t + \pi) = -u(t)$ for all $t \in [0,\pi)$.

In view of this symmetry, the Fourier series of $u$ only involves the odd terms (as the even terms just vanish), i.e.



with



and



Moreover, the functions we are considering are piecewise constant with a finite number of switches, taking values only in $\mathcal{U}$, i.e. of the form



for some $\mathcal S = {s_m}_{m=0}^M$ satisfying



and $\Phi = {\phi_m}_{m=1}^{M}$ such that



Here $\mathcal S$ and $\Phi$ are the waveform and the switching angles that we mentioned above.

Observe that any function $u$ of the form \eqref{eq:uExpl} is fully characterized by its waveform $\mathcal S$ and switching angles $\Phi$. An example of such a function is displayed in Fig. 1


  


 Figure 1: a possible solution to the SHM Problem, where we considered the control-set $\mathcal{U} = \{-1, -1/2, 0, 1/2, 1\}$. We show the switching angles $\Phi$ and the waveform $\mathcal S$. The function $u(t)$ is displayed on the whole interval $[0,2\pi)$ to highlight the half-wave symmetry.


In practical engineering applications, due to technical limitations, it is preferable to employ signals taking consecutive values in $\mathcal{U}$. This property of the waveform, to which we shall refer as the staircase property, can be rigorously formulated as follows:

Definition: We say that a signal $u$ of the form \eqref{eq:uExpl} fulfills the staircase property if its waveform $\mathcal S$ satisfies



We can now formulate the SHM problem as follows.

 SHM problem: let $\mathcal{U}$ be given as in \eqref{eq:Udef}, and let $\mathcal E_a$ and $\mathcal E_b$ be finite sets of odd numbers of cardinality $\vert\mathcal{E}_a\vert=N_a$ and $\vert\mathcal{E}_b\vert=N_b$ respectively. For any two given vectors $a^T\in\mathbb{R}^{N_a}$ and $b^T\in\mathbb{R}^{N_b}$, construct a function $u:[0,\pi)\to\mathcal{U}$ of the form \eqref{eq:uExpl}, satisfying \eqref{eq:staircase prop}, such that the vectors



satisfy $a = a^T$ and $b = b^T$.

SHM as an optimal control problem

To solve the SHM problem, in [3] we propose an optimal control-based approach in which the Fourier coefficients of the signal $u(t)$ are identified with the terminal state of a controlled dynamical system of $N_a+N_b$ components defined in the time-interval $[0,\pi)$.  The control of the system is precisely the signal $u(t)$, defined as a function $[0,\pi)\to \mathcal{U}$, which has to steer the state from the origin to the desired values of the prescribed Fourier coefficients.

The starting point of this approach is to rewrite the Fourier coefficients of the function $u(t)$ as the final state of a dynamical system controlled by $u(t)$. To this end, let us first note that, by definition, for all $u\in L^\infty ([0,\pi);\mathbb{R})$ any Fourier coefficient $a_j$ satisfies $a_j = y(\pi)$, with $y\in C([0,\pi);\mathbb{R})$ defined as



Besides, as a consequence of the fundamental theorem of calculus, $y(\cdot)$ is the unique solution to the differential equation



Analogously, we can also write the Fourier coefficients $b_j$ as the solution at time $t=\pi$ of a similar differential equation.

Hence, for $\mathcal{E}_a$, $\mathcal{E}_b$, $a^T$, and $b^T$ given, the SHM Problem can be reduced to finding a control function $u$ of the form \eqref{eq:uExpl}, satisfying \eqref{eq:staircase prop}, such that the corresponding solution $\textbf{y}\in C([0,\pi);\mathbb{R}^{N_a+N_b})$ to the dynamical system



satisfies $\textbf{y}(\pi) = [a^T;b^T]^\top$, where



with $\mathcal{D}^a(t) \in \mathbb{R}^{N_a}$ and $\mathcal{D}^b(t) \in \mathbb{R}^{N_b}$ given by



Here, $e_a^i$ and $e_b^i$  denote the elements in $\mathcal{E}_a$ and $\mathcal{E}_b$, i.e.


Moreover, we can reverse the time using the transformation $\textbf{x} (t) = \textbf{y}(\pi - t)$, so that the SHM problem turns into the following null controllability one, for a dynamical system with initial condition $\textbf{x}(0) = [a^T;b^T]^\top$.

 SHM problem via null controllability: let $\mathcal{U}$ be given as in \eqref{eq:Udef}. Let $\mathcal{E}_a$, $\mathcal{E}_b$ and the targets $a^T$ and $b^T$ be given. We look for a function $u: [0,\pi)\to [-1,1]$ of the form \eqref{eq:uExpl}, satisfying \eqref{eq:staircase prop}, such that the solution to the initial-value problem



satisfies $\textbf{x} (\pi) = 0$.

A natural approach for null controllability problems is to formulate them as optimal control ones. In the case of the SHM problem just presented, in [3] we proposed to obtain the optimal control $u$ in the following way.

Penalized OCP for SHM: fix $\varepsilon&amp;gt;0$ and a convex function $\mathcal{L}\in C([-1,1];\mathbb{R})$. Let $\mathcal{E}_a$, $\mathcal{E}_b$ and the targets $a^T$ and $b^T$ be given, and denote



We look for a control $u\in \mathcal A$ solution to the following optimal control problem:



Observe that in the above formulation we do not impose the constraint that the control has to be of the form \eqref{eq:uExpl}, satisfying the staircase property \eqref{eq:staircase prop}. These features of $u$ will arise naturally from a suitable choice of the penalization term $\mathcal{L}$, as we describe below.

Bi-level SHM problem via OCP (Bang-Bang Control)

In this case, the control set $\mathcal{U}$ defined in \eqref{eq:Udef} has only two elements, i.e. $\mathcal{U}={-1,1}$. In the control theory literature, a control taking only two values is known as bang-bang control. In the SHM literature, this kind of solution are called bi-level solutions. 
Moreover, in [3], we proved that bi-level controls are obtained through our optimal control approach when selecting $\mathcal L(u)=\alpha u$ with $\alpha\neq 0$.

Theorem 1: let $L=2$, $\mathcal{U}$ as in \eqref{eq:Udef}, and $\textbf{x}_0$ be given. For some $\alpha\in \mathbb{R}$ with $\alpha\neq 0$, consider the penalization $\mathcal{L} (u) = \alpha\, u$. Then, the optimal control $u^\ast$ is unique and has a bang-bang structure, i.e. it is of the form \eqref{eq:uExpl}. In addition to that, the solution $u^\ast$ is continuous with respect to $\textbf{x}_0$ in the strong topology of $L^1(0,\pi)$.

We point out that the continuity of the solution with respect to the target frequencies is a highly desirable property in real applications of SHM, and sometimes, difficult to achieve.

Multilevel SHM problem via OCP

Another typical situation, known in the power electronics literature as the multilevel SHM problem, is the case when $\mathcal{U}$ contains more than two elements.

In [3], we proved that multilevel controls are obtained when selecting $\mathcal L(u)$ as the interpolate a parabola in $[-1,1]$ by affine functions, considering the elements in $\mathcal{U}$ as the interpolating points.

Theorem 2: let $\textbf{x}_0$ be given, and let $\mathcal{U}$ be a given set as in \eqref{eq:Udef}. For any $\alpha&amp;gt;0$ and $\beta\in \mathbb{R}$, set the function



Consider the penalization



where



Assume in addition that the function $\mathcal{L}$ has a unique minimum in $[-1,1]$. Then, the optimal control $u^\ast$ is unique and has the form \eqref{eq:uExpl} satisfying \eqref{eq:staircase prop}. Moreover, the solution $u^\ast$ is continuous with respect to $\textbf{x}_0$ in the strong topology of $L^1(0,\pi)$.

The assumption of $\mathcal L$ having a unique minimum in $[-1,1]$ is
actually necessary to ensure the staircase form for the solution.
Not assuming this hypothesis would entail the possibility of having
continuous solutions for specific targets. Nevertheless, this assumption can be easily ensured by choosing, for instance, $\beta = \pm1$.

We illustrate in Fig. 2 different examples of penalization functions $\mathcal{L}$ giving rise to multilevel solutions to the SHM problem. We point out that, by varying the values of $\alpha$ and $\beta$ in $\mathcal P$, we can obtain solutions with different waveforms.


  

 
 Figure 2: some examples of convex piecewise affine penalization functions $\mathcal{L}$. The examples $\mathcal {L}_1$, $\mathcal{L}_2$ and $\mathcal{L}_3$ satisfy the hypotheses of Theorem 2. On the contrary, the function $\mathcal{L}_4$ has not a unique minimizer, and then, we cannot ensure that the solution has staircase form.


Numerical simulations

Let us now present several examples in which we implement the optimal control strategy we proposed to solve the SHM problem.

To solve our optimal control problem, we employ the direct method which, in broad terms, consists in discretizing the cost functional and the dynamics, and then apply some optimization algorithm. The dynamics is approximated with the Euler method, while for solving the discrete minimization problem we employ the nonlinear constrained optimization tool CasADi. CasADi is an open-source tool for nonlinear optimization and algorithmic differentiation which implements the interior point method via the optimization software IPOPT. To be efficiently applied to solve an optimal control problem, we then need the functional we aim to minimize to be smooth. While this is clearly true in the bi-level case, for the multilevel one the functional, due to the piecewise affine penalization, is not differentiable at the points $u_k\in\mathcal U$. For this reason, when treating the multilevel case, we will first need to build a smooth approximation of the  function $\mathcal L$.

Smooth approximation of $\mathcal L$ for multilevel control

The piecewise affine function defining the penalization $\mathcal L$ in case of multilevel controls can be regularized as follows.

First of all, for all real parameter $\theta&amp;gt;0$, we define the $C^\infty(\mathbb{R})$ function



and observe that, for almost every $x\in \mathbb{R}$,  we have that $h^\theta(x)\to h(x)$ as $\theta\to +\infty$, where $h$ is the Heaviside function



Secondly, for all $k \in {1,\dots,N_u-1}$ we define the (smooth) function $\chi_{[u_k,u_{k+1})}^\theta:\mathbb{R} \rightarrow \mathbb{R}$ given by



which, as $\theta\to +\infty$, converges in $L^\infty(\mathbb{R})$ to the characteristic function $\chi_{[u_k,u_{k+1})}$. Finally, we employ $\chi_{[u_k,u_{k+1})}^\theta$ to define



which, as $\theta\to +\infty$, converges in $L^\infty(\mathbb{R})$ to the penalization function $\mathcal L$ we defined for multilevel control.

Direct method  for OCP-SHE

The starting point for employing a direct method to solve the optimal control problem we are considering is to discretize the cost functional and the dynamics. To this end, let us consider a $N_t$-points partition of the interval $[0,\pi]$, $\mathcal{T} = {t_k}_{k=1}^{N_t}$, and denote by $\textbf{u} \in \mathbb{R}^{N_t}$ the vector with components $u_k = u(t_k)$, $k=1,\ldots,N_t$.

Then the optimal control problem can then be approximated by a finite-dimensional optimization one with variable $\textbf{u} \in \mathbb{R}^{N_t}$.

Numerical OCP: given two sets of odd numbers $\mathcal{E}_a$ and $\mathcal{E}_b$ with cardinalities $\vert\mathcal{E}_a\vert=N_a$ and $\vert\mathcal{E}_b\vert=N_b$, respectively, the target vectors $a^T\in\mathbb{R}^{N_a}$ and $b^T\in\mathbb{R}^{N_b}$, 
and the partition $\mathcal{T}$ of the interval $[0,\pi]$, we look for $\textbf{u}\in\mathbb{R}^{N_t}$ that solves the following minimization problem:



where



Numerical experiments

We now present several numerical experiments to show the effectiveness of our optimal control approach to solve SHM problems. All the examples share the following common parameters 

We consider the frequencies 

and the target vectors 


We shall consider three different control sets $\mathcal{U}$ which correspond to different types of control:


	Bang-bang control: $\mathcal{U} = \{-1,1\}$.
	Bang-off-bang control:  $\mathcal{U} = \{-1,0,1\}$.
	5-multilevel control: $\mathcal{U} = \{-1,-1/2,0,1/2,1\}$.


The results of our simulations are displayed in Fig. 3, 4 and 5. We have plotted the function



where, for each $m \in [-0.8,0.8]$, $u_m^\ast (\cdot)$ represents the solution to the SHM problem with the target frequencies we are considering.



  
  	
	 
	
	 
	 Figure 3: bang-bang control for the SHM problem.
	 
  

  
  
  

 
 Figure 4: bang-off-bang control for the SHM problem.
 





  

 
 Figure 5: multilevel control for the SHM problem.


In Fig. 3, 4 and 5, for each value of the parameter $m$ in the horizontal axis, we observe that the optimal control has the staircase structure. The controls take values only in $\mathcal U$, which are represented by the different colors displayed at the right. For instance, in Fig. 3, the control is $u=-1$ in the blue region and $u=1$ in the red one.

References

[1] J. Sun, S. Beineke and H. Grotstollen Optimal PWM based on real-time solution of harmonic elimination equaations, IEEE Trans. Power Electron., 11(4): 612-621, 1996.

[2] J. Sun and H. Grotstollen Solving nonlinear equations for selective harmonic eliminated PWM using predicted initial values, Proceedings of the 1992 intrenational conference on industrial electronics, control, instrumentation and automation, 1: 259-264.

[3] D. J. Oroya-Villalta, C. Esteve and U. Biccari. Multilevel Selective Harmonic Modulation via Optimal Control, preprint.

</description>
        <pubDate>Thu, 25 Mar 2021 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/DyCon-Blog/tutorial/wp99/P00014</link>
        <guid isPermaLink="true">http://localhost:4000/DyCon-Blog/tutorial/wp99/P00014</guid>
        
        
        <category>tutorial</category>
        
        <category>WP99</category>
        
      </item>
    
      <item>
        <title>Opening the black box of Deep Learning</title>
        <description>Opening the black box of Deep Learning

Deep Learning is one of the three main paradigms of Machine Learning, and roughly consists on extracting patterns from data using neural networks.

Its impact in modern technologies is huge. However, there is not a clear high-level description of what these algorithms are actually doing.

In this sense, Deep Learning models are sometimes referred to as “black box” models”; that is, we can characterize the models by its inputs and outputs, but with any knowledge of its internal workings.

The aim is then to open the black box of Deep Learning, and try to gain an intuition of what are these models doing. 
One of the most succesful mathematical theories in recent years has been connecting Deep Learning with Dynamical Systems.

The aim of this post is to introduce to some of those exciting ideas.

Data and classification
Supervised Learning

Deep Learning essentially wants to solve the problem of Supervised Learning. That is, we want to learn a function that maps an input to an output, based on examples of input-output pairs.

A classical Deep Learning problem would be the following: we have some pictures of cats and dogs, and want to make an algorithm that learns when the input image shows a dog and when it shows a cat. Note that we could give the algorithm pictures that it has not previously seen, and the algorithm should label the pictures correctly.

A formal description of our problem is the following.

Consider a dataset $\mathcal{D}$ consisting on $S$ distinct points ${x_i}_{i=1}^{S}$, each of them with a corresponding value $y_i$ such that 


we expect that the labels $y_i$ are a function of $x$, such that


where $\epsilon _i$ is included so as to consider, in principle, noisy data.

The aim of supervised learning is to recover the function $\mathcal{F}(\cdot)$ such that not only fits well in the database $\mathcal{D}$ but also generalises to previously unseen data.

That is, it is inferred the existence of 
 
where 


being $\mathcal{F} (\cdot)$ the desired previous application, and $\mathcal{D} \subset \tilde{\mathcal{D}}$.


  


 Figure 1.  *The figure in the left represents a finite set of points in $(x,y)$, where $x$ represents the input and $y$ the output. The figure on the right is the expected distribution from which we are sampling the input-output examples. The whole distribution can be understood by $\tilde{\mathcal{D}}$.*




A well-known dataset in which to use Deep Learning algorithm is the CIFAR-10 dataset.


  


 Figure 2.  Scheme of the CIFAR-10 dataset.


In this case, the input images consist on a value in $[0,1]$ for each pixel and Red / Green / Blue color. Since the resolution is $32 \times 32$ pixels, the input space would be $[0,1]^d$ with $d=32 \times 32 \times 3 = 3072$. Each image has a correspondent label (airplane, automobile, …).

The dataset is then given by $ { x_j, y_j = f^{\star(x_j)} } $ where the inputs $ x_j \in [0,1]^{3072} $, and the labelling function acts as $f^* : [0,1]^{3072} \to  { \text{airplane, automobile, … } }$.

We want that $ f^*( \text{each image} ) = \text{correct label of that image} $.

Note that the function $ f^* $ we are talking about is not the same as the function $\mathcal{F}$ that we used when defining the notion of dataset. This is because the function $ f^* $ is the application that connects the images with their given labels, whereas the function $\mathcal{F}$ is the function that connects any feasible input image with its true label. We have access to $f^*$, but not $\mathcal{F}$.



Noisy vs. noisless data

What are the true labels of a dataset? We have no a priori idea. What is the mathematical definition of “pictures of airplanes”? Our best guess is to learn the definition by seing examples.

However, those examples may be mistakenly labelled, since the datasets are in general noisy. What does a noisy dataset mean?


  
    We refer to noisy data when the samples in our dataset $\tilde{\mathcal{D}}$ actually comes from a random vector $(\mathbf{x}, \mathbf{y})$ with a non-deterministic joint distribution, such that it is not possible to find a function $\mathcal{F}$ such that $\mathcal{F}(\mathbf{x}) = \mathbf{y}$.

    The aim of supervised learning, in this case, would be to find a function such that
  $\mathcal{F}(\mathbf{x}) \approx \mathbf{y}$. For example, $\mathcal{F}(\tilde{x}_i) = \tilde{y}_i$ for almost every sample $i$.
  
  
    In the case of noisless data (which does not happen in most practical situations, but it makes things far simpler) then it is possible to find a function $\mathcal{F}$ such that $\mathcal{F}(\tilde{x}_i) = \tilde{y}_i \; \forall i$, or equivalently $\mathcal{F}(\mathbf{x}) = \mathbf{y}$.
  


Note that, given that the probability of a collision (of having two different data-points such that $x_i = x_j$ for $i \neq j$) is practically $0$, it is in principle possible to find a function $\mathcal{F}$ such that $\mathcal{F}(x_i)=y_i$ for all of our samples i, even in the noisy case (for example, we may use interpolation). 
However, the aim of supervised learning is to be able to generalise well to previously unseen data, $\tilde{\mathcal{D}}$, and so interpolation is not effective, and may not be well-defined, since the same input $x_i$ can be labelled differently, due to noise.


  

Representation of the optimal function $\mathcal{F}$ as the number of samples increases, in the case of noisy data. Note that the function “stabilizes” at a certain point, and thus it is expected to behave properly in the limit of infinitely many samples.


  


 Representation of the optimal function $\mathcal{F}$ as the number of samples increases, in the case of noisless data. In this case the function could have been obtained by interpolation, and would stabilize fast. 




Binary classification
In most cases, the data points $x_i$ are in $\mathbb{R}^{n}$. In general, we may restrict to a subspace of feasible images $X \in \mathbb{R}^{n}$.
So as to ease the training of the models, the inputs are usually normalized, and it is considered $X = [-1,+1]^{n}$, with $n$ the input dimension.

There are different supervised learning problems depending on the space $Y$ of the labels.
If $Y$ is discrete, such that $Y = { 0, … , L-1}$, then we are dealing with the problem of classification. In the case of CIFAR-10 we have $10$ classes, and so $L=10$.
 If $L=2$, then it is a binary classification problem, such as in the case of classifying images of cats and dogs (in this case, dog may be represented by “0” and cat by “1”).

In the particular setting of binary classification, the aim is to find a function $\mathcal{F} (x)$ such that it returns $0$ whenever the corresponding data point $x$ is of class $0$, and $1$ conversely.

Data, subspaces and manifolds

Consider $M^0$ the subset of $X$ such that $F( \tilde{x} _i) =0$  for all the points $\tilde{x}_i$ in $M^0$. And consider $M^1$ such that  $\, F(\tilde{x}_i)=1$ for all the points in $M^1$.

It is useful to consider $d(M^0 , M^1) &amp;gt; \delta$. That is, we want the subspace of images of cats and the subspace of images of dogs to have a nonzero distance in the input space. This makes sense physically, since we do not expect the pictures of dogs and cats to overlap.

An interesting hypothesis is to consider that $M^0$ and $M^1$ are indeed manifolds in a low-dimensional space. This is called the manifold hypothesis.

That is, out of the whole input space of all possible images, only a low-dimensional subspace of those would correspond to the images of cats, and one could go from one picture of a cat to any other picture of a cat by a path in which all the point are also pictures of cats. This has been widely hypothesized in many datasets, and there are some experimental results in that direction [9].


  


 Example of the input space in a binary classification setting. The axis $\mathbf{x}^1$ and $\mathbf{x}^2$ represent the components of the vectors $x$ the input space (we are considering $[0,1]^2$). The points in the subspace $M_0$ are labelled as $0$, and the points in $M_1$ as $1$. The points that are not in $M_0$ or $M_1$ do not have a well-defined label


Deep Learning

Now that we have some inuition about the data, it’s time to focus on how to approximate the functions $f$ that would fit that data.

When doing supervised learning, we need three basic ingredients:


  An hypothesis space, $\mathcal{H}$, consisting on a set of trial functions. All the functions in the hypothesis space can be determined by some parameters $\theta$. That is, all the functions $f \in \mathcal{H}$ are given by $f ( \cdot ; \theta )$. We therefore want to find the best parameters $\theta ^*$.
  A loss function, that tells us how close we are from our desired function.
  An optimization algorithm, that allows us to update our parameters at each step.


Our algorithm consists on initializing the process by choosing a function $f( \cdot , \theta) \in \mathcal{H}$. In Deep Learning, this is done by randomly choosing some parameters $\theta _0$, and initializing the function as $f( \cdot ; \theta_0)$.

In  Deep Learning, the loss function is given by the empirical risk. That is, we choose a subset of the dataset, 
 and compute the loss as 
, where 
 could be simply given by 
.

The optimization algorithms used in Deep Learning are mainly based in Gradient Descent, such as


Where the subindex denotes the parameters at each iteration.

We expect our algorithm to converge; that is, we want that, as $k \to \infty$, we would have $\theta_{k}$ close to $\theta^*$, where the notion of “closeness” is given by the chosen loss function.

Paradigms of Deep Learning

There are many questions about Deep Learning that are not yet solved. Three of that paradigms, that woud be convenient to treat mathematically, are:


  Optimization: How to go from the initial function, $F_0$, to a better function $\tilde{F}$? This has to do with our optimization algorithm, but also depends on how do we choose the initialization, $F_0$.
  Approximation: Is the ideal solution $ F^* $ in our hypothesis space $\mathcal{H}$? Is $\mathcal{H}$ at least dense in the space of feasible ideal solutions $ F^* $ ? This is done by characterizing the function spaces $\mathcal{H}$.
  Generalization: How does our function $\tilde{F}$ generalise to previously unseen data? This is done by studying the stochastic properties of the whole distribution, given by the generalised dataset.



  


 Diagram of the three main paradigms of Deep Learning. The function $F_0$ is the initial one, $\tilde{F}$ is the final solution as updated by the optimization algorithm, $\hat{F}$ is the closest function in $\mathcal{H}$ to $F^*$, and $F^*$ is the best ideal solution.


The problem of approximation is somehow resolved by the so-called Universal Approximation Theorems (look, for example, at the results by Cybenko [11]). This results state that the functions generated by Neural Networks are dense in the space of continuous functions. However, in principle it is not clear what do we gain by increasing the number of layers (using deeper Neural Networks), that seem to give better results in practuce.

Regarding optimization, altough algorithms based in Gradient Descent are widely used, it is not clear whether the solution gets stucked in local minima, if convergence is guaranteed, how to initialize the functions…

When dealing with generalization, everything gets more complicated, because we have to derive some properties of the generalised dataset, from which we do not have access. It is probably the less understood of those three paradigms.

Indeed, those three paradigms are very connected. For example, if the problem of optimization is completely solved, we would know that our final solution given by the algorithm, $\tilde{F}$, would be the best possible solution in the hypothesis space, $\hat{F}$; and if approximation is solved, we would also end up getting the ideal solution $F^*$.

One of the main problems regarding those questions is that the hypothesis space $\mathcal{H}$ is generated by a Neural Network, and so it is quite difficult to characterize mathematically. That is, we do not know much about the shape of $\mathcal{H}$ (although the Universal Approximation Theorems give valuable insight here), nor how to “move” in the space $\mathcal{H}$ by tuning $\theta$.

Let’s take a look at how the hypothesis spaces look like.

Hypothesis spaces

In classical settings, an hypothesis space is often proposed as 


where $\theta = { a_i }_{i=1}^{m}$ are the coefficients, ${ \psi _i}$ are the proposed functions (for example, they would be monomials if we are doing polynomial regression).

The function $\mathcal{F}$ correspondent to the dataset $\mathcal{D}$ is then defined as


where $d$ is a distance defined by a loss function, and $\phi$ is a function that transforms $f(x_i; \theta)$ such that we can compare it with $y_i$. For example, if we want to do binary classification, we would use $\phi$ as a function that divides the space into two hiperplanes, and assigns $0$ to one of the parts of the divided space, and $1$ to the other.

This means that  it is the function in the hypothesis space that minimizes the distance with the target function, which connects the datapoints $x_i$ with their correspondent labels $y_i$.

When using Deep Neural Networks, the hypothesis space $\mathcal{H}$ is generated by the composition of simple functions $f$ such that


where $\theta = { \theta_k }_{k=1}^{L}$ are the training weights of the network and $L$ the number of hidden layers.

Multilayer perceptrons

In the case of the so-called multilayer perceptrons, the functions $f^k_{\theta_k}$ are constructed as


being $A^k$ a matrix, $b^k$ a vector, $\theta _k = 
{A^k , b^k}$ and $\sigma \in C^{0,1}(\mathbb{R})$ a fixed nondecreasing function, called the activation function.

Since we restrict to the case in which the number of neurons is constant through the layers, we have that $A^k \in \mathbb{R}^{d \times d}$ and $b^k \in \mathbb{R}^d$.


  


Diagram of a multilayer perceptron, also called fully-connected neural network. However, this graph representation seems a bit difficult to treat, so we use an algebraic representation instead


That is, shematically, one has that $\mathcal{H} = { F : F(x) = F_T \circ F_{T-1} \circ … \circ F_0 (x) }$, the hypothesis space is generated by the discrete composition of functions.

It can be seen that the complexity of the hypothesis spaces in Deep Learning is generated by the composition of functions. Unfortunately, the tools regarding the understanding of the discrete composition of functions are quite limited.

Moreover, considering $b^k=0$, we have that $F(x) = \sigma \circ A_T \circ \sigma \circ A_{T-1}  \circ … \circ \sigma A_0 \circ x$. The gradient would then go as $\nabla_\theta F \sim A_T \cdot A_{T-1} \cdot … \cdot A_0 \sim k^L$. For $L$ large, the gradients would become very large or very small, and so training would not be feasible by gradient descent.

This is one of the reasons for introduction the Residual Neural Networks.

Residual Neural Networks
In the simple case of a Residual Neural Network (often referred to as ResNets) the functions $f_{\theta_k}^{k} (\cdot)$ are constructed as


where $\mathbb{I}$ is the identity function. The parameters $\sigma$, $A^k$ and $b^k$ are equivalent to those of the multilayer perceptrons.

We will see that it is sometimes convenient to numerically add a parameter $h$ to the residual block, such that


being $h \in \mathbb{R}$ a scalar.


  


 Diagram of a ResNet, in a graph manner. Again, an algebraic representation is preferred.




Visualizing Perceptrons

The Neural Networks then represent the compositions of functions $f$, which consist on an affine transformation plus a nonlinearity.

If we use the activation function $\sigma ( \cdot ) = tanh (\cdot)$, then each function $f$ (and so each layer of the Neural Network) is first rotating and translating the original space, and then “squeezing” it.


  


 Example of the transformation of a regular grid by a perceptron.


Binary classification with Neural Networks

When dealing with data classification, it is very useful to just assign a color / shape to every label, and so be able to visualize data in a lower-dimensional plot.

The aim of classification is to associate different classes to different regions of the initial space,
When using Neural Networks, the initial space $X$ is propagated by the Neural Network as $F = f ^L \circ  f^{L-1} \circ … \circ f^1 (X)$. 
Then the function $\phi$ would divide the space by an hyperplane, associate $0$ to one region and $1$ to the other.

So, while it may be difficult to classificate the datapoints in the initial space $X$, the Neural Networks should make things simpler and just propagate the data so as to make it linearly separable (and we would see this in real action!).

Our aim is therefore to transform the initial space $X$ in such a way that $M^0$ and $M^1$ are linearly separable. That is, 
 is linearly separable from 
.


  


 Example of the transformation of a regular grid by a perceptron.


Neural Networks as homeomorphisms

We know that the composite of continuous functions is continuous, and that the composite of bijections is a bijection. Then, a *Neural Network would represent a bijection if and only if all of its layers represent bijections. And, if the inverse is continuous, it would also be an homeomorphism.

In the case of multilayer perceptrons, since they are given by $f^L \circ f^{L-1} \circ … \circ f^1 (\cdot)$, they will represent an homeomorphism if each $f$ is an homeomorphism [5] [6].

$f = \sigma (A (\cdot) + b)$

In the case $\sigma = tanh$, restricting to our domain, we have a continous inverse.

Then it is enough that $A (\cdot) + b$ is homeomorphic. If $A$ is nonsingular, we know that there is an inverse function. Therefore, in the case of multilayer perceptrons, it is enough that all the matrices $A^k$ are nonsingular for the Neural Network to represent an homeomorphism.

Indeed, since the matrices $A^k$ are initialized at random, it is possible that some of them are singular, and so that the Neural Network does not represent an homeomorphism, but rather a function in which we “lose information”  in the way by setting some directions to $0$. This has to do with the fact that arbitrarily large multilayer perceptrons are difficult to use in practice, and this is another reason why ResNets were introduced.

In the case of ResNets, all the functions


should be homeomorphisms. But in this case is more tricky than in the multilayer perceptrons case.

Theorem 1 (Sufficient condition for invertible ResNets [1]):

Let 
$\hat{\mathcal{F}} \, : \, \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ with 
 denote a ResNet with blocks defined by 

Then, the ResNet $\hat{\mathcal{F}}_{\theta}$ is invertible if 


We expect the lipschitz constant of $g_\theta$ to be bounded, and so $h\to 0$ would make our ResNets invertible.

Neural Networks and Dynamical Systems

From what we have seen, now the relation between Neural Networks and Dynamical Systems is just a matter of notation!


  
    Consider the multilayer perceptron, given by 


    Simply renaming the index $l$ by the index $t-1$, and $L$ by $T-1$, we see that

    

    If we define $x_t = f^t \circ … \circ f^0 (x)$, we see that what the neural network is doing is defining a dynamical system in which our initial values, denoted by $x$, evolve in a time defined by the index of the layers.

    However, this general discrete maps are not easy to characterize. We can see that we are more lucky with ResNets.
  
  
    In the case of Residual Neural Networks, the residual blocks are given by
   
  where the supindex in $A$ and $b$ denote that $A$ and $b$, in general, change in each layer.
  


Defining the function $G(x, t) =  (A^t (x) + b^t)/h$, we can rewrite our equation as


with the initial condition given by our input data (the ones given by the dataset).

Remember that the index $t$ refers to the index of the layer, but can be now easily interpreted as a time.

In the limit $h\to 0$ it can be recasted as


and, since the discrete index $t$ becomes continuous in this limit, it is useful to express it as


This dynamic should be such that $x(0)$ is the initial data, and $x(T)$ is a distribution such that the points that were in $M^0$ and $M^1$ are now linearly separable.

Therefore the ResNet architecture can be seen as the discretization of an ODE, and so the input points $x$ are expected to follow a smooth dynamics layer-to-layer, if the residual part is sufficiently small.

The question is: is the residual part sufficiently small? Is $h$ indeed small? There are empirical evidence showing that, as the number of layers ($T$) incresases, the value $h$ gets smaller and smaller.

But we do not need to worry too much, since we can easily define a general kind of ResNets in which the residual blocks are given by


in which $h$ could be absorbed easily by $A$ and $b$.

Deep Learning and Control

Recovering the fact that $f^t$ are indeed $f^t( \cdot ;\theta ^t)$, where $\theta^t$ are the parameters $A^t$ and $b^t$, we can express our dynamical equations, in the case of resnets as



where $x(0)$ is our initial data point, and we want to control our entire distribution of initial datapoints into a final suitable distribution, given by $x(T)$. In the case of binary classification, that “suitable distribution” would for example consist in one in which the datapoints labelled as $0$ move further away from the datapoints labelled as $1$, yielding a final space that could be separated by an hyperplane.

Exploiting this idea, we have the Neural Ordinary Differential Equations [10].

Visualizing Deep Learning

In a binary classification problem using Deep Learning, we essentially apply the functions $f\in \mathcal{H}$, where $\mathcal{H}$ has been previously defined, to our input points $x$, and then the space $f(x)$ is divided by two by an hyperplane: one of the resulting subspaces would be classified as $0$, and the other as $1$.

Therefore, our problem consists on making an initial dataset linearly separable.

Consider a binary classification problem with noisless data, such that




We consider two distributions, the ring distribution and the spiral distribution.


  



  


 Plot of the ring and spiral distribution.


And let’s put into practice what we have learned! For that, we use the library TensorFlow [14], in which we can easy implement the Multilayer Perceptrons and Residual Neural Networks that we have defined.

We are going to look at the evolution of $x(t)$, with $x(0)$ the initial datapoint of the dataset. Remember that we have two cases

  In a multilayer perceptron, $x(t) = x_t = f^t \circ f^{t-1} \circ … \circ f^1(x)$.
  In a ResNet, $x(t)$ would be given by $x(t+1) = x(t) + f(x(t))$  that is, approximately, $\dot{x}(t) = G(x(t), t)$.


Multilayer Perceptrons

In the case of multilayer perceptrons, we choose $3$ neurons per layer. This means that we are working with points $x(t)$ in $\mathbb{R}^3$.

We train the networks, and after lots of iterations of the gradient descent method, this is what we obtain.


  



  


We see that in the final space (hidden layer 5) the blue and yellow points can be linearly separable.

However, the dynamics is far from smooth, and we see that in the final layers we seem to occupy a $2-$dimensional space rather than a $3-$dimensional one. This is because multilayer perceptrons tend to “lose information”. This type of behaviour causes that if we had increased the number of layers this architectures would be super difficult to train (we would have to try lots of times, since each time we initialize the parameters at random).

Visualizing ResNets

With ResNets we are far luckier!

Consider two neurons per layer. That is, we would be working with $x(t)$ in $\mathbb{R}^2$.


  





  


We observe that altough the spiral distribution can be classified by this neurons (the transformed space, in hidden layer $49$, makes the distribution linearly separable) this does not happen with the ring distribution!

We should be able to respond why this happens with what we have learned: since the ResNets are homeomorphisms, we would not be able to untie the tie in the ring distribution!

However, we should be able to untie it if going to a third dimension. Let’s try that.

Now we consider $3$ neurons per layer, i.e. $x(t) \in \mathbb{R}^3$.




  





  




Now both distributions can be classified. Indeed, every distribution in a plane could be in principle classified by a ResNet, since at most we would have ties of order $2$, that can be untied in a dimension $3$.

But could indeed all distributions be classified? Are the ResNets indeed such powerful approximators that we could recreate almost any dynamics out of them? Are we able to find the parameters $\theta$ (that can be seen as controls) such that all distributions can be classified? What do we gain by increasing $T$ (having more and more layers)?

Those and far more other questions arise out of this approach. And those cannot be answered easily without further research.

Results, open questions and perspectives

Noisy vs noisless data

  


 Representation of a more natural distribution at the left, and examples of realisations of that distribution in the right.


It seems like we almost forgot that the data is (always) noisy! What happens in that case?

Considering the distribution in the figure (that can be seen as a more “natural” ring distribution), if we only cared about moving the blue points to the top, and the yellow points to the bottom, we would have an inecessary complex dynamics, since we expect some points to be mislabelled.

We should stick to “simple dynamics”, that are the ones we expect would generalise well. But how to define “simple dynamics”?
 In an optimal control approach, this has to do with adding regularization terms.

Moreover, Weinan E. et al [2] propose the approach of considering, instead of our points $x(t)$ as objects of study for the dynamics, the probabilty distribution, and so recasting the control as a mean field optimal control. With this approach, some we gain some insight into the problem of generalization.

This has to do with the idea that indeed what the Neural Networks take as inputs are probabilty distributions, and the dynamics should be focused on those. In here theories such as transportation theory try to give some insight into happens when consider the more general problem of Deep Learning: one in which we have noise.

Complexity and control of Neural Networks

We also do not know what happens when increasing the number of layers. Why increasing the number of layers is better?

In practice, having more layers (working in the so called overparametrization regime) seems to perform better in training. Our group indeed gave some insight in that direction [15] by connecting this problems with the turnpike phenomenon [16].

Regarding optimality, it is not easy to define, since we have multiple possible solutions to the same problem. That is, there are multiple dynamics that can make an initial distribution linearly separable.


  



  


 Dynamics given by a ResNet with 3 neurons per layer, with two different initializations.


The question is again: if there is a family of dynamics that solve the classification problem, which ones out of that family are optimal?

It also seems that one should take into account some problem of stochastic control, since the parameters $\theta$ are initialized at random, and different initializations give different results in practice. Moreover, the stochastic gradient descent, which is widely used, introduces some randomness to our problem. This indicates that Deep Learning is by no means a classical control problem.

Conclusions

Altough the problem of Deep Learning is not yet resolved, we have made a some very interesting equivalences:

  The problem of approximation in Deep Learning is now the problem of how is the space of possible dynamics? In the case of multilayer perceptrons, we can make use of discrete dynamical systems to solve that, or characterize their complexity; and in the case of ResNets, papers such at the one by Q. Li et. deals with that question [7].
  The problem of optimization is the problem of finding the best weights $\theta$, that can be recasted as controls. That is, is a problem of optimal control, and we should be able to use our knowledge of optimal control (for example, the method of succesive approximations) in this direction.
  The generalization problem can be understood by consdering the control of the whole probability distribution, instead of the risk minimization problem [2].


All in all, there is still lot of work to do to understand Deep Learning.

However, the presented approaches have been giving very interesting results in the recent years, and everything indicates that they will continue to do so.

References

[1] J. Behrmann, W. Grathwohl, R. T. Q. Chen, D. Duvenaud, J. Jacobsen (2018).  Invertible Residual Networks arxiv

[2] Weinan E, Jiequn Han, Qianxiao Li, (2018).  A Mean-Field Optimal Control Formulation of Deep Learning. volume 6. Research in the Mathematical Sciences. paper

[4] B. Hanin, (2018).  Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients?   arxiv

[5] C. Olah,  Neural Networks, Manifolds and Topology . blog post

[6] G. Naitzat, A. Zhitnikov, L-H. Lim, (2020).  Topology of Deep Neural Networks  arxiv

[7] Q. Li, T. Lin, Z. Shen, (2019).  Deep Learning via Dynamical Systems: An Approximation Perspective arxiv

[8] B. Geshkovski,  The interplay of control and deep learning . blog post

[9] C. Fefferman, S. Mitter, H. Narayanan,  Testing the manifold hypothesis . pdf

[10] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, D. Duvenaud, (2018).  Neural Ordinary Differential Equations  arxiv

[11] G. Cybenko.  Approximation by Superpositions of a Sigmoidal Function  pdf

[12] Weinan E, Chao Ma, S. Wojtowytsch, L. Wu.  Towards a Mathematical Understanding of Neural Network-Based Machine Learning_ what we know and what we don’t.  arxiv

[14]  Introduction to TensorFlow . tutorial

[15] C. Esteve, B. Geshkovski, D. Pighin, E. Zuazua  Large-time asymptotics in deep learning . arxiv

[16]  Turnpike in a Nutshell . Collection of talks

</description>
        <pubDate>Mon, 22 Mar 2021 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/DyCon-Blog/tutorial/wp99/P00013</link>
        <guid isPermaLink="true">http://localhost:4000/DyCon-Blog/tutorial/wp99/P00013</guid>
        
        
        <category>tutorial</category>
        
        <category>WP99</category>
        
      </item>
    
      <item>
        <title>Points of Interest Extraction and Prediction of Next Locations</title>
        <description>Introduction
GPS enabled devices, such as smart phones, have gained large popularity in the last few years. It allows people to record their outdoor activities through GPS trajectories, which enables researchers to infer further knowledge about the moving behavior of mobile users [8]. An important topic in analyzing GPS data is the identification of significant or interesting places visited by users [1] because if we extract them, we can know each user well and this will allow us to have a better prediction on the user’s next location.
Generally, a prediction problem involves using past observations to predict or forecast one or more possible future observations. The goal is to guess about what might happen in the future. Knowing the future can impact our decisions today so we have a great interest in predicting it.
In this project we tried first working on finding an algorithm that can help us define the points of interest of the users we have in the dataset. For that we implement an algorithm that can identify the users stop points. 
Next step was predicting the next location of the users to do that we used different approaches (Markov chains, Random forest and Recurrent neural network).

KEYWORDS: GPS Trajectory; Clustering algorithm; Points of interests (POI); Prediction; Markov chain; Random Forest; DBSCAN; HDBSCAN.

Motivation
With the popularity of smartphones and other equipment that can be used to record locations via the Global Navigation Satellite System (GNSS) [8], capturing and recording trajectory data based on such equipment becomes more convenient. The smartphone for exemple combines different sophisticated features. It allows users to keep pictures, memories, personal info, health [9,10] etc. And with each use to some application we can get the user GPS coordinates at a period of time [11].
So why don’t we use this data to better understand the users? If we get to know the next step of each individual we can for example suggest some near restaurants, shops and places they used to go to, or notify them if someone they know is close to where they are.
It could be also beneficial for the police department [12], it can help them identify criminals’ places or finding missing people and more likely it can also help people with Alzheimer’s, by notifying their relatives about their places.

Problem statement
Mobility is part of human’s daily life. We move from one place to another for different reasons. For example we go to a certain place to work and to another place to meet people.
Mobility patterns of users have been studied intensively. The ubiquity of mobile phones with their embedded sensors and available applications opened new possibilities both for academia and industry to study human mobility patterns [2,3].

In this project the main objective is to extract the useful data from different users trajectories in order to predict their next locations afterwards.

Presentation of our data
In this project we are working on a time series dataset (see Fig. 1) that contains 288 user GPS coordinates of three month (March, April, May). These users lives in Spain.
The data was collected whenever the users are connected to their accounts.


  


 Figure 1.  Time series database of GPS coordinates.


We can see in Figure 2 the representation of 10\% of the users on the map.  As we can notice the users are so active inside and outside of Spain. In Figure 3 we see the GPS representation of a user on the map.


  


 Figure 2.  Represantation of 10\% of the users population we have on the map.



  


 Figure 3.  Representation of one user GPS coordinates on the map.


Methodology
The first objective is to extract the points of interest (POIs) which represent the locations that the individual visit frequently (Home, Place of work, gym \dots, etc), of the different users we have in the dataset after grouping  it by users so that it would be easy to analyse them separately.

Working on a real world data means the existence of noise the first two clustering algorithms (K-means and GMM) as we can see in Figures 4 y 5 they weren’t capable of eliminating the noise in our data, because in these algorithms each point must belong to one cluster which is not convenient for our data, not all the points are part of a location some points shows us only the trajectory that the user did to get to the desired place so we can say that these two approaches are not able to differentiate between a stop point and a trajectory.


  


 Figure 4.  Graphic representation of a user GPS coordinations after k-means clustering.



  


 Figure 5.  Graphic representation of a user GPS coordinations after GMM clustering.


So we had to look for other approaches that can eliminate the noise. The first algorithm we tried was DBSCAN [6], but it wasn’t easy for us to estimate the best parameters, especially that each user has it’s proper trajectory and also the density of the points wasn’t unified for the same user, some group of points are so close to each other then others.

For that the next algorithm we used was HDBSCAN [5] because in this algorithm the threshold is not fixed so it’s capable of identifying clusters of different densities. This algorithm gives us some good results but after implementing the clusters on the map we can see that for some users the clusters didn’t include just the meaningful points but also some of the noise (the trajectory of the user) as we can see in Figure 4.


  



  


 Figure 6.  Implementation of a user clusters on the map after removing the noise with HDBSCAN


For that we needed to implement an algorithm that can identify the users stop points, which tells us where and when a user has stopped (stayed). It consists of a timestamp, a user identifier as well as longitude and latitude coordinates. It also has two additional variables: t_start and t_end indicating the start-time and end-time of the stop (see Figure 5). 
The stop location extraction algorithm [7] has two parameters which are dependent on the application:

$\bullet \quad $  Roaming distance:  defines how far a user is allowed to roam to still be considered part of the stop location (dashed orange line

$\bullet \quad $  Stay duration:  specifies the minimum amount of time a user has to stay within the roaming distance for his position to be considered as being part of a particular stop location.


  


 Figure 7.  Output of the stop algorithm.



  


 Figure 8.  Representation of the stop points of the users on the map.


The next step is to aggregates stop locations in close proximity of each other to know where and how often a user has stopped at a destination. The destination consists of a timestamp, a user identifier as well as latitude and longitude coordinates. In addition, the visitation count denotes the number of stops at a particular destination and cluster_assignment identifies the destination itself.

What we are actually interested in is the destination where a user has stopped and not necessarily the exact GPS position recorded. For instance, the group of stop locations surrounding a building should be regarded as one destination. In other words, we need to cluster or aggregate the stop locations into destinations (see Figure 7).

To aggregate the stop locations into destinations we use Scipy’s hierarchical clustering functions (see Figure 8). For our application there are two important parameters: \emph{The linkage parameter} defines how the clusters are formed. \emph{The distance parameter} specifies within what spatial distance stop locations are to be considered of the same cluster.


  



 Figure 9.  Output of destination algorithm


To get medoid for each destination, now that we have assigned each stop location to a destination, we want to have only one point representing each destination.

To have one representative point per cluster, we calculate the medoid of the stop locations within each destination. We also calculate the number of stop locations at each destination, which is useful for ranking destinations based on visitation counts.


  



  


 Figure 10.  Representation of the users destinations on the map.


Next step is to predict n-next steps to do that we use two different approaches the markov chains and random forest. But before that we need to split our data into two parts train which we give 90\% of our data and the rest(10\%) to the test.

After running the hidden markov chains model on our data we get to predict the next clusters of each user. We are predicting the clusters because each cluster represent a POI.

After training our model we calculate the accuracy of it for each user and to get the general accuracy score we get the mean score and it was 49\%. that means that our model is not that good at predicting the next location.

So the next approach we used is random forest which is a supervised learning algorithm that can be used for class classification or regression (prediction).

The accuracy of Random forest model is 94\% which shows that the model is doing well at predicting the users next locations.

References
[1] S. Gambs and M.O. Killijian, Next Place Prediction using Mobility Markov Chains, MPM’12, April 10, 2012, Bern, Switzerland.

[2] Z. Fu, Z. Tian, Y. Xu and C. Qiao, A Two-Step Clustering Approach to Extract Locations from Indi- vidual GPS Trajectory Data, ISPRS Int. J. Geo-Inf. Vol. 5, 166, 1–17, 2016. doi:10.3390/ijgi5100166

[3] N. Palmius and A. Tsanas and K. E. A. Saunders and A. C. Bilderbeck and J. R. Geddes and G. M. Goodwin and M. De Vos, Detecting Bipolar Depression From Geographic Location Data, IEEE Trans Biomed Eng., Vol. 64, No. 8, 1761–1771, 2017. doi: 10.1109/TBME.2016 2611862

[4] J. Busk and M. Faurholt-Jepsen and M. Frost and J. E. Bardram and L. Vedel Kessing and O. Winther, Forecasting Mood in Bipolar Disorder From Smartphone Self-assessments: Hierarchical Bayesian Approach, JMIR Mhealth Uhealth, Vol. 8, No. 4, 1–14, 2020

[5] P. Widhalm and P. Nitsche and N. Brandie, Transport mode detection with realistic Smartphone sensor data. In Proceedings of the International Conference on Pattern Recognition, Tsukuba, Japan, 11–15 November 2015; IEEE: Piscataway, NJ, USA, 2012; 573–576.

[6] A. Bogomolov and B. Lepri and J. Staiano and N. Oliver and F. Pianesi and A. Pentland, Once Upon a Crime: Towards Crime Prediction from Demographics and Mobile Data. In Proceedings of the 16th International Conference on Multimodal Interaction, Istanbul, Turkey, 12–16 November 2014; ACM: New York, NY, USA, 2014; 427–434.

[7] V. Bogorny, C. Renso, A.R. de Aquino, F. de Lucca Siqueira and L.O. Alvares, Constant–A Con- ceptual Data Model for Semantic Trajectories of Moving Objects, Trans. GIS Vol. 18, 66–88, 2014.

[8] L. Xiang, M. Gao and T. Wu, Extracting Stops from Noisy Trajectories: A Sequence Oriented Clustering Approach, ISPRS Int. J. Geo-Inf. Vol. 5, 29, 1–18, 2016. doi:10.3390/ijgi5030029

[9] Ester M., Kriegel H.P., Sander J. and Xu X. A density-based algorithm for discovering clusters in large spatial databases with noise. KDD-96 Proceedings, 226–231, 1996

[10] Campello R.J.G.B., Moulavi D., Sander J. Density-Based Clustering Based on Hierarchical Density Estimates. In: Pei J., Tseng V.S., Cao L., Motoda H., Xu G. (eds) Advances in Knowledge Discovery and Data Mining. PAKDD 2013. Lecture Notes in Computer Science, Vol. 7819. Springer, Berlin, Heidelberg, 2013.

[11] R. Hariharan and K. Toyama. Project Lachesis: Parsing and Modeling Location Histories. KDD- 96 Proceedings, In: Egenhofer M.J., Freksa C., Miller H.J. (eds) Geographic Information Science. GIScience 2004. Lecture Notes in Computer Science, Springer, Berlin, Heidelberg, Vol. 3234, 106– 124, 2004

[12] Camacho-Lara, S. Current and Future GNSS and Their Augmentation Systems In: Handbook of Satellite Applications; Springer Science and Business Media LLC: Berlin, Germany, 2013, pp. 617?654.

</description>
        <pubDate>Wed, 10 Mar 2021 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/DyCon-Blog/tutorial/wp99/P00012</link>
        <guid isPermaLink="true">http://localhost:4000/DyCon-Blog/tutorial/wp99/P00012</guid>
        
        
        <category>tutorial</category>
        
        <category>WP99</category>
        
      </item>
    
      <item>
        <title>Averaged dynamics and control for heat equations with random diffusion</title>
        <description>Background and motivation

Let us consider the random heat equation described 
by the following system:



for $G$  a domain, $G_0\subset G$ a subdomain, 
$f$ a control,  $y^0$  the initial configuration 
and $\alpha$ the diffusivity coefficient, which is a positive 
random variable with density function $\rho$.
We have that the averaged solution of (1)
is given by:



We want to determine if, given a positive random variable 
$\alpha$ and an initial configuration $y^0\in L^2(G)$, there 
is some $f\in L^2((0,T)\times G_0)$ such that $\tilde y(T,\cdot)=0$.

In order to illustrate the effect of averaging in
 the dynamics, let us study the dynamics of
(1) when $G=\mathbb R^d$ and $f=0$.
As averaging and the Fourier transform commute, we work on the
Fourier transform of the fundamental solution of the
heat equation, which is given by:



Consequently, the Fourier transform of the average
of the fundamental solutions is given by:



i.e. the Laplace transform of $\rho$ evaluated in $|\xi|^2t$. 
In particular, for $r\in(0,1)$ if $\rho(\alpha)\sim_{0^+} e^{-C\alpha^{-\frac{r}{1-r}}}$
we have that:



when $|\xi|^2t\to+\infty$, which can be proved by the Laplace method.
Thus, for those density functions 
the averaged dynamics in $\mathbb R^d$ has a fractional nature. As it is proved in [1],
for $G$ bounded this is also true and we have the usual controllability 
and observability results of fractional dynamics;
that is, (2) implies that
the averaged unique continuation is preserved,
but (2) preserves the null averaged observability
if and only if $r&amp;gt;1/2$, being the threshold density functions  those
which satisfy:



Some numerical simulations

Let us now illustrate the difference between several probability distributions for the diffusion through numerical simulations. For that, we recall that 
the optimal control is given by $\varphi(t,x;\phi)1_{G_0}$, for $\varphi$ the 
averaged solution of



and $\phi$ the state which minimizes the functional:



Due to the hardness of the numerical computations in higher dimensions and to get
better illustrations we work in $d=1$, and in particular in $G=(0,\pi)$.
We also consider $G_0=(1,2)$, $T=1$ and $y^0=\frac{1}{2}$. 
Moreover, to illustrate the difference between diffusivities inside and 
outside the null controllability regime, we consider $\rho=1_{(1,2)}$, which is inside,
and $\rho=1_{(0,1)}$, which is outside.

In order to numerically implement this problem, we approximate it by
minimizing $J$ in $V_M:=\langle e_i \rangle_{i=1}^M$ 
for ${e_i}$ the eigenfunctions of the Dirichlet Laplacian
 and for $M\in{40,50,60}$. 
Since $V_M$ is a finite dimensional space, computing the 
minimum of $J$ is equivalent to solving numerically a linear system,
which can be easily done by using any numerical computing environment
(in our case MATLAB). We have the following illustrations:

First, we illustrate in Figure 1 
(resp. in Figure 2)
the controls induced by the minimum of $J$ for $\rho=1_{(1,2)}$
(resp. for $\rho=1_{(0,1)}$).
For  $\rho=1_{(1,2)}$ the 
sequence of controls converges, which is something that can be seen in an even 
more clear way when $t\in[0,1/2]$. Of course, the closer the time is to $1$,
the more slowly the punctual values of the control converges pointwise with $M$ 
(and in $t=1$ it diverges),
which is a well-known behaviour when
controlling a parabolic dynamics (see, for instance,
[2], [3] and [4]).
 However, for $\rho=1_{(0,1)}$ the sequence of  controls 
diverges, which is something that we can appreciate in a more 
detailed way when $t\in[0,1/2]$.




 
 


 
 


 
 




 Figure 1: 
 The optimal control for  $\rho=1_{(1,2)}$ and $y^0=\frac{1}{2}$ 
induced by the minimum of the functional $J$ in $V_{40}$, $V_{50}$
and $V_{60}$.
In the left column we illustrate the whole controls, whereas in the right 
column we illustrate the controls with the time variable zoomed in $[0,1/2]$.





 
 


 
 


 
 




 Figure 2: 
 The optimal control for  $\rho=1_{(0,1)}$ and $y^0=\frac{1}{2}$ 
induced by the minimum of the functional $J$ in $V_{40}$, $V_{50}$
and $V_{60}$.
In the left column we illustrate the whole controls, whereas in the right 
column we illustrate the controls with the time variable zoomed in $[0,1/2]$.


Next, we show in Figure 3
the canonical prolongation of the previously obtained controls to $t=0$.
 Again, for $\rho=1_{(1,2)}$  we have a clear convergence,
whereas for $\rho=1_{(0,1)}$  it diverges.




 
 



 Figure 3: 
The natural extensions to $t=0$ of the controls 
induced by the minimum of the functional $J$ in $V_{40}$, $V_{50}$ 
and $V_{60}$ with $y^0=\frac{1}{2}$. In the left figure
we have considered $\rho =1_{(1,2)}$ and in the right one $\rho=1_{(0,1)}$.


Finally, we illustrate in Figure 4 the state at $t=1$
of the respective solutions of the averaged heat equation
 with the previously obtained controls.
For $\rho=1_{(1,2)}$ the solution converges smoothly to $0$,
whereas for $\rho=1_{(0,1)}$ the solution diverges.




 
 



 Figure 4: 
The state in time $t=1$ of the averaged solutions of the heat equation after
applying the control induced by the minimum of $J$ in $V_{40}$, $V_{50}$
and $V_{60}$ with $y^0=\frac{1}{2}$. In the left figure
we have considered $\rho =1_{(1,2)}$ and in the right one $\rho=1_{(0,1)}$.


Bibliography
[1] J. A. Bárcena-Petisco, E. Zuazua,  Averaged dynamics and control for heat equations with random diffusion . Preprint https://hal.archives-ouvertes.fr/hal-02958671/

[2] E. Fernández-Cara and A. Münch. Strong convergent approximations of null controls for the 1D
heat equation. SeMA journal, 61(1):49-78, 2013.

[3] R. Glowinski and J. L. Lions. Exact and approximate controllability for distributed parameter
systems.  Acta Numer., 1:269-378, 1994.

[4] A. Münch and E. Zuazua. Numerical approximation of null controls for the heat equation: illposedness
and remedies. Inverse Probl., 26(8):085018, 2010.
</description>
        <pubDate>Mon, 23 Nov 2020 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/DyCon-Blog/tutorial/wp01/P0013</link>
        <guid isPermaLink="true">http://localhost:4000/DyCon-Blog/tutorial/wp01/P0013</guid>
        
        
        <category>tutorial</category>
        
        <category>WP01</category>
        
      </item>
    
      <item>
        <title>Understanding Deep Learning: Neural Networks as bending manifolds</title>
        <description>Data and classification
Supervised Learning

Consider a dataset $\mathcal{D}$ consisting on $S$ distinct points ${x_i}_{i=1}^{S}$, each of them with a corresponding value $y_i$ such that





where $\epsilon _i$ is included so as to consider, in principle, noisy data.

The aim of supervised learning is to recover the function $\mathcal{F}(\cdot)$ such that not only fits well in the database $\mathcal{D}$ but also generalises to previously unseen data.

That is, it is inferred the existence of



where



being $\mathcal{F} (\cdot)$ the desired previous application, and $\mathcal{D} \subset \tilde{\mathcal{D}}$.





Noisy vs. noisless data

The generalised dataset $\tilde{\mathcal{D}}$ can be seen as the realisation of a random vector $(\mathbf{x}, \mathbf{y})$ generated by an unknown multivariate joint distribution between $\mathcal{X}$ and  $\mathcal{Y}$.

Then, the datasets used in Supervised Learning can be seen as a finite sample of the random vector $(\mathbf{x}, \mathbf{y})$, and so the underlying joint distribution between $\mathcal{X}$ and $\mathcal{Y}$ could be in principle recovered in the limit of infinitely many samples, $\tilde{\mathcal{D}}$ .

We refer to noisy data when the samples in our dataset $\mathcal{D}$ actually comes from a random vector $(\mathbf{x}, \mathbf{y})$ with a non-deterministic joint distribution, such that it is not possible to find a function $\mathcal{F}$ such that $\mathcal{F}(\mathbf{x}) = \mathbf{y}$.

The aim of supervised learning, in this case, would be to find a function such that $\mathcal{F}(\tilde{x}_i) \approx \tilde{y}_i$ for almost every sample, as with respect to a given loss function.

In case the data is considered to be noisless (which does not happen in most practical situations, but it makes things far simpler) then the joint distribution between $\mathcal{X}$ and $\mathcal{Y}$ of the random vector $(\mathbf{x}, \mathbf{y})$ is rather deterministic. In this case, it is possible to find a function $F$ such that $\mathcal{F}(\tilde{x}_i) = \tilde{y}_i \; \forall i$, or equivalently $\mathcal{F}(\mathbf{x}) = \mathbf{y}$.

Note that, given that the probability of a collision (of having two different data-points such that $x_i = x_j$ for $i \neq j$) is practically $0$, it is in principle possible to find a function $\mathcal{F}$ such that $\mathcal{F}(x_i)=y_i \; \forall i = { 1, …  ,S }$, even in the noisy case (i.e. we may use interpolation). 
However, the aim of Supervised Learning is to be able to generalise well to previously unseen data, $\tilde{\mathcal{D}}$, and so interpolation is not effective.

Some systems are often approximated as being noisless. Since the aim of supervised learning is finding functions $\mathcal{F}$, there is always the implicit assumption that the generalised dataset $\tilde{\mathcal{D}}$ can be approximately represented by a function $\mathcal{F}$, and so the noise is expected to be small, compared to the “real data”. Distinguishing data from noise is often far a from trivial issue.

The idea of working with noisy data can be recasted as a Mean-Field problem, as proposed by Weinan E. et al [2]. The idea is to control a distribution, instead of a set of points in an euclidean space. Although understanding Supervised Learning with noisless data from a Dynamical Control perspective is often enough ill-posed, considering noisy data unlocks the full theoretical experience.





Binary classification
In most cases, the data points $x_i$ are in $\mathbb{R}^{n}$. In general, $x_i \in X \; \forall i$, with $X \subset \mathbb{R}^{n}$. So as to ease the training of the models, it is usually considered $X = [-1,+1]^{n}$, with $n$ an arbitrary dimension. This is called data normalization in the Machine Learning literature.

There are different supervised learning problems depending on the space $Y$ of the corresponding labels, such that $y_i \in Y$.
If $Y$ is discrete, such that $Y = { 0, … , L-1}$, then we are dealing with the problem of classification. If $L=2$, then it is a binary classification problem.

In this particular setting, the aim is to find a function $F (\cdot)$ such that it returns $0$ whenever the corresponding data point is of class $0$, and $1$ conversely. 
Note that such a function cannot be continuous except for trivial cases, given that  the data points in $X$ cover all the space $[-1, +1]$, due to the existence of a boundary between points of class $0$ and $1$.

Data, subspaces and manifolds

Considering $X^0$ the subset of $X$ such that $F(\tilde{x}_i) =0 \; \; \forall \tilde{x}_i \in X^0$ and $X^1$ s.t.  $\, F(\tilde{x}_i)=1 \; \; \forall \tilde{x}_i \in X^1$, it is often useful to consider $d(X^0 , X^1) &amp;gt; \delta \in \mathbb{R}^{+}$. 
Consider that the $x-$points in the dataset $\tilde{\mathcal{D}}$ are in $X= X^0 \cup X^1$.

In this aforementioned case, it is in principle possible to find a continuous function $F(\cdot)$ such that $F(\tilde{x}_i) = \tilde{y}_i \; \forall i$ , since it does not need to be defined in the boundary (the boundary is not in $X$).

The simpler case is one in which $X^0$ and $X^1$ are connected spaces.



Deep Learning
Neural Networks

The problem is now how to obtain the functions $\mathcal{F}$.

In classical settings, an hypothesis space is often proposed as



where $\theta = { a_i }_{i=1}^{m}$ are the coefficients, ${ \psi _i}$ are the proposed functions (for example, they would be monomials if we are doing polynomial regression, sine and cosines if using Fourier, …) and the function $\phi$ is the terminal loss function, that is used to match the dimensionality of $f(x_i \; ;  \theta)$ with $y_i$; for example, in the case of binary classification, $\phi (  \cdot )$ can be chosen to be the function $sign ( \cdot )$, and so $\phi (\cdot ) : \mathbb{R} \to {0,1 }$.

The function $\mathcal{F}$ correspondent to the dataset $\mathcal{D}$ is then defined as



where $d ( , )$ is a distance defined by a loss function. 
That is, it is the function in the hypothesis space that minimizes the “distance” with the target function, which connects the datapoints $x_i$ with their correspondent labels $y_i$.

When using Neural Networks, the hypothesis space $\mathcal{H}$ is generated by the composition of functions, such that



where $\Theta = { \theta_k }_{k=1}^{L}$ are the training weights of the network and $L$ the number of hidden layers.

Multilayer perceptrons

In the case of the so-called multilayer perceptrons, the functions $f^k_{\theta_k}$ are constructed as



being $A^k$ a matrix,$b^k$ a vector and $\sigma \in C^{0,1}(\mathbb{R})$ a fixed nondecreasing function.

Since we restrict to the case in which the number of neurons is constant through the layers, we have that $A^k \in \mathbb{R}^{d \times d}$ and $b^k \in \mathbb{R}^d$.



Residual Neural Networks
In the simple case of a Residual Neural Network (often referred to as ResNets) the functions $f_{\theta_k}^{k} (\cdot)$ are constructed as



where $\mathbb{I}$ is the identity function. The parameters $\sigma$, $A^k$ and $b^k$ are equivalent to those of the multilayer perceptrons.

It is sometimes convenient to numerically add a parameter $h$ to the residual block, such that



being $h \in \mathbb{R}$ a scalar.



Training Neural Networks

The aim is to find the weights ${ A, b }$ of every layer such that the final function represented by the Neural Network minimizes a given loss function.

The weights are initialized randomly, and the algorithms (i.e. gradient descent) are aimed at updating them, such that eventually the Neural Network represents a function  $\hat{F}$  such that $\hat{F} (x_i) \approx y_i \, \forall i$.



Visualizing Deep Learning


  
    
    
    
  
  
    
    
    
  


Classification

When dealing with data classification, it is very useful to just assign a color / shape to every label, and so be able to visualize data in a lower-dimensional plot.

The aim of classification is to associate different classes to different regions of the initial space,
When using Neural Networks, the initial space $X$ is propagated by the Neural Network as $f ^L \circ  f^{L-1} \circ … \circ f^1 (X)$. 
Then the function $\phi$ would be the one in charge of associating different classes to different regions, but the functions $\phi$ are pretty simple (essentially linear separators)!

So, while it may be difficult to classificate the datapoints in the initial space $X$, the Neural Networks should make things simpler and just propagate the data so as to make it linearly separable (and we would see this in real action!).

Neural Networks as homeomorphisms

Lemma 1.1
A Neural Network represents a continuous function if each one of its blocks are continuous.

Lemma 1.2 
A Neural Network represents a bijection if each one of its blocks are bijective.

These come trivially from the fact that the composite of continuous functions is continuous, and that the composite of bijections is a bijection.

Theorem 1 (Sufficient condition for invertible ResNets [1]):

Let $\hat{\mathcal{F}} \, : \, \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ with $\hat{\mathcal{F}} (\cdot ) = ( L^{1}_{\theta} \, \circ … \circ \, L^{T}_{\theta}  )$ denote a ResNet with blocks defined by 


Then, the ResNet $\hat{\mathcal{F}}_{\theta}$ is invertible if



Proof 
Consider that each block of the ResNet is given by



where $x$ is the input of the block, and $G(x)$ given by the residual block.
That can be rewritten as



We can construct the inverse function of the block as



Which is not an explicit inverse, since $g(x)$ depends on $x$. Approximating the solution as an interation,



where $\lim_{k \to \infty} x_t ^{k} = x_t$ if the iteration converges. 
Since $g(\cdot) : \mathbb{R}^{d} \to \mathbb{R}^{d}$ is an operator on a Banach space, due to the Banach fixed point theorem is it enough that $Lip(g) &amp;lt; 1$, or equally that $Lip(h\cdot G) &amp;lt;1$ for the iteration to converge, and so for the block of the ResNet to be invertible. [1]

Comment: In the limit $h \to 0$, and $G(x)$ Lipschitz, we have a trivial explicit expression of the inverse of the function represented by the ResNet.

Consider the ResNet as generated by the iteration of blocks



where $G(x)$ is generated by a residual block . We consider $G(x)$ Lipschitz.

Then,



is an second order approximation of the inverse function of $Z$ in terms of $h$, since







Indeed, if we express the residual blocks as



the limit $h\to 0$ can be recasted as



and, since the discrete index $t$ becomes continuous in this limit, it is useful to express it as



where in this case we make explicit the dependence of $G$ on $t$, that was hidden in the previous notation.




  
    
    
  


Setup

Consider a binary classification problem with noisless data, such that






  
    
    
  
  
    
    
  





  
    
    
  
  
    
    
  





  
    
    
  





  
    
    
  
  
    
    
  




Open questions, issues and perspectives



References

[1] J. Behrmann, W. Grathwohl, R. T. Q. Chen, D. Duvenaud, J. Jacobsen (2018).  Invertible Residual Networks pdf

[2] Weinan E, Jiequn Han, Qianxiao Li, (2018).  A Mean-Field Optimal Control Formulation of Deep Learning. volume 6. Research in the Mathematical Sciencespdf

[3] S. Shalev-Shwartz, S. Ben-David,  The Understanding Machine Learning: From Theory to Algorithms . pdf

[4] B. Hanin, (2018).  Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients?   pdf

[5] C. Olah,  Neural Networks, Manifolds and Topology . blog post

[6] G. Naitzat, A. Zhitnikov, L-H. Lim, (2020).  Topology of Deep Neural Networks  pdf

[7] Q. Li, T. Lin, Z. Shen, (2019).  Deep Learning via Dynamical Systems: An Approximation Perspective pdf

[8] B. Geshkovski,  The interplay of control and deep learning . blog post

[9] C. Fefferman, S. Mitter, H. Narayanan,  Testing the manifold hypothesis . pdf

[10] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, D. Duvenaud, (2018).  Neural Ordinary Differential Equations  pdf

</description>
        <pubDate>Sun, 01 Nov 2020 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/DyCon-Blog/pretutorial/wp99/P0011</link>
        <guid isPermaLink="true">http://localhost:4000/DyCon-Blog/pretutorial/wp99/P0011</guid>
        
        
        <category>pretutorial</category>
        
        <category>WP99</category>
        
      </item>
    
      <item>
        <title>ResNet stabilization in MNIST dataset</title>
        <description>
MNIST dataset
Consideramos el problema de clasificación MNIST. Este problema tiene  datos de entrada tal que $x \in \mathcal{M}_{28 \times 28}(\mathbb{R})$ y datos de salida  tal que $y \in  \mathbb{R}^{10}$. Los elementos de la base canónica de $\mathbb{R}^{10}$, $ \{e_1,e_2,\dots,e_{10}\} $, representa el conjunto de dígitos $ \{0,1,2,\dots,8,9 \}$ en ese orden.


Figura 1. Distintos datos de entrada $x \in \mathcal{M}_{28 \times 28}(\mathbb{R})$ y sus correspondientes datos de salida $y \in  \mathbb{R}^{10}$ en la parte superior de cada una.   

2. Modelo
Buscamos una función $f_{\Omega}: \mathcal{M}_{28 \times 28}(\mathbb{R}) \rightarrow \mathbb{R}^{10}$ que sea capaz de reproducir el comportamiento que vemos en los datos. Consideraremos el modelo $\color{red}{y} = f_\Omega (\color{green}{x})$ como:



Donde:

  $\{z_t\}_{t=0}^{T} \in \mathbb{R}^n \ / \ n &amp;lt; 28^2$.
  $\mathcal{P} \in \mathcal{M}_{n\times 10}$ es una matriz constante que proyecta el estado $z_t \in \mathbb{R}^n$ a $\mathbb{R}^{10}$
  Las variables $\Omega = \{ A_t,b_t\}_{t=0}^T$ pueden verse como variables de control del sistema (\ref{sys}).
  $A_0 \in \mathcal{M}_{28^2 \times n}(\mathbb{R})$ y $b_0 \in \mathbb{R}^{28^2}$; mientras que $ \{A_t\}_{t=1}^{N} \in \mathcal{M}_{n \times n}(\mathbb{R}) $ y los siguientes $b_t$ bias son $ \{b_t\}_{t=1}^N \in \mathbb{R}^n $



Figura 2. Arquitectura de la red 

3. Problema de Control
Si llamamos $M$ al número total de datos de entrenamiento y considerando los datos de entrenamiento $\{ x_m,y_m\}_{m=1}^M$, podemos plantear el siguiente problema de control.



4. Resultados numéricos


Figura 3. Comportamiento de la proyección $\mathcal{P}z_t$ para un dado de entrada concreto. 


Figura 4. Evolución  $||\mathcal{P}z_t||_{L^2}^2$ para los distintos datos de entrada $x$ 

</description>
        <pubDate>Thu, 29 Oct 2020 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/DyCon-Blog/tutorial/wp02/P0005</link>
        <guid isPermaLink="true">http://localhost:4000/DyCon-Blog/tutorial/wp02/P0005</guid>
        
        
        <category>tutorial</category>
        
        <category>WP02</category>
        
      </item>
    
      <item>
        <title>Q-learning for finite-dimensional problems</title>
        <description>Reinforcement Learning

Reinforcement Learning (RL) is, together with Supervised Learning and Unsupervised Learning, one of the three fundamental learning paradigms in Machine Learning. The goal in RL is to enhance the manipulation of a controlled system by using data from past experiments. It differs from supervised learning and unsupervised learning because the algorithms are not trained with a fixed sampled dataset. Rather, they learn through trial and error.

In a generic RL problem, an agent, or decision maker, interacts with an unknown environment which is influenced and evolves according to the controls, or actions, that are chosen by the agent. The control chosen in each situation is then evaluated by means of a reward signal. Based on the experience acquired in previous trials, the algorithm has to learn what actions are better in each situation, in the sense that the accumulated long-time reward is maximized.

An optimal control problem

From a mathematical viewpoint, we can see RL as an optimal control problem. The environment is described through a dynamical system of the form



At each time-step, $x_t$ represents the state of the environment, which evolves according to the function $f: \mathbb{R}^n\times \mathcal{U} \to \mathbb{R}^n$. This function might be unknown by the agent, and depends on the current state $x_t$ and the control $u_t$ chosen by the agent. This choice is then evaluated by a reward $r_t = r(x_t,u_t)$.

The reward function $r: \mathbb{R}^n\times \mathcal{U}\to \mathbb{R}$ has to be designed in such a way that the desired behavior of the system is stimulated by providing greater rewards. It is important to note that the choice of $u_t$ not only affects the reward $r_t$. It also affects the future rewards as they will depend on future states of the system. The goal in RL is not to maximize the reward $r_t$ at each time-step, but rather to maximize the reward accumulated during a certain interval of time.

The optimal control problem that we are considering consists in maximizing, over the control strategy, the rewards accumulated during a process of infinite length



The parameter $\gamma \in (0,1)$ is called a discount factor, and ensures that the sum of rewards over the time-steps is finite. This is the so-called discounted infinite-horizon problem. In many applications, the introduction of the discount factor is also motivated by the fact that the rewards obtained in the near future are considered to be more important than those obtained with a bigger delay.

It is to be pointed out that this general setting is not exhaustive, and other classes of optimal control problems can be considered in the application of RL techniques. For example, it is typical to consider stochastic dynamics,  which is very suitable when the environment is considered to be unknown, or is subject to changes that we cannot control. The reward functional (2) can also have a different form (finite-time horizon, terminal reward, no discount factor,…). Although the discrete-time setting is the most common in RL, continuous-time problems can also considered.

The value function

The probably most important element in Reinforcement Learning is the optimal value function, defined as



It represents the best total reward that one can expect if the initial state is $x$. The value function satisfies the following Dynamic Programming Principle, also known as Bellman equation.



It allows the design of an optimal control in a feedback form.



The $Q$-function

In view of (3), we can construct a nearly optimal feedback control by approximating the value function $V^\ast$. But even if we were able to exactly compute the value function, the feedback law given in (3) makes use of the dynamics $f$. This means that the choice of the optimal control at a given state relies on the possibility of making predictions of what will be the next state, and as we mentioned, it is not in general the case in RL.

Therefore, rather than approximating the value function, we may concentrate our efforts on approximating the $Q$-function, defined as



It represents the best total reward that we can expect if the initial state is $x$ and the first control taken is $u$.

Observe that the optimal value function $V^\ast(x)$ determines how good is to be at a certain state $x$, whereas the function $Q(x,u)$ determines the quality of choosing $u$ given that the current state is $x$.

The $Q$-function allows the design of an optimal feedback control without using the dynamics $f$.



Using the identity $V^\ast (x) = \max_{u\in \mathcal{U}} Q(x,u)$, we can deduce the Bellman equation for the $Q$-function.



$Q$-learning

$Q$-learning is a RL algorithm, introduced by Watkins in 1989, that seeks to approximate the $Q$-function by exploring the state-control space $\mathbb{R}^n\times \mathcal{U}$. The exploration is made by running experiments of finite time-steps considering a randomized initial state. At each step, the approximation of the $Q$-function is improved by using the Dynamic Programming equation (4). During the learning process, an $\varepsilon$-greedy policy with respect to the current approximation of $Q$ is used. This policy ensures the exploration of the whole state-control space at the same time that it exploits the information obtained in previous experiments to enhance the approximation of $Q$ in regions that seem to be better in terms of total reward.

These are the main steps in the $Q$-learning algorithm.

  Initialize an arbitrary $Q$-function, $\widehat{Q}_0(x,u)$.
  Run a number $N$ of experiments (episodes) of $T$ steps each one.
  In total there will be $N\, T$ number of steps.
  $\varepsilon$-greedy policy:




and $u_k$ is chosen randomly with probability $\varepsilon$ (exploration vs exploitation).


  Improvement of $\widehat{Q}(x_t, u_t)$: after each time-step, we use the observed $(x_{t+1}, r_t)$ to improve the approximation of $Q(x_t,u_t)$ in the following way




here, $\alpha&amp;gt;0$ is called the learning rate.




 
  



 Figure 1.




Example

We illustrate the $Q$-learning algorithm through a simple example. In a given two-dimensional discrete domain, we consider the problem of finding the shortest path between any position of the domain and a prescribed target area.

Let us first define the environment. We consider a discretized rectangular domain:



We want to find the shortest path between any starting position in the domain and the top center of the domain, white area in the video below. The position can be moved, at each time-step, one unit up, down, right or left, i.e. the underlying dynamics are given by



where $\tau$ is the first time the state reaches the limits of the domain. The experiment is considered a success if the terminal position is in the target area, and a defeat otherwise. The possible controls are



We denote by $\partial \Omega$ the limits of the domain (blue squares in the video), i.e.



We denote by $\mathcal{T}$ the target area (white squares)



Now we need to define a reward that makes the agent move the position from the initial one $x$ to the target area in the minimum number of steps, without reaching the limits of the domain.



Giving a negative reward when $x_t\in \Omega$ stimulates the agent to take the minimum possible number of steps until the target area.

Since the state-space $\Omega$ and the set of possible actions $\mathcal{U}$ are both finite. The $Q$-function can be represented by a table with $40\times 40$ rows and $4$ columns. The entries in this table can be learned from experiments using the $Q$-learning algorithm described above. Once we have a good approximation of $Q$, for a given position $x$, we can determine the best possible action by looking at the corresponding row in the table and taking the action corresponding to the minimum value in that row.

In the following video we see the obtained behavior after applying the $Q$-learning algorithm with 10000 experiments, with a discount factor $\gamma =0.9$, learning rate $\alpha =0.9$ and an $\varepsilon-$greedy policy with $\varepsilon = 0.9$.
The plot represents the evolution of the success rate with the number of experiments during the training.




 
 


 Figure 2a.
Figure 2b.




We can change the environment by adding a potential that moves the position to the right when it is in a certain region (dark green area)



In the new environment, the agent do not know how to get to the target area.

In the following video, the agent uses the $Q$-function obtained in the original environment. However, the new dynamics are given by






 


 Figure 3.




After training the $Q$-function in the new environment, it learns how to deal with the potential to get to the target area. Observe that the success rate never reaches one. This is due to the fact that there is a region in the domain (the dark green area near the right-hand limit of the domain) from which it is impossible to reach the target.








 Figure 4a.
Figure 4b.




We can also add obstacles in the domain, or even combine obstacles with a potential.








 Figure 5a.
Figure 5b.




</description>
        <pubDate>Thu, 22 Oct 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/DyCon-Blog/tutorial/dp00/P0004</link>
        <guid isPermaLink="true">http://localhost:4000/DyCon-Blog/tutorial/dp00/P0004</guid>
        
        
        <category>tutorial</category>
        
        <category>DP00</category>
        
      </item>
    
      <item>
        <title>Moving control strategy for memory-type equations</title>
        <description>This tutorial shows how to implement the moving control strategy for the heat equation in the presence of a memory term.

The standard null controllability problem for the heat equation reads as follows



where $\Omega$ is a bounded domain, $u_0\in L^2(\Omega)$ and the control $f$ acts on the subdomain $\omega\subset\Omega$. The objective is to steer the system from $u_0$ to zero in time $T$,

It is known that the null controllability of the heat equation holds in any positive time $T&amp;gt;0$ and for any open control region $\omega$. Nevertheless, the situation changes when in the model appears a memory term:



As a matter of fact, when the equation involes a memory term, it is impossible to achive null controllability in the classical sense, unless the control region coincides with the entire domain ($\omega=\Omega$).

This fact can be easily seen when applying a LQR control on both systems (\ref{heatinterior}) and (\ref{heatinterior_memory}). In the former case (without memory) the minimum of the LQR functional is an optimal control stabilizing the dynamics (Video 1), whereas when the memory enters into the model this minimum leaves the system far from the equilibrium (Vdeo 2).




    
        
          
             
           
        
        
          
             
           
        
    
    
        
          
              Video 1: LQR in the heat equation (Equation \ref{heatinterior})
           
        
        
          
              Video 2:  LQR in the heat equation with memory (Equation \ref{heatinterior_memory})
           
        
     
    
        
          
              We can see the time evolution of the two models with a control region, $\chi_{\omega}$, located in the position of the blue ellipsoid in the simulation. We observe that the LQR control is effective for the heat equation but not for the heat equation with memory.
           
        
     


To understand the reason behind the lack of null controllability for the memory-type equation (\ref{heatinterior_memory}), we can introduce the new variable



In this way, (\ref{heatinterior_memory}) is transformed into a coupled PDE/ODE system:



In (\ref{heatinterior_memory_z}), the equation $z_t = u$ is an infinte-dimensional ODE which, since it has no diffusion term, introduces non-propagation phenomena in the system. The non propagating components corresponding to this ODE are unable to reach the control region $\omega$ and, therefore, cannot be controlled.

A natural remedy to this issue is to operate with a moving control strategy. If the ODE components of the system cannot reach the control region $\omega$, then is the control region who needs to reach these components. Hence, our control region would be  function of the time variable, $\omega = \omega(t)$, and the corresponding model will be



This moving control strategy has been proposed and succesfully applied in several contributions of our team [1,2,3,4].

Memory-type equations are one of the core topics of WP5 of the DyCon ERC project. An abridged presentation of this working package can be found at the following link.

Numerical implementation of the moving control strategy

We present here the numerical implementation of the moving control strategy for the memory-type heat equation (\ref{heatinterior_memory_z_moving}). All the simulations will be in $2D$.

STEP 1: construction of the control region

The first step is to construct the moving control region $\omega(t)$, which requires to design the function $\chi_{\omega(t)}$. To this end, we made the assumption that the size and orientation of $\omega(t)$ do not change during the time interval $[0,T]$. Hence, once fixed the size and shape of the control region, we will obtain a function depending only on a point $x\in \Omega$.

In this simulation, we create the control region as a square which moves in the domain $\Omega$. This square can be constructed with a function $W(x)$ defined as



where $H(x)$ is the Heaviside function. Nevertheless, the function $W$ constructed in this way would be non-differentialbe at the points $x=a$ and $x=b$. This would represent a problem when implementing the gradient method to compute the control. To bypass this issue, in the definition of $W(x,a,b)$ we will use a smooth approximation of $H$ (See Figure 1).


  
    
      
    
  
  
    
      Figure 1: Graphicla representation of $W(x,a,b)$
    
  


Moreover, the generalization of this function to dimension two is immediate



In this way, given $(x_{min}, x_{max}, y_{min}, y_{max})$, we obtain the corresponding characteristic function which only depends on the position of a single point.

STEP 2: construction of the dynamics

Since $\chi_{\omega}$ can move and, moreover, we want to obtain the optimal trajectory for the control, we will incorporate in our system a moving particle whose position and velocity are denoted by $\textbf{d} = (d_x,d_y)$ and $\textbf{v} = (v_x,v_y)$, respectively (Video 3).


  
    
       
    
  
  
    
      Video 3: Representación gráfica de  $W_{2D}$
    
  


Besides, we will add a control $\textbf{g}(t) = (g_x(t),g_y(t))$ which will act as an exterior force moving the subset $\chi_{\omega}$.

In this way, our model takes the final form



STEP 3: optimal control problem

Following a standard optimal control strategy, the control function for our memory-type equation will be computed throught the minimization problem





Simulations

All the simulations in this tutorial have been performed using the DyCon Toolbox and CasADi. Since CasADi is incorporated into the DyCon toolbox, we can install it in the following way:

unzip('https://github.com/DeustoTech/DyCon-Computational-Platform/archive/master.zip')
addpath(genpath(fullfile(cd,'DyCon-toolbox-master'))
StartDyconPlatform



We shall now write the discrete version of equation (\ref{heatinterior_memory_z_all}):



Secondly, let us create the mesh variable

% Create the mesh variables
clear;
Ns = 7;
Nt = 15;
xline = linspace(-1,1,Ns);
yline = linspace(-1,1,Ns);
[xms,yms] = meshgrid(xline,yline);



and the matrix $A$ containing the $2D$ Laplacian and the ODE dynamics

A  = FDLaplacial2D(xline,yline);

Atotal = zeros(2*Ns^2+4,2*Ns^2+4);
%
Atotal( 1:Ns^2  , 1:Ns^2 ) = A;
%
Atotal( Ns^2+1 : 2*Ns^2   ,   1    :  Ns^2   )  =  eye(Ns^2);
Atotal(    1   :  Ns^2    , Ns^2+1 : 2*Ns^2  )  =  50*eye(Ns^2); % z = 50*y

RumbaMatrixDynamics = [0 0 1 0; ...
                       0 0 0 1; ...
                       0 0 0 0; ...
                       0 0 0 0 ];

Atotal(2*Ns^2+1:end,2*Ns^2+1:end) = RumbaMatrixDynamics;
Atotal = sparse(Atotal);



Finally, we create the function $B(\textbf{d}) = B(x_d,y_d)$ for the dynamics of the moving control
%%
% We create the B() function
xwidth = 0.3;
ywidth = 0.3;
B = @(xms,yms,xs,ys) WinWP05(xms,xs,xwidth).*WinWP05(yms,ys,ywidth);
Bmatrix =  @(xs,ys) [diag(reshape(B(xms,yms,xs,ys),1,Ns^2)) ;zeros(Ns^2)];



We now have everything we need to construct and solve the optimal control problem

opti = casadi.Opti();  % CasADi optimization structure

% ---- Input variables ---------
Ucas = opti.variable(2*Ns^2+4,Nt+1); % state trajectory
Fcas = opti.variable(Ns^2+2,Nt+1);   % control

% ---- Dynamic constraints --------
dUdt = @(y,f) Atotal*u+ [Bmatrix(u(end-3),u(end-2))*f(1:end-2) ; ...
                                         0                     ; ...
                                         0                     ; ...
                                     f(end-1)                  ; ...
                                     f(end)                    ]; 

% -----Euler backward method-------
for k=1:Nt % loop over control intervals
   y_next = Ucas(:,k) + (T/Nt)*dUdt(Ucas(:,k+1),dUdt(:,k+1)); 
   opti.subject_to(Ucas(:,k+1)==y_next); % close the gaps
end

% ---- State constraints --------
opti.subject_to(Ucas(:,1)==[Y0 ; 0.7 ; 0.7; -1.5 ; -1.5]);

% ---- Optimization objective  ----------
Cost = (Ucas(1:end-4,Nt+1))'*(Ucas(1:end-4,Nt+1));

opti.minimize(Cost); % minimizing L2 at the final time

% ---- initial guesses for solver ---
opti.set_initial(Ucas, Unum_free);
opti.set_initial(Fcas, 0);

% ---- solve NLP              ------
p_opts = struct('expand',false);
s_opts = struct('acceptable_tol',1e-4,'constr_viol_tol',1e-3,'compl_inf_tol',1e-3);
opti.solver('ipopt',p_opts,s_opts); % set numerical backend
tic
sol = opti.solve();   % actual solve
toc



The following video shows the results of our simulations, which allowed to compute an effective moving control steering the dynamics to rest at time $T$.



  
    
       
    
  



Video 3




Bibliography

[1] U. Biccari and S. Micu, Null-controllability properties of the wave equation with a second order memory term, J. Differential Equations, 267(2), 2019, 1376-1422.

[2] U. Biccari and M. Warma, Null-controllability properties of a fractional wave equation with a memory term, Evol. Eq. Control The., 9(2), 2020, 399-430.

[3] F. W. Chaves-Silva, X. Zhang and E. Zuazua, Controllability of evolution equations with memory, SIAM J. Control Optim., 55(4), 2017, 2437–2459.

[4] Q. Lü, X. Zhang and E. Zuazua, Null controllability for wave equations with memory, J. Math. Pures Appl., 108 (4), 2017, 500-531.
</description>
        <pubDate>Tue, 20 Oct 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/DyCon-Blog/tutorial/wp05/P0011</link>
        <guid isPermaLink="true">http://localhost:4000/DyCon-Blog/tutorial/wp05/P0011</guid>
        
        
        <category>tutorial</category>
        
        <category>WP05</category>
        
      </item>
    
      <item>
        <title>Optimal Control Problem vs  Classification Problem in DyCon Toolbox</title>
        <description>En este tutorial veremos como podemos implementar un problema de clasificación simple mediante la librería de control óptimo DyCon-Toolbox.

Generación de datos

Para resolver un problema de clasificación supervisada necesitamos datos de entrada ${ \vec{x}{i} }{i=1}^N$ y de salida ${ \vec{y}{i} }{i=1}^N$. Eso lo generaremos de manera sintéctica con las siguientes lineas de código
clear;
N = 12;
xdata = linspace(-1,1,N);
ydata = [-1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1];


Podemos ver los datos generados
f = figure(1)
plot(xdata,ydata,'o-','LineWidth',2)
xlabel('x-data')
ylabel('y-data')
title('Data')
print('data.png','-dpng')





Ahora queremos una ecuación diferencial controlada para una variable $z(t)$ tal que cuando reciba una condición inicial $z(0) = x_i$ evolucione durante $t=1$, y devuelva $z(t)=y_i$, esto para $\forall i \in {1,\dots,N}$.

Es decir una ecuación diferencial, tal que



Donde $\mathcal{P}$ es una proyección del 
Es decir es una
sigma = @(x) tanh(x);
%% Dynamic definition
import casadi.*
As = SX.sym('A',[2 2]);
bs = SX.sym('b',[2 1]);
zs = SX.sym('z',[2*N 1]);
% Create a Big matrices
Afull =  SX.zeros(2*N,2*N);
for i = 1:N
   ind = ((i-1)*2+1):(i*2);
   Afull(ind,ind) = As; 
end
%
bfull = repmat(bs,N,1);
% sym time
ts = SX.sym('t');
% this is the control
us = [As(:);bs];
% define the dynamic equation
Fs = casadi.Function('F',{ts,zs,us},{Afull*sigma(zs) + bfull});


Creamos el objeto ode
Nt = 50; tspan = linspace(0,1,Nt);
%
idyn = ode(Fs,zs,us,tspan); % &amp;lt;- idyn is a ode object of DyCon Toolbox
SetIntegrator(idyn,'RK4') % &amp;lt;- For solve you need choose a numerical squeme
% initial condition
z0          = zeros(2*N,1);
z0(1:2:2*N) = xdata';
% put in idyn object 
idyn.InitialCondition = z0;


Optimal Control Definition

P = [1 1]; % no optimize P
Pfull = repmat(P,N,1);
Pzminusy = P*reshape(zs,2,N)-ydata; % &amp;lt;-- (P*z_i - y_i)
L   = casadi.Function('L',  {ts,zs,us},{ sum(sum(As.^2))  + bs'*bs  });
Psi = casadi.Function('Psi',{zs}      ,{ 1e5*(Pzminusy*Pzminusy') });
%
iocp = ocp(idyn,L,Psi);



Solve
[Uopt,Zopt] = IpoptSolver(iocp,ZerosControl(idyn));



%% Plot
figure(1)
clf
subplot(1,2,1)
l1 = plot(tspan,Zopt(1:2:2*N,:)','b');
hold on
l2 = plot(tspan,Zopt(2:2:2*N,:)','r');
legend([l1(1) l2(1)],{'z_i^1(t)','z_i^2(t)'},'Location','bestoutside')

title('Optimal State')
ylabel('z_i(t)')
xlabel('time')

subplot(2,2,2)
plot(tspan,Uopt(1:4,:)')
title('Optimal Control - A(t)')
xlabel('time')
legend({'A_1','A_2','A_3','A_4'},'Location','bestoutside')

subplot(2,2,4)
plot(tspan,Uopt(5:end,:)')
title('Optimal Control - b(t)')
xlabel('time')
legend({'b_1','b_2'},'Location','bestoutside')


Animation
fig = figure(2);
clf
hold on

xlim([-5 5])

ylim([-5 5])
for iter = 1:N
    if ydata(iter) &amp;gt; 0
        color = 'r';
    else
        color = 'b';
    end
    ilines(iter) = plot(Zopt(2*(iter-1)+1,1),Zopt(2*(iter-1)+2,1),'Color',color,'Marker','.','MarkerSize',20);
    jlines(iter) = plot(Zopt(2*(iter-1)+1,1),Zopt(2*(iter-1)+2,1),'Color',color,'Marker','none','MarkerSize',20);

end
% 
for it = 1:length(tspan)
   for iter = 1:N
    ilines(iter).XData = Zopt(2*(iter-1)+1,it);
    ilines(iter).YData = Zopt(2*(iter-1)+2,it);
    jlines(iter).XData = Zopt(2*(iter-1)+1,1:it);
    jlines(iter).YData = Zopt(2*(iter-1)+2,1:it);
   end
    pause(0.05)
end


</description>
        <pubDate>Mon, 27 Jul 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/DyCon-Blog/pretutorial/wp01/P0012</link>
        <guid isPermaLink="true">http://localhost:4000/DyCon-Blog/pretutorial/wp01/P0012</guid>
        
        
        <category>pretutorial</category>
        
        <category>WP01</category>
        
      </item>
    
      <item>
        <title>Stochastic optimization for simultaneous control</title>
        <description>What is a simultaneous control problem?

Consider the following parameter-dependent linear control system



with



The matrix $\mathbf{A}_\nu$ is associated with the Brunovsky canonical form of the linear ODE



where $x^{(N)}_\nu(t)$ denotes the $N$-th derivative of the function $x(t)$.

In (1)-(2), $x_\nu(t)\in L^2(0,T;\mathbf{R}^N)$, $N\geq 1$, denotes the state, the $N\times N$ matrix $\mathbf{A}_\nu$ describes the dynamics, and the function $u(t)\in L^2(0,T;\mathbb{R}^M)$, $1\leq M\leq N$, is the $M$-component control acting on the system through the $N\times M$ matrix $\mathbf{B}$. Here 

 is a random parameter following a probability law $\mu$, with $(\mathcal K,\mathcal F,\mu)$ the corresponding complete probability space.

With simultaneous controllability we refer to the problem of designing a unique parameter-independent control function capable to steer all the different realizations of (1) to some prescribed final target in time $T$, that is (see Figure 1 and Figure 2):





Figure 1: evolution of the free (left) and controlled (right) dynamics of (1)-(2) with $N=4$.



Figure 2: parameter-independent control function.

How can we solve a simultaneous controllability problem?

The computation of a simultaneous control for (1)-(2) can be carried out through a standard optimal control methodology by solving the following minimization problem



subject to the dynamics given by (1)-(2). Here $\mathbb{E}[\cdot]$ denotes the expectation operator.

Typical approaches to solve the optimization problem (3) are (see [3]):


  The Gradient Descent (GD) algorithm: we find the minimizer $\widehat{u}$ as the limit $k\to +\infty$ of the following iterative process




where for all $\nu\in\mathcal K$ the pair $(x_\nu,p_\nu)$ solves the coupled system




  The Conjugate Gradient (CG) algorithm: the gradient $\nabla F_\nu$ is rewritten in the form




where the operators $\mathcal L_{T,\nu}$ and $\mathcal L_{T,\nu}^\ast$ are defined as



with $U:=L^2(0,T;\mathbb{R}^M)$ and



We then use the conjugate gradient algorithm to solve the linear system $\mathbb{A}u = b$.

These two procedures are impractical when the cardinality of the parameter set $\mathcal K$ is large because they require repeated resolutions of the dynamics (4) for all $\nu\in \mathcal K$.

This issue can be bypassed by employing a stochastic optimization method. In particular, we can consider the following approaches:


  The Stochastic Gradient Descent (SGD) algorithm (see [2]): it is a drastic simplification of the classical GD in which, instead of computing the gradient of the functional for all parameters $\nu\in\mathcal K$, in each iteration this gradient is estimated on the basis of a single randomly picked configuration. This translates in the following recursion process




with $\nu_k$ selected i.i.d. from $\mathcal K$.

  The Continuous Stochastic Gradient (CSG) algorithm: it is a variant of SGD, based on the idea of reusing previously obtained information to improve the efficiency. The CSG recursion process for optimizing $F_\nu$ is given by




where the weights ${\alpha_\ell}_{\ell = 1}^k$ are obtained through the methodology presented in [4].

Experimental Results: comparison of the algorithms

We can test the efficiency of each one of the four aforementioned algorithms by performing simulations for increasing values of 
.
 In these simulations, we have chosen the initial state $x^0 =(1,1,1,1)^\top$ and the final target $x^T=(0,0,0,0)^\top$. The time horizon is set to be $T=1s$. The parameter set is a $|\mathcal K|$ points partition of the interval $[1,6]$: 

 with $\nu_1=1$ and $\nu_{|\mathcal K|}=6$.

Figure 3 displays the computational times (in logarithmic scale) the four algorithms need to compute a simultaneous control for (1)-(2).



Figure 3: computational time (in logarithmic scale) to converge to the tolerance $\varepsilon = 10^{-4}$ of the GD, CG, SGD and CSG algorithms applied to the problem (1)-(2) with different values of $|\mathcal K|$.

We can observe the following facts:


  The GD algorithm is the one showing the worst performances. This because it has to copy with a high per-iteration cost but also with the fact that controllability problems are typically bad conditioned.
  The CG algorithm is the one requiring the lower number of iterations to converge. This implies that CG is the best approach among the one considered when dealing with a low and moderate amount of parameters.
  The stochastic approaches SGD and CSG appear to be insensitive to the cardinality of the parameter set. This fact is not surprising if we consider that, no matter how many parameters enter in our control problem, with SGD and CSG each iteration of the optimization process always requires only one resolution of the coupled system (4).
  The CSG algorithm always outperforms SGD in terms of the number of iterations it requires to converge and, consequently, of the total computational time. This because in CSG the approximated gradient is close to the full gradient $\nabla F_\nu$ of the objective functional when $k\to +\infty$. This translates in a less noisy optimization process with better convergence behavior (see Figure 4).




Figure 4: convergence of the error for SGD and CSG. The plots correspond to $50$ launches of the two algorithms with a tolerance $\varepsilon = 10^{-4}$.


All these considerations corroborate the fact that, when dealing with large parameter sets, a stochastic approach is preferable to a deterministic one to address the simultaneous controllability of (1)-(2).

A more complete discussion on the employment of stochastic optimization algorithms for simultaneous controllability can be found in [1].

Bibliography

[1] U. Biccari, A. Navarro-Quiles and E. Zuazua, Stochastic optimization methods for the simultaneous controllability of parameter-dependent systems, preprint (2020).

[2] L. Bottou, F. E. Curtis and J. Nocedal, Optimization methods for large-scale machine learning, SIAM Rev., Vol. 60, No. 2 (2018), pp. 223-311.

[3] J. Nocedal and S. Wright, S, Numerical optimization, Springer Science &amp;amp; Business Media, 2006.

[4] L. Pflug, N. Bernhardt, M. Grieshammer and M. Stingl, A new stochastic gradient method for the efficient solution of structural optimization problems with infinitely many state problems, preprint (2020).
</description>
        <pubDate>Wed, 03 Jun 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/DyCon-Blog/tutorial/wp01/P0011</link>
        <guid isPermaLink="true">http://localhost:4000/DyCon-Blog/tutorial/wp01/P0011</guid>
        
        
        <category>tutorial</category>
        
        <category>WP01</category>
        
      </item>
    
  </channel>
</rss>
