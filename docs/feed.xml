<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DyCon Blog</title>
    <description>Welcome to the web interface of DyCon Toolbox, the computational platform developed within the &lt;a href='https://cmc.deusto.eus/dycon/' target='_blank'&gt;ERC DyCon - Dynamic Control&lt;/a&gt; project.</description>
    <link>https://deustotech.github.io/DyCon-Blog/</link>
    <atom:link href="https://deustotech.github.io/DyCon-Blog/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 27 Apr 2020 08:44:03 +0200</pubDate>
    <lastBuildDate>Mon, 27 Apr 2020 08:44:03 +0200</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Control of reaction-diffusion under state constraints - Heterogeneous setting: Gene-flow.</title>
        <description>
    
    Control of reaction-diffusion
    This is a post in a collection of several on control of reaction-diffusion under state constraints
    See more ...
    


Equation and drift effect

The equation under concern is:



where $N$ is a positive regular function.

The physical meaning of the equation is a reaction-diffusion process on the proportion $u$ of individuals along a static heterogeneous distribution $N$.

The effect of this heterogeneous drift can be qualitatively understood in Figure 1. The blue line in the figure is the distribution $N$ and the orange one the term $-2\frac{N_x}{N}$. In the picture in the left we have the Gaussian with variance $\sigma$, leading to a drift effect that is pushing $u$ towards the boundary. At the right, we have the function $e^{x^2/\sigma}$ leading to the contrary effect, the population in the boundary is pushed inside the domain.

From the discussion above we expect the boundary control to be enhanced in the case of $N(x)=e^{x^2/\sigma}$ while diminished in the case of $N(x)=e^{-x^2/\sigma}$.



    
        
            
        
         
            
               
    


Figure 1. In blue the curves $N(x)=e^{\mp x^2/\sigma}$ and in orange the term $-2\frac{N_x}{N}$.

Slowly varying drifts: Perturbed Stair-case method

Whenever the drift term $-2\frac{N_x}{N}$ is small enough, one can consider the already developed path in the homogeneous setting (see Blog staircaise), taka a sequence and apply the implicit function theorem to obtain a sequence close enough to guarantee the controllability (see [1] for details). This guarantees the controllability from the steady state $0$ to the target $\theta$.

In Figure 2 an intuitive sketch of this procedure is represented.




Figure 2. Perturbed staircase method.

Large drifts: New Barriers

One of the main concerns when state-constraints are present is whether or not barriers exist (see Blog Barriers). If $\theta&amp;lt;\frac{1}{2}$ then, for large $L&amp;gt;L_{\sigma}(0)&amp;gt;0$ we will have obstructions to reach $\theta$ and $0$ from $1$.

In the homogeneous case, nontrivial elliptic solutions with boundary value $1$ do not exist. However, in the case of heterogeneous drift this is no longer true in general.

If we set $N(x)=e^{-x^2/\sigma}$ then for every $\sigma$ one can find an $L_{\sigma}(1)&amp;gt;0$ big enough such that for every $L&amp;gt;L_{\sigma}$ there is a nontrivial solution with boundary value $1$. This nontrivial solution does not correspond to a global minima of the associated energy functional since the trivial solution $u=1$ is the global minimizer.

In order to understand how this barrier can exist we have to study the following non-autonomous dynamics in the phase-plane:



Any solution of the ODE above is an elliptic solution for some boundary condition. What we want to find is that there exist an $L_{\sigma}(1)$ such that the solution of the ODE at $L_{\sigma}(1)$ is $1$.

Making use of the energy of the ODE



and seting $a$ small one can see that the trajectory of the dynamics will eventually cross $1$. See Figure 3.



    
        
            
        
         
            
               
    



Figure 3. At the left, the trajectory in the phase-plane associated with the elliptic solution. At the right, the resulting nontrivial solution.

In Figure 4 one can see the effect of the nontrivial solution in a simulation. The red lines are snapshots of the controlled trajectory, getting darker when advancing the time, the dotted blue line is the profile $N(x)$.




Figure 4. Simulation of the equation with control $a=1$ with $N(x)=e^{-x^2/\sigma}$.

Minimal controllability time depending on the drift

We have seen that the influence of the drift is key. Figure 5 shows the minimal controllability time for a fixed $L$ depending on the parameter $\frac{1}{\sigma}$. The initial state is the $0$ configuration and the target the constant steady state $\theta$.

When the variance is very big we tend to the homogeneous case, while when the variance decreases the opposite phenomena is observed; when the drift enhances the control the minimal controllability time is reduced making it tend to zero while, when the drift is pushing the population outside the minimal controllability time increases and eventually blows up with the emergence of the barrier.



    
        
            
        
         
            
               
    



Figure 5. Minimal controllability time depending on the variance. At the left for $N(x)=e^{x^2/\sigma}$ and at the right for $N(x)=e^{-x^2/\sigma}$. 

References:
[1] I. Mazari, D. Ruiz-Balet, and E. Zuazua, Constrained control of bistable reaction-diffusion equations: Gene-flow and spatially heterogeneous models, preprint: https://hal.archives-ouvertes.fr/hal-02373668/document (2019).

[2] D. Pighin, E. Zuazua, Controllability under positivity constraints of multi-d wave equations, in:
Trends in Control Theory and Partial Differential Equations, Springer, 2019, pp. 195–232.

[3] J.-M. Coron, E. Trélat, Global steady-state controllability of one-dimensional semilinear heat equa-
tions, SIAM J. Control. Optim. 43 (2) (2004) 549–569.

[4] C. Pouchol, E. Trélat, E. Zuazua, Phase portrait control for 1d monostable and bistable reac-
635
tion–diffusion equations, Nonlinearity 32 (3) (2019) 884–909.

[5]   D.  Ruiz-Balet  and  E.  Zuazua. Controllability  under  constraints  for  reaction-diffusionequations:   The  multi-dimensional  case.   Preprint  available  athttps://cmc.deusto.eus/domenec-ruiz-balet/.

[6] D.  Ruiz-Balet  and  E.  Zuazua. Control of certain parabolic models from biology and social sciences.   Preprint  available  at https://cmc.deusto.eus/domenec-ruiz-balet/.

</description>
        <pubDate>Sun, 19 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0008</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0008</guid>
        
        
        <category>tutorial</category>
        
        <category>WP03</category>
        
      </item>
    
      <item>
        <title>Control of reaction-diffusion under state constraints - Numerical exploration of controls</title>
        <description>
    
    Control of reaction-diffusion
    This is a post in a collection of several on control of reaction-diffusion under state constraints
    See more ...
    


In this tutorial we will observe different phenomena that arise in the control with state-constraints and moreover we will observe different ways to approach a desired target.

Observation of several phenomena

In the simulations below, we will consider the following optimal control problem:



In Figure 1, the initial state was $u_0=1$ and we can observe how the solution has been successfully driven to the target $u_1=0.33$. For this length of the domain there is no barrier (see Barriers)), for this reason the optimal solution can achieve the target.

The trajectory of the control can also be understood from the theoretical understanding of the controllability under state constraints.

In the first phase, the control takes values that are below the target $u_1=0.33$. This behavior is necessary because for this length of the domain there exist nontrivial solutions with boundary value $0.33$ that are between $0$ and $1$.

These nontrivial solutions do not constitute an intrinsic obstruction since we know that the equation is controllable for this length of the domain (see [1,3] and Staircase post and Barriers post). However, the strategy of setting boundary equal to $0.33$ for a long time and then doing a local control will not work.

The second oscillatory phase has been already observed in the unconstrained case (see [4]).



    
        
            
        
         
            
               
    



Figure 1. At the left, the controlled state, at the right, the control function. $u_0=1$ and $u_1=0.33$. 

Another important feature is the existence of a minimal controllability time. In Figure 2, $u_0=0$ and the target is $u_1=0.33$. From the theoretical perspective, if the time horizon is large enough we are always able to go from the initial state to the target. This is due to the existence of an admissible continuous path of steady states (Staircase post).

However, in Figure 2, one observes that the target $u_1=0.33$ has been not achieved. The reason is that $T$ is not large enough for the equation to be controllable.



    
        
            
        
         
            
               
    



Figure 2. At the left, the controlled state displayed in space-time, at the right, different snapshots of the state, darker curves are associated with larger times. $u_0=0$ and $u_1=0.33$ .

The other phenomenology already mentioned above and in the blog post Barriers is the lack of controllability due to the emergence of nontrivial solutions. In Figure 3, the initial state is $u_0=1$ and the target is $u_1=0$. We can observe how the state gets stuck to the barrier.



    
        
            
        
         
            
               
    



Figure 3. At the left, the controlled state displayed in space-time, at the right, different snapshots of the state, darker curves are associated with larger times. $u_0=1$ and $u_1=0$ .

Exploration

Now we turn our attention to the exploration of different control strategies.

Quasistatic

In Figure 4, we seek to find a control that is quasistatic, for doing so we penalize the time derivative of the control in the discrete level and we add the restriction $u(x,T)=0.33$.





    
        
            
        
         
            
               
    



Figure 4. At the left, the controlled state displayed in space-time, at the right, the control function. $u_0=0$ and $u_1=0.33$ . 

Minimal control time

In Figure 5, we compute the control in minimal time by minimizing the functional:





    
        
            
        
         
            
               
    



Figure 5. At the left, the controlled state displayed in space-time, at the right, the control function. $u_0=0$ and $u_1=0.33$ .

Figure 5 points out that the control in minimal time has the bang-bang property.

Minimal flow control

In Figure 6, we compute the control with minimal flow by minimizing the functional:





    
        
            
        
         
            
               
    



Figure 6. 

References:

[1] D.  Ruiz-Balet  and  E.  Zuazua. Control of certain parabolic models from biology and social sciences.   Preprint  available  at https://cmc.deusto.eus/domenec-ruiz-balet/.

[2] J.-M. Coron, E. Trélat, Global steady-state controllability of one-dimensional semilinear heat equa-
tions, SIAM J. Control. Optim. 43 (2) (2004) 549–569.

[3] C. Pouchol, E. Trélat, E. Zuazua, Phase portrait control for 1d monostable and bistable reac
tion–diffusion equations, Nonlinearity 32 (3) (2019) 884–909.

[4]  R. Glowinski, J. L. Lions and J. He. “Exact and approximate controllability for distributed parameter systems: a numerical approach.” Encyclopedia of Mathematics and its Applications (2008).

</description>
        <pubDate>Sat, 04 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0009</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0009</guid>
        
        
        <category>tutorial</category>
        
        <category>WP03</category>
        
      </item>
    
      <item>
        <title>Inverse problem for Hamilton-Jacobi equations</title>
        <description>Background and motivation

We consider the initial-value problem for a Hamilton-Jacobi equation of the form



where $u_0\in C^{0,1}(\mathbb{R}^n)$ and the Hamiltonian $H: \mathbb{R}^n\rightarrow\mathbb{R}$ is assumed to satisfy the following hypotheses:



Here, the unknown $u$ is a function $[0,T]\times\mathbb{R}^n\longrightarrow\mathbb{R}$,
and the inequality $H_{pp}(p)&amp;gt;0$ means that the Hessian matrix of $H$ at $p$ is positive definite.
The study of Hamilton-Jacobi equations
arises in the context of optimal control theory and calculus of variations,
where the value function satisfies, in a weak sense, a Hamilton-Jacobi equation.
In this context, Hamilton-Jacobi equations have applications in a wide range of fields such as
economics, physics, mathematical finance, traffic flow and geometrical optics.

Our goal in this tutorial is to study the inverse design problem associated to (HJ).
More precisely, for a given target function $u_T$ and a time horizon $T&amp;gt;0$,
we want to construct all the initial conditions $u_0$ such that the viscosity solution of (HJ) coincides with $u_T$ at time $T$.

The study of this problem can also be motivated by considering the following question:

Given an observation of the solution to (HJ) at time $T&amp;gt;0$, can we construct all the possible initial data that agree we the observation at time $T$ ?

For this purpose, for a fixed $T&amp;gt;0$, 
we define the following nonlinear operator, which associates to any initial condition $u_0$, the function
$u(T,\cdot)$, where $u$ is the viscosity solution of (HJ):



The inverse design problem that we are considering is then reduced to, for $u_T$ and $T&amp;gt;0$ given,
characterize all the initial conditions $u_0$ satisfying $S_T^+ u_0 =u_T$.

Reachability of the target

The first thing one notices when addressing this problem is that not all the Lipschitz targets are reachable.
Indeed, as it is well known, the viscosity solution to (HJ) is always a semiconcave function (see [2,4]). 
Therefore, an obvious necessary (but not sufficient) condition for the reachability of $u_T$ is that it must be a semiconcave function.
See Figures 1 and 2 for some examples of unreachable targets in dimension 1 and 2 respectively. Observe that, in particular, they are not semiconcave functions.



  




 Figure 1:  Two unreachable target functions in dimension 1.


We then start our study by characterizing the set of targets that are reachable.
In other words, for a given $u_T$ and $T&amp;gt;0$, we aim to determine whether or not there exists at least one initial condition $u_0$ satisfying $S_T^+ u_0=u_T$.
The natural candidate is the one obtained by reversing the time in the equation, considering $u_T$ as terminal condition. Here, we have to consider the class of backward viscosity solutions, for which the terminal-value problem associated to (HJ) is well-posed (see for example [1]).

We then define the backward operator as follows:



which associates to any terminal condition $u_T$, the function $w(0,\cdot)$,
i.e. the unique backward viscosity solution at time $0$, with terminal condition $u_T$.



  




 Figure 2:  Two unreachable target functions in dimension 2.


Let us introduce, for a target $u_T$ and a time horizon $T&amp;gt;0$, the set



of initial conditions $u_0$ satisfying $S_T^+ u_0 = u_T$.
We say that a target $u_T$ is reachable for the time horizon $T$ if $I_T(u_T)\neq \emptyset$.
We can equivalently say that $u_T$ is an admissible observation at time $T&amp;gt;0$.

Here we give a result that identifies the set of reachable targets (or admissible observations) at time $T$,
with the fix-points of the composition operator $S_T^+\circ S_T^-$.



Theorem 1: Let $H$ satisfy (2), $u_T\in C^{0,1}(\mathbb{R}^n)$ and $T&amp;gt;0$. Then, the set $I_T(u_T)$ is nonempty 
if and only if  $S_T^+ \left( S_T^- u_T\right) = u_T.$



In other words, a target is reachable if and only if the initial condition $\tilde{u}_0:= S_T^- u_T$, obtained by applying the backward operator, satisfies $\tilde{u}_0\in I_T(u_T)$.
Or equivalently, an observation $u_T$ at time $T$ is admissible if and only if $S_T^+ \left( S_T^- u_T\right) = u_T.$

Projection on the set of reachable targets

In the case the observation $u_T$ at time $T$ is not admissible, maybe due to noise effects or to errors in the measurements, we can actually project it on the set of admissible observations by applying the operator $S_T^+\circ S_T^-$. For a given $u_T\in C^{0,1}(\mathbb{R}^n)$, we define this projection as follows:



Observe that $I_T(u_T^\ast)\neq \emptyset$ for any $u_T\in C^{0,1}(\mathbb{R}^n)$. 
Indeed, $\tilde{u}_0:=S_T^- u_T\in I_T (u_T^\ast)$ by definition.



  

  




 Video 1:  Projection of the examples in Figure 1 on the set of admissible observations at times $T=0.3$ and $T=0.8$ respectively.


In the Videos 1 and 2, we observe how the examples in Figures 1 and 2 are projected on the set of reachable targets.
We recall that the projection is obtained by solving the problem (HJ) backward in time and the forward.
Observe that, when the time goes backward, the solution is semiconvex, while in the forward resolution, it becomes semiconcave. See [1,4] for the classical results concerning this regularizing effect and [2] for the definition and properties of semiconcave and semiconvex functions.



  

  




 Video 2:  Projection of the examples in Figure 2 on the set of admissible observations at time $T=1$.


Construction of initial data

Once we have projected the observation $u_T$ on the set of reachable targets at time $T$, we proceed to the construction of all the initial data $u_0$ satisfying $S_T^+ u_0 = u_T^\ast$.
This construction is contained in the recent work [3], and reads as follows:


Theorem 2: Let $H$ satisfy (2) and $T&amp;gt;0$.
Let $u_T\in C^{0,1}(\mathbb{R}^n)$ and define the functions



Then, for any $u_0\in C^{0,1} (\mathbb{R}^n)$, the two following statements are equivalent:

  $u_0\in I_T(u_T^\ast)$;
  $u_0(x)\geq \tilde{u}_0 (x), \ \forall x\in \mathbb{R}^n \quad \text{and} \quad u_0(x) = \tilde{u}_0(x), \ \forall x\in X_T(u_T^\ast),$


where $X_T(u_T^\ast)$ is the subset of $\mathbb{R}^n$ given by





In view of this result, we observe that, for a given admissible observation $u_T^\ast$ at time $T$,
the construction of the possible initial data can be carried out after obtaining the two following ingredients:

  the function $\tilde{u}_0 = S_T^- u_T^\ast$, obtained as the backward viscosity solution to (HJ) with terminal condition $u_T^\ast$;
  and the set $X_T(u_T^\ast)\subset \mathbb{R}^n$, that can be deduced from the differentiability points of $u_T^\ast$.


In Figures 3 and 4 below, we can see the scheme of the construction of all the initial data for the projection $u_T^\ast = S_T^+ (S_T^- u_T)$ of the non-admissible observations in Figure 1.
Observe that all the initial data in $I_T(u_T^\ast)$ must coincide with $\tilde{u}_0$ on the red region, while on the black region, they must be greater or equal than $\tilde{u}_0$.



  




 Figure 3:  Reconstruction scheme for the admissible target obtained in Video 1 at the left.




  




 Figure 4:  Reconstruction scheme for the admissible target obtained in Video 1 at the right.


An important consequence of Theorem 2 is that the initial data in $I_T(u_T^\ast)$ are not unique whenever $X_T(u_T^\ast) \neq \mathbb{R}^n$.
Indeed, the set $I_T(u_T^\ast)$ can be given in the following way:



In the case of the examples illustrated in Figures 3 and 4,
we can see in the videos 3 below, examples of different initial conditions whose solution coincides with $u_T^\ast$ at time $T$.
These exmaples have been generated randomly by adding to $\tilde{u}_0$ a nonegative a Lipschitz function vanishing in $X_T(u_T^\ast)$.



  
  




 Video 3:  Examples of initial data $u_0\in I_T(u_T^\ast)$ for the reconstruction schemes depicted in Figures 3 and 4.


A possible interpretation of this reconstruction result is that the initial datum can only be reconstructed in the region $X_T(u_T^\ast)$, where it is uniquely determined by $\tilde{u}_0$,
while in the region $\mathbb{R}^n\setminus X_T(u_T^\ast)$, only a lower bound can be deduced.
In this last region, we can say that the information of the initial data has been partially lost after the time-interval $[0,T]$.
In the case of smooth solutions (when they exists) we have backward uniqueness, i.e. $I_T(u_T)$ is either empty or a singleton (see [1]).
In this case it holds that $X_T(u_T)=\mathbb{R}^n$.

Next, we see the reconstruction schemes for the examples depicted in Video 2, that correspond to the projection on the set of admissible observations of $u_T$ from Figure 2.
Observe the different structure of the set $\mathbb{R}^n\setminus X_T(u_T^\ast)$ (white region in the plot at the right)
in the surrounding area of bumps and wells of $u_T^\ast$.

  Near a bump, the set where $u_T^\ast$ is not differentiable is an isolated point, and then, it produces a ball for the set $\mathbb{R}^n\setminus X_T(u_T^\ast)$.
  Near a well, $u_T^\ast$ is non-differentiable on a circumference. This produces an annulus for the set $\mathbb{R}^n\setminus X_T(u_T^\ast)$.




  




 Figure 5:  Reconstruction scheme for the admissible target obtained in Video 2 at the left.




  




 Figure 6:  Reconstruction scheme for the admissible target obtained in Video 2 at the right.


Evolution of $X_T(u_T)$

As we have seen in the previous section, given an admissible observation $u_T$ of the solution at time $T$,
the initial datum can only be reconstructed in a region $X_T(u_T)$, while in its complementary, only a lower bound can be obtained.
Here, we are interested in the  the role that $T$ plays in this reconstruction issue.
For this purpose, we fix an initial datum $u_0$ and then, for each $T&amp;gt;0$,
we construct the set $X_T(u_T)$, where $u_T := S_T^+ u_0$.

Our goal is to observe how $X_T(u_T)$ evolves as we increase $T$,
or in other words, we want to know what information from $u_0$ can be recoverd when the solution is observed at time $T$.
As we see in the following videos, $X_T(u_T)$ becomes smaller as we increase $T$, meaning that less information from the initial datum can be recovered if the observation is made after a long time interval.

In Figure 7 we fix two initial data in dimension 1.



  




 Figure 7:  Two initial data in dimension 1.


Next, we can see in in Video 4, how the reconstruction scheme for $u_0$ (at the left in Figure 7) evolves as we increase the observation time.
For $T$ small, very little information is lost, however, for $T$ large, $X_T(u_T)$ only intersects the nonpositive region of $u_0$, and therefore, the two bumps can no longer be reconstructed.



  




 Video 4:  Evolution of the target and the reconstruction scheme for the initial datum at the left in Figure 7.


In Video 5, we see the evolution of the reconstruction scheme for $u_0$ at the right in Figure 7.
In this case, $u_0$ is constituted of two wells, however, if the observation is made at time $T$ large enough, only the deepest well can be identified.



  




 Video 5:  Evolution of the target and the reconstruction scheme for the initial datum at the right in Figure 7.


We end this blog post with three examples in the two-dimensional case.
Here we have the three examples of initial data that we have considered.



  




 Figure 8:  Three initial data in dimension 2.


In Video 6, we see the evolution of the reconstruction scheme for our first 2-dimensional example.
In this case, $u_0$ has a well and a bump. We observe that, as we increase the observation time $T$,
the white region, which corresponds to $\mathbb{R}^n\setminus X_T(u_T)$ becomes bigger and bigger.
This corresponds to the fact that, when the observation is made after a long time-interval, less information from the initial datum can be recovered. In addition, if we look at $u_T^\ast$ and $\tilde{u}_0$ in the two first plots, we see that the bump disappears for $T$ large enough.
This implies that after a long time-interval, the bump cannot be reconstructed.



  




 Video 6:  Evolution of the reconstruction scheme for the first initial datum in Figure 8.


In Video 7, we see the evolution of the reconstruction scheme for the second example in Figure 8.
Here, the initial datum has two bumps of different height but same shape and size in their support.
Observe that, when $T$ is large enough, both bumps look the same, and therefore we cannot guess which one was taller at $t=0$. In fact, only the shape of their support can be reconstructed.



  




 Video 7:  Evolution of the reconstruction scheme for the second initial datum in Figure 8.


In Video 8, we see the evolution of the reconstruction scheme for the third example in Figure 8.
Here, the initial datum has two wells of different depth.
We observe the same behaviour as in Video 5, where only the information of the deepest well is preserved for $T$ large enough.



  




 Video 8:  Evolution of the reconstruction scheme for the third initial datum in Figure 8.


References

[1] E. Barron, P. Cannarsa, R. Jensen, C. Sinestrari,  Regularity of Hamilton-Jacobi equations when forward is backward. Indiana University mathematics journal, pages 385-409, 1999.

[2] P. Cannarsa, C. Sinestrari,  Semiconcave functions, Hamilton-Jacobi equations, and optmial control. volume 58. Springer Science &amp;amp; Business Media, 2004.

[3] C. Esteve, E. Zuazua,  The inverse problem for Hamilton-Jacobi equations and semiconcave envelopes. Preprint &amp;lt;https://arxiv.org/abs/2003.06914

[4] P.L. Lions,  Generalized solutions of Hamilton-Jacobi equations. volume 69. London Pitman, 1982.
</description>
        <pubDate>Fri, 03 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0010</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0010</guid>
        
        
        <category>tutorial</category>
        
        <category>WP04</category>
        
      </item>
    
      <item>
        <title>Control of reaction-diffusion under state constraints - Application of the staircase method</title>
        <description>
    
    Control of reaction-diffusion
    This is a post in a collection of several on control of reaction-diffusion under state constraints
    See more ...
    


In this tutorial, we will present how to generate admissible paths of steady states for the homogeneous reaction-diffusion equation:



This a key argument for ensuring controllability that has been studied in [2,3,4,5].

Stair-case method
The stair-case method (see [1]) guarantees the following:

If we have an admissible continuous path of steady states, for any initial datum in the path, any target function in the path and for $T&amp;gt;0$ sufficiently large, there exists a function $a$ such that drives the system from the initial datum to the target fulfilling the state-constraints along the trajectory.




Figure 1. Qualitative understanding of the Stair-case method. When we have a continuous admissible path of steady-states, one can find a control that is connecting the initial steady state with the target by &quot;jumping&quot; along the continuous path.

Extension to ball, and the phase-plane analysis

We will restrict ourselves to the construction of paths that connect the steady state $w\equiv 0$ with the steady state $w\equiv \theta$. For doing such example with our model bistable equation.

In order to construct the paths we will use phase-plane techniques for which we use radial coordinates. Our original domain might not be a ball, for this reason we extend it to a ball and firstly construct the path there.




Figure 2. Extension of our domain to a ball.

Remind that the important issue is to be able to guarantee that for every domain $w\equiv0$ and $w\equiv\theta$ are connected in an admissable way and this is seen in the phase plane representation of the elliptic equation.





Now, considering the energy


where $F(u)=\int_0^u f(s)ds$, one can see that the radial ODE dissipates.

Define the following region:

Let $\theta_1$ be defined as:


Note that the region defined by 

Note that $\Gamma\subset D$.

Take $(u_0,0)\in \Gamma$, then the solution of the radial equation with initial
datum $(u_0,0)$ satisfies:



So $(u,v)\in\Gamma$ for all $r&amp;gt;0$.

We have that the blue line (the border of $\Gamma$) in the following figure determines a positively invariant region.

Then, making $a$ change continuously from $0$ to $\theta$ we generate a continuous path (by the Gromwall inequality) that is admissible since the invariant region $\Gamma$ is inside the admissible set.




Figure 3. Invariant region and construction of the path.

Animation of the path



  


Figure 4. Path of steady states.

If our domain is not a ball we restrict our path to the original domin to obtain the desired path.

Other features

General existence of admisible paths is not true due to the comparison principle. Non-trivial steady states can exist that block any possibity to control.

Here we restrict ourselves in the one dimensional case. In the following figure one can see how can we connect the steady state $w\equiv 0$ with the first nontrivial solution with boundary value $0$.




    
        
            
        
         
            
                           
    



Figure 5. Path of steady states.

However, since this path touches the boundary of the admissible set controllability cannot be guaranteed due to the comparison principle.

Another important remark to be mentioned is that if we forget about the state constraints, more paths can be generated, provided that the ODE representation of the elliptic equation does not blow up. The following figure is an example of a path connecting $w\equiv 0$ with $w\equiv 1$ that violates the constraints:



    
        
            
        
         
            
                           
    


Figure 6. Path of steady states violating constraints.

References:

[1] D. Pighin, E. Zuazua, Controllability under positivity constraints of multi-d wave equations, in:
Trends in Control Theory and Partial Differential Equations, Springer, 2019, pp. 195–232.

[2] J.-M. Coron, E. Trélat, Global steady-state controllability of one-dimensional semilinear heat equa-
tions, SIAM J. Control. Optim. 43 (2) (2004) 549–569.

[3] C. Pouchol, E. Trélat, E. Zuazua, Phase portrait control for 1d monostable and bistable reac-
635
tion–diffusion equations, Nonlinearity 32 (3) (2019) 884–909.

[4]   D.  Ruiz-Balet  and  E.  Zuazua. Controllability  under  constraints  for  reaction-diffusionequations:   The  multi-dimensional  case.   Preprint  available  athttps://cmc.deusto.eus/domenec-ruiz-balet/.

[5] D.  Ruiz-Balet  and  E.  Zuazua. Control of certain parabolic models from biology and social sciences.   Preprint  available  at https://cmc.deusto.eus/domenec-ruiz-balet/.

</description>
        <pubDate>Fri, 03 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0005</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0005</guid>
        
        
        <category>tutorial</category>
        
        <category>WP03</category>
        
      </item>
    
      <item>
        <title>The Optimal control on the Kuramoto adaptative coupling model with DyCon Toolbox</title>
        <description>In this tutorial, we present how to use Pontryagin environment to control a consensus system that models the complex emergent dynamics over a given network. The control basically minimize the cost functional which contains the running cost and desired final state.

Setup

In this tutorial we need DyCon Toolbox, to install it we will have to write the following in our MATLAB console:

unzip('https://github.com/DeustoTech/DyCon-Computational-Platform/archive/master.zip')
addpath(genpath(fullfile(cd,'DyCon-toolbox-master')))

Model

The Kuramoto model describes the phases $\theta_i$ of active oscillators, which is described by the following dynamics:



Here, the first constant terms $\omega_i$ denote the natural oscillatory behaviors, and the interactions are nonlinearly affected by the relative phases. The amplitude and connections of interactions are determined by the coupling strength, $K_{i,j}$.

Control strategy

The control interface is on the coupling strength as follows:



This is a nonlinear version of bi-linear control problem for the Kuramoto interactions. The idea is as follows;


  
    There are $N$ number of oscillators, oscillating with their own natural frequencies.
  
  
    We want to make a collective behavior using their own decision process. The interaction is given by the Kuramoto model, or may follow other interaction rules. The network can be given or flexible with control.
  
  
    The cost of control will be related to the collective dynamics we want, such as the variance of frequencies or phases.
  


Numerical simulation

Here, we consider a simple problem: we control the all-to-all network system to get gathered phases at final time $T$. We first need to define the system of ODEs in terms of symbolic variables.

clear all
m = 50;  %% [m]: number of oscillators.

import casadi.*

t     = SX.sym('t');
symTh = SX.sym('y', [m,1]);   %% [y] :  phases of oscillators, $\theta_i$.
symOm = SX.sym('om', [m,1]);  %% [om]:  natural frequencies of osc., $\omega_i$.
symK  = SX.sym('K',[m,m]);    %% [K] :  the coupling network matrix, $K_{i,j}$.
symU  = SX.sym('u',[1,1]);    %% [u] :  the control function along time, $u(t)$.


syms Vsys;    %% [Vsys]: the vector fields of ODEs.
symThth = repmat(symTh,[1 m]);


Vsys = casadi.Function('V',{symOm,symK,symTh,symU}, ...
{ symOm + (symU./m)*sum(symK.*sin(repmat(symTh,[1 m]).' - repmat(symTh,[1 m])),2) }); 


The parameter $\omega_i$ and $K_{i,j}$ should be specified for the calculations. Practically, $K_{i,j}u(t) &amp;gt; \vert \max\Omega - \min\Omega \vert$ leads to the synchronization of frequencies. We normalize the coupling strength to 1, and give random values for the natural frequencies from the normal distribution $N(0,0.1)$. We also choose initial data from $N(0,\pi/4)$.

rng('default');
rng(1,'twister');
Om_init = normrnd(0,0.2,m,1);
Om_init = Om_init - mean(Om_init);  %% Mean zero frequencies
Th_init = normrnd(0,pi/8,m,1);


K_init = ones(m,m);                 %% Constant coupling strength, 1.
T = 5;                              %% We give enough time for the frequency synchronization.


symF = casadi.Function('F',{t,symTh,symU},{Vsys(Om_init,K_init,symTh,symU)});

tspan = linspace(0,T,110);
odeEqn = ode(symF,symTh,symU,tspan);
SetIntegrator(odeEqn,'RK4')
odeEqn.InitialCondition = Th_init;


We next construct cost functional for the control problem.

PathCost  = casadi.Function('L'  ,{t,symTh,symU},{ (1/2)*(symU'*symU)           });
FinalCost = casadi.Function('Psi',{symTh}      ,{  1e5*(1/m^2)*sum(sum((symThth.' - symThth).^2))  });

iCP_1 = ocp(odeEqn,PathCost,FinalCost);


Solve Gradient descent

tic
ControlGuess = ZerosControl(odeEqn);
[OptControl_1 ,OptThetaVector_1] =  ArmijoGradient(iCP_1,ControlGuess);
toc


Length Step has been change: LenghtStep = 0.00025
iter: 001 | error: 2.215e+02 | LengthStep: 5.00e-04 | J: 1.16893e+03 | Distance2Target: NaN 
iter: 002 | error: 2.214e+02 | LengthStep: 1.00e-03 | J: 1.16780e+03 | Distance2Target: NaN 
iter: 003 | error: 2.212e+02 | LengthStep: 2.00e-03 | J: 1.16554e+03 | Distance2Target: NaN 
iter: 004 | error: 2.207e+02 | LengthStep: 4.00e-03 | J: 1.16103e+03 | Distance2Target: NaN 
iter: 005 | error: 2.198e+02 | LengthStep: 8.00e-03 | J: 1.15206e+03 | Distance2Target: NaN 
iter: 006 | error: 2.180e+02 | LengthStep: 1.60e-02 | J: 1.13430e+03 | Distance2Target: NaN 
iter: 007 | error: 2.144e+02 | LengthStep: 3.20e-02 | J: 1.09950e+03 | Distance2Target: NaN 
iter: 008 | error: 2.074e+02 | LengthStep: 6.40e-02 | J: 1.03270e+03 | Distance2Target: NaN 
iter: 009 | error: 1.938e+02 | LengthStep: 1.28e-01 | J: 9.09692e+02 | Distance2Target: NaN 
iter: 010 | error: 1.682e+02 | LengthStep: 2.56e-01 | J: 7.01968e+02 | Distance2Target: NaN 
iter: 011 | error: 1.235e+02 | LengthStep: 5.12e-01 | J: 4.10547e+02 | Distance2Target: NaN 
iter: 012 | error: 6.257e+01 | LengthStep: 1.02e+00 | J: 1.47399e+02 | Distance2Target: NaN 
Length Step has been change: LenghtStep = 0.256
iter: 013 | error: 4.113e+01 | LengthStep: 5.12e-01 | J: 1.20852e+02 | Distance2Target: NaN 
iter: 014 | error: 2.215e+01 | LengthStep: 1.02e+00 | J: 9.58348e+01 | Distance2Target: NaN 
Length Step has been change: LenghtStep = 0.256
Length Step has been change: LenghtStep = 0.064
Length Step has been change: LenghtStep = 0.016
Length Step has been change: LenghtStep = 0.004
Length Step has been change: LenghtStep = 0.001
Length Step has been change: LenghtStep = 0.00025
Length Step has been change: LenghtStep = 6.25e-05
Length Step has been change: LenghtStep = 1.5625e-05
Length Step has been change: LenghtStep = 3.9063e-06
Length Step has been change: LenghtStep = 9.7656e-07
Length Step has been change: LenghtStep = 2.4414e-07
Length Step has been change: LenghtStep = 6.1035e-08
Length Step has been change: LenghtStep = 1.5259e-08
Length Step has been change: LenghtStep = 3.8147e-09

    Mininum Length Step have been achive !! 

Elapsed time is 2.131655 seconds.



Visualization

First, we present the dynamics without control,

FreeThetaVector = solve(odeEqn,ControlGuess);
FreeThetaVector = full(FreeThetaVector);
figure
plot(FreeThetaVector')
ylabel('Phases [rad]')
xlabel('Time [sec]')
title('The dynamics without control (incoherence)')




and see the controled dynamics.

figure
plot(OptThetaVector_1')
ylabel('Phases [rad]')
xlabel('Time [sec]')
title(&quot;The dynamics under control L^2 | N_{osc}=&quot;+m)




We also can plot the control function along time.

figure
plot(OptControl_1)
legend(&quot;norm(u(t)) = &quot;+norm(OptControl_1))
ylabel('u(t)')
xlabel('Time [sec]')
title('The control function')




The problem with different regularization

In this part, we change the regularization into $L^1$-norm and see the difference.

PathCost  = casadi.Function('L'  ,{t,symTh,symU},{ sqrt(symU.^2+1e-3)});
iCP_2 = ocp(odeEqn,PathCost,FinalCost);


tic
[OptControl_2 ,OptThetaVector_2] =  ArmijoGradient(iCP_2,ControlGuess);
toc


Length Step has been change: LenghtStep = 0.00025
iter: 001 | error: 2.363e+01 | LengthStep: 5.00e-04 | J: 1.11735e+02 | Distance2Target: NaN 
iter: 002 | error: 2.363e+01 | LengthStep: 1.00e-03 | J: 1.11730e+02 | Distance2Target: NaN 
iter: 003 | error: 2.361e+01 | LengthStep: 2.00e-03 | J: 1.11720e+02 | Distance2Target: NaN 
iter: 004 | error: 2.359e+01 | LengthStep: 4.00e-03 | J: 1.11699e+02 | Distance2Target: NaN 
iter: 005 | error: 2.353e+01 | LengthStep: 8.00e-03 | J: 1.11659e+02 | Distance2Target: NaN 
iter: 006 | error: 2.343e+01 | LengthStep: 1.60e-02 | J: 1.11579e+02 | Distance2Target: NaN 
iter: 007 | error: 2.322e+01 | LengthStep: 3.20e-02 | J: 1.11420e+02 | Distance2Target: NaN 
iter: 008 | error: 2.282e+01 | LengthStep: 6.40e-02 | J: 1.11109e+02 | Distance2Target: NaN 
iter: 009 | error: 2.208e+01 | LengthStep: 1.28e-01 | J: 1.10509e+02 | Distance2Target: NaN 
iter: 010 | error: 2.081e+01 | LengthStep: 2.56e-01 | J: 1.09388e+02 | Distance2Target: NaN 
iter: 011 | error: 1.890e+01 | LengthStep: 5.12e-01 | J: 1.07391e+02 | Distance2Target: NaN 
iter: 012 | error: 1.674e+01 | LengthStep: 1.02e+00 | J: 1.04033e+02 | Distance2Target: NaN 
iter: 013 | error: 1.607e+01 | LengthStep: 2.05e+00 | J: 9.86861e+01 | Distance2Target: NaN 
iter: 014 | error: 4.081e+01 | LengthStep: 4.10e+00 | J: 9.24707e+01 | Distance2Target: NaN 
Length Step has been change: LenghtStep = 1.024
Length Step has been change: LenghtStep = 0.256
Length Step has been change: LenghtStep = 0.064
Length Step has been change: LenghtStep = 0.016
Length Step has been change: LenghtStep = 0.004
Length Step has been change: LenghtStep = 0.001
Length Step has been change: LenghtStep = 0.00025
Length Step has been change: LenghtStep = 6.25e-05
Length Step has been change: LenghtStep = 1.5625e-05
Length Step has been change: LenghtStep = 3.9063e-06
Length Step has been change: LenghtStep = 9.7656e-07
Length Step has been change: LenghtStep = 2.4414e-07
Length Step has been change: LenghtStep = 6.1035e-08
Length Step has been change: LenghtStep = 1.5259e-08
Length Step has been change: LenghtStep = 3.8147e-09

    Mininum Length Step have been achive !! 

Elapsed time is 2.494788 seconds.



figure

line(tspan,OptThetaVector_2')
ylabel('Phases [rad]')
xlabel('Time [sec]')
title(&quot;The dynamics under control L^1 | N_{osc}=&quot;+m)




figure
plot(OptControl_1)
line(1:length(OptControl_2),OptControl_2,'Color','red')

Psi_1 = norm(sin(OptThetaVector_1(:,end).' - OptThetaVector_1(:,end)),'fro');
Psi_2 = norm(sin(OptThetaVector_2(:,end).' - OptThetaVector_2(:,end)),'fro');

legend(&quot;u(t) with L^2-norm; Terminal cost = &quot;+Psi_1,&quot;u(t) with L^1-norm; Terminal cost = &quot;+Psi_2)
ylabel('The coupling strength (u(t))')
xlabel('Time [sec]')
title('The comparison between two different control cost functionals')




As one can expected from the regularization functions, the control function from $L^2$-norm acting more smoothly from $0$ to the largest value. The function from $L^2$-norm draws much stiff lines.

YFr = FreeThetaVector';
YL1 = OptThetaVector_1';
YL2 = OptThetaVector_2';
%%
animationpendulums({YFr,YL1,YL2},tspan,{'Free','L^2 Control','L^1 Control'})


Finally, we can see the behavior of the two control types against the evolution of free dynamics.



</description>
        <pubDate>Wed, 01 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp06/P0007</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp06/P0007</guid>
        
        
        <category>tutorial</category>
        
        <category>WP06</category>
        
      </item>
    
      <item>
        <title>Simultaneous Control with DyCon Toolbox</title>
        <description>In this tutorial we will present a simultaneous control problem in a linear system dependent on parameters. We will use the MATLAb DyCon Toolbox library.

Setup

In this tutorial we need DyCon Toolbox, to install it we will have to write the following in our MATLAB console:

unzip('https://github.com/DeustoTech/DyCon-Computational-Platform/archive/master.zip')
addpath(genpath(fullfile(cd,'DyCon-toolbox-master')))

Problem definition

The simultaneous control problem is defined as:



subject to:



where:



Numerical Implementation

First we import the CasAdi library in order to create symbolic variables

import casadi.*
%% In this case $\nu_i$ are
M = 50;
nu = linspace(1,6,M);

Then we create the matrices of linear dynamics.



[A,B] = GenMatSim(nu);


With these matrices we create the ode object

Nt = 500;T  = 0.8;
tspan = linspace(0,T,Nt);
%% create linear dynamic
iode = linearode(A,B,tspan);
%% set initial condition
Y0 = ones(2, 1);
iode.InitialCondition = repmat(Y0,M,1);


then we create the optimal control problem
% Get Symbolical variable
Ys  = iode.State.sym;
Us  = iode.Control.sym;
ts  = SX.sym('t'); %% &amp;lt;= Create a symbolical time
% Set Target
YT = zeros(2, 1);YT = repmat(YT,M,1);
%
PathCost  = casadi.Function('L'  ,{ts,Ys,Us},{ (1/2)*(Us'*Us)           });
FinalCost = casadi.Function('Psi',{Ys}      ,{  1e7*((Ys-YT).'*(Ys-YT)) });
% Create the optimal control
iocp = ocp(iode,PathCost,FinalCost);


Solve Optimal Control Problem with ipopt solver

U0 = ZerosControl(iode);
[Uopt ,Yopt] =  IpoptSolver(iocp,U0);


This is Ipopt version 3.12.3, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation).

Number of nonzeros in equality constraint Jacobian...:   199700
Number of nonzeros in inequality constraint Jacobian.:        0
Number of nonzeros in Lagrangian Hessian.............:      600

Total number of variables............................:    50500
                     variables with only lower bounds:        0
                variables with lower and upper bounds:        0
                     variables with only upper bounds:        0
Total number of equality constraints.................:    50000
Total number of inequality constraints...............:        0
        inequality constraints with only lower bounds:        0
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0

iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
   0  1.0000000e+09 1.12e-02 7.88e+00  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0
   1  4.0934345e+06 1.26e-13 5.63e-09  -1.0 1.66e+02    -  1.00e+00 1.00e+00f  1

Number of Iterations....: 1

                                   (scaled)                 (unscaled)
Objective...............:   2.0467172272726700e+01    4.0934344545453396e+06
Dual infeasibility......:   5.6305735629536002e-09    1.1261147125907198e-03
Constraint violation....:   1.2612133559741778e-13    1.2612133559741778e-13
Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00
Overall NLP error.......:   5.6305735629536002e-09    1.1261147125907198e-03


Number of objective function evaluations             = 2
Number of objective gradient evaluations             = 2
Number of equality constraint evaluations            = 2
Number of inequality constraint evaluations          = 0
Number of equality constraint Jacobian evaluations   = 2
Number of inequality constraint Jacobian evaluations = 0
Number of Lagrangian Hessian evaluations             = 1
Total CPU secs in IPOPT (w/o function evaluations)   =      1.391
Total CPU secs in NLP function evaluations           =      0.027

EXIT: Optimal Solution Found.
               t_proc [s]   t_wall [s]    n_eval
       nlp_f     0.000381      0.00038         2
       nlp_g       0.0042       0.0042         2
  nlp_grad_f      0.00167      0.00167         3
  nlp_hess_l      0.00331      0.00332         1
   nlp_jac_g        0.026        0.026         3
      solver         1.48         1.36         1
Elapsed time is 3.853462 seconds.



Compute Free solution

Yfree = solve(iode,U0*0);
Yfree = full(Yfree);


Visualization

fig = figure;
fig.Units = 'norm';fig.Position = [0.1 0.1 0.6 0.5];
%%
plotSimu(tspan,Yfree,Yopt,Uopt,M)




Figure 1. The different colors represent the dynamic system under different parameters. It can be seen how the same control is obtained acting for all the dynamic systems is capable of driving the systems to the target.
</description>
        <pubDate>Wed, 01 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0009</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0009</guid>
        
        
        <category>tutorial</category>
        
        <category>WP01</category>
        
      </item>
    
      <item>
        <title>Control of reaction-diffusion under state constraints - Barriers</title>
        <description>
    
    Control of reaction-diffusion
    This is a post in a collection of several on control of reaction-diffusion under state constraints
    See more ...
    


This tutorial is focused on the understanding of of the problem



The solutions for $a=0$ constitute an intrinsic obstruction to the controllability with state-constraints due to the comparison principle [1]. Indeed, for any control function $0\leq a(x,t)\leq 1$
the solution of the problem:



where $u$ is an elliptic nontrivial solution satisfies that $v(x,t)\geq u(x)$. Therefore, we cannot expect to control to any function below $u$.

This is why the understanding of the existence or non-existence of nontrivial solutions is of main importance for the controllability under state-constraints.

Non-trivial solutions around the boundary value $a=\theta$ do not constitute an intrinsic obstruction to the controllability because the boundary value $\theta$ is not in the border of the admissible set. However, the existence of such solutions create a technical difficulty for achieving the controllability to the steady state $w\equiv \theta$ which can be solved by constructing paths of steady states (See this blog entry ).

Elliptic nontrivial solutions with boundary value 1 do not exist. This can be related to the fact that the traveling waves for the Cauchy problem in the real line are approaching the steady state $w\equiv 1$ [2]

The existence of non-trivial solutions can be done either by comparison principles or by understanding the critical points of a functional (see [3]).

Any critical point of the functional:





is a weak solution of the main equation.

In order to guarantee that the solution will be between $0$ and $1$, one can extend $f$ by $0$ outside of the interval $[0,1]$.

Restricting ourselves in the one dimensional case (the argument also holds in several dimensions) and making the spatial change fo variables $x\to \frac{x}{L}$, where $L$ is the length of the domain the functional reads:



In the expression above we see that if $L$ is small the convex part dominates while if $L$ is large and $\int_0^1f(s)ds&amp;gt;0$ it might not be convex.

In the videos below one can see the evolution of the functional depending on the parameter $\lambda=L^2$. The representation is the evaluation of the functional along the first and the third eigenfunction, i.e. $e_1=\sin(\pi x)$ and $e_3=\sin(3\pi x)$,



Here, one can see the evolution of the functional for $a=0$ and for $a=\theta=1/3$ for values of $\alpha$ and $\beta$.


    
        
            
            
                
        
         
            
            
            
               
    

Figure 1. Animation of the functional $J$ for diferent values of $\lambda=L^2$. At the left the boundary value is $0$ while at the right the boundary value is $\theta$.

In the next figure a qualitative bifurcation diagram is represented, the red curve represents the nontrivial solutions with boundary value $\theta$ and the blue one the boundary value $0$. $\lambda^\ast$ is the minimum $\lambda=L^2$ for which a nontrivial solution around $0$ exists, analogously $\lambda^*_\theta$.




Figure 2. Bifurcation diagram . The blue line represents the maximum of the nontrivial solutions with boundary value $0$. The red curve represents some nontrivial solutions with boundary value $\theta$, the red line is the maximum of the nontrivial solution whenever the line is above $\theta$ and represents the minimum when the red line is below $\theta$.

The value $\frac{\lambda_1}{f’(\theta)}$ is the critical value for which the stationary solution $\theta$ becomes unstable. After this situation we have a nontrivial solution above and below $\theta$.

There will be further bifurcations around the boundary value $\theta$ when we increase $\lambda$. These bifuractions will lead to oscillatory nontrivial solutions that can be well understood in the phase-plane, see [2].

Here one can see different non-trivial solutions for different boundary values. The green curve corresponds to a section of the nontrivial solution in the whole $\mathbb{R}$:






Figure 3. Several nontrivial solutions, in blue the ones with boundary value $0$, the red ones have boundary value $\theta$ and the green one is the solution in the whole $\mathbb{R}$. $\theta_1$ is the number such that $\int_0^{\theta_1} f(s)ds = 0 $.

References:

[1] M.H. Protter and H.F. Weinberger, Maximum principles in differential equations, Springer Science &amp;amp; Business Media, 2012.

[2] D.  Ruiz-Balet  and  E.  Zuazua. Control of certain parabolic models from biology and social sciences.   Preprint  available  at https://cmc.deusto.eus/domenec-ruiz-balet/.

[3]  P.L. Lions, On the existence of positive solutions of semilinear elliptic equations, SIAM Rev. 24 (1982), no. 4, 441–467.

</description>
        <pubDate>Tue, 31 Mar 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0006</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0006</guid>
        
        
        <category>tutorial</category>
        
        <category>WP03</category>
        
      </item>
    
      <item>
        <title>Control of reaction-diffusion under state constraints</title>
        <description>Reaction-diffusion equations appear frequently in natural phenomena such as:

  Population dynamics and invasion of species (see [1]).
  Neuroscience, where models for neuronal impulses exhibit traveling waves (see [2]).
  Chemical Reactions: modeling the evolution of concentrations of chemicals (see [3]).
  In evolutionary game theory  (see [4, 5]).
  Magnetic systems in material science and their phase transitions (see [6]).
  Linguistics, we refer to [7] where the authors consider reaction-diffusion for analyzing language shift by means of a traveling wave.


In the majority of the systems mentioned above, the state $u$ of an equation of the type



represents a population, concentration or proportion. For this reason, any model intending to predict the behavior of such quantity must fulfill a maximum principle [8].

In this group of tutorials, our aim is to explain the phenomenology arising when considering a control problem in the contexts mentioned above.

In general, the controllability of parabolic equations has been widely studied (for instance [9,10,11]). However, in this literature, the requirement that the trajectory has to be positive or between prescribed bounds was not a concern.

In Figure 1 one can see a boundary control of the semilinear equation



with $a$ being the control and the target function $v\equiv 0.33$.



    
        
            
        
         
            
               
    



Control from $u(0)\equiv 0$ to $u(T)\equiv 0.33$.

This model models the evolution of a proportion, and we observe that the control does not preserve the positivity of the state neither the meaningful upper bound.

From the application point of view of several of the applications mentioned, any control action proposed must fulfill that the associated trajectory has meaning.

The ideas exposed in this tutorial group are the following:


  
    
    
    
        The presence of state-constraints can create intrinsic obstructions for achieving the controllability. This is due to the emergence on non-trivial solutions and the comparison principle. The topic is treated in this blog.
        
  
  
    
  
  
  
    
       Control of reaction-diffusion under state constraints - Barriers 
    

    
      This tutorial is part of the control under state constraints. We will show how obstructions to the state constraint controllability can appear.
    

    
        
          Author:
          
          
            
              Domenec Ruiz
          
          - 31 March 2020
        
        
        MATLAB-CasADi
    
    

  


    
        In order to prove the existence of controls one can use the stair-case method. This method relies on the construction of paths of steady-states that the controlled trajectory can follow. One can find more information about these constructions in this blog.
        
  
  
    
  
  
  
    
       Control of reaction-diffusion under state constraints - Application of the staircase method 
    

    
      In this tutorial, we will present how to generate admissible paths of steady states for the homogeneous reaction-diffusion equation
    

    
        
          Author:
          
          
            
              Domenec Ruiz
          
          - 03 April 2020
        
        
        MATLAB-CasADi
    
    

  


    
        In contrast to the unconstrained case, the presence of constraints induces a minimal controllability time. However, the construction done with the path of steady-states is only a way to control and it requires typically a large time, much larger than the minimal one.  In this blog post, we explore different ways to control the system, a minimal controllability time control, a quasistatic control and we explore numerically the effect of the presence of barriers in an optimal control framework.
        
  
  
    
  
  
  
    
       Control of reaction-diffusion under state constraints - Numerical exploration of controls 
    

    
      This tutorial is part of the control under state constraints. We will simulate different control strategies to the same target by minimizing different functionals.
    

    
        
          Author:
          
          
            
              Domenec Ruiz
          
          - 04 April 2020
        
        
        MATLAB-CasADi
    
    

  


    
        Many models in population dynamics take care about spatial heterogeneity, this can lead for example to new types obstructions. In the blog, we explain the main features and the influence of an heterogeneous drift in a bistable equation called gene-flow. The heterogeneity in the drift comes from an approximation of a system of a system of two equations with an heterogeneous environment.
        
  
  
    
  
  
  
    
       Control of reaction-diffusion under state constraints - Heterogeneous setting: Gene-flow. 
    

    
      This tutorial is part of the control under state constraints. We will present the main features regarding the controllability of bistable reaction-diffusion equations with heterogeneous drifts.
    

    
        
          Authors:
          
          
            
              Domenec Ruiz,
          
            
              Idriss Mazari
          
          - 19 April 2020
        
        
        MATLAB-CasADi
    
    

  


    
    
  


References:

[1] A. Kolmogorov, Étude de l’équation de la diffusion avec croissance de la quantité de matière et son
application à un problème biologique, Bull. Univ. Moskow, Ser. Internat., Sec. A 1 (1937) 1–25.

[2] J. Evans, Nerve axon equations 4: the stable and unstable impulse, Indiana Univ. Math. J. 24 (12)
(1975) 1169–1190.

[3] B. Perthame, Parabolic equations in biology : growth, reaction, movement and diffusion, Lecture
notes on mathematical modelling in the life sciences, 2015.

[4] Travelling waves for games in economics and biology, Nonlinear Analysis: Theory, Methods and
Applications 30 (2) (1997) 1235 – 1244, proceedings of the Second World Congress of Nonlinear
Analysts.

[5] V. Hutson, K. Mischaikow, G. T. Vickers, Multiple travelling waves in evolutionary game dynamics,
Japan Journal of Industrial and Applied Mathematics 17 (3) (2000) 341.

[6] A. De Masi, P. Ferrari, J. Lebowitz, Reaction diffusion equations for interacting particle systems, J.
Stat. Phys. 44 (3-4) (1986) 589–644.

[7] K. Prochazka, G. Vogl, Quantifying the driving factors for language shift in a bilingual region 114 (17) (2017) 4365–4369.

[8] M.H. Protter and H.F. Weinberger, Maximum principles in differential equations, Springer Science &amp;amp; Business Media, 2012.

[9] G. Lebeau, L. Robbiano, Contrôle exact de léquation de la chaleur, Communications in Partial
Differential Equations 20 (1-2) (1995) 335–356.

[10] E. Fernández-Cara and E. Zuazua, Null and approximate controllability for weakly blowing up semilinear heat equations, Ann. Inst. H. Poincaré Anal. Non Linéaire 17 (2000), no. 5, 583 – 616.

[11] A.V. Fursikov and O.Y. Imanuvilov, Controllability of evolution equations, no. 34, Seoul National University, 1996.

[12] D.  Ruiz-Balet  and  E.  Zuazua. Control of certain parabolic models from biology and social sciences.   Preprint  available  at https://cmc.deusto.eus/domenec-ruiz-balet/.

[13]  P.L. Lions, On the existence of positive solutions of semilinear elliptic equations, SIAM Rev. 24 (1982), no. 4, 441–467.

[14] D. Pighin, E. Zuazua, Controllability under positivity constraints of multi-d wave equations, in:
Trends in Control Theory and Partial Differential Equations, Springer, 2019, pp. 195–232.

[15] J.-M. Coron, E. Trélat, Global steady-state controllability of one-dimensional semilinear heat equa-
tions, SIAM J. Control. Optim. 43 (2) (2004) 549–569.

[16] C. Pouchol, E. Trélat, E. Zuazua, Phase portrait control for 1d monostable and bistable reac-
635 tion–diffusion equations, Nonlinearity 32 (3) (2019) 884–909.

[17]   D.  Ruiz-Balet  and  E.  Zuazua. Controllability  under  constraints  for  reaction-diffusionequations:   The  multi-dimensional  case (2019).   Preprint  available  athttps://cmc.deusto.eus/domenec-ruiz-balet/.

[18] I. Mazari, D. Ruiz-Balet, and E. Zuazua, Constrained control of bistable reaction-diffusion equations: Gene-flow and spatially heterogeneous models, preprint: https://hal.archives-ouvertes.fr/hal-02373668/document (2019).

</description>
        <pubDate>Mon, 30 Mar 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0007</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0007</guid>
        
        
        <category>tutorial</category>
        
        <category>WP03</category>
        
      </item>
    
      <item>
        <title>Control for a semilinear heat equation and analogies with a collective behavior model</title>
        <description>In this tutorial we will apply the DyCon toolbox to find a control to the semi-discrete semi-linear heat equation.



where $N^2A$ is the discretization of the Laplacian in 1d in $N$ nodes. We are looking for a control that after time $T$ steers the system near zero.

Setup

In this tutorial we need DyCon Toolbox, to install it we will have to write the following in our MATLAB console:

unzip('https://github.com/DeustoTech/DyCon-Computational-Platform/archive/master.zip')
addpath(genpath(fullfile(cd,'DyCon-toolbox-master')))

Methodology
In order to do so we will frame the problem as a minimization of a functional and we will apply gradient descent to find it. Note that the convexity of the functional is not proven, therefore, we will obtain a local minima for the functional. The functional considered will be:



where by $| \cdot |_{L^2}$ we understand the discrete $L^2$ norm.

Once this control is computed for a certain N, we will think on a dynamical system that models an opinion dynamics with $N$ agents communicating through a chain.

\begin{equation}\label{m1}y_t-\frac{1}{N}Ay=G(y)+Bv\end{equation}

The goal will be to compute also the control $v$ thinking model \eqref{m1} as if it was a semidiscretization of a heat equation with diffusivity $\frac{1}{N^3}$. Furthermore we will also compute the control for model \eqref{m1} with the non-linearity being non-homogeneous on $N$ and a time horizon being $T_N=N^3T$.

clear all
import casadi.*


Definition of the time

ts = SX.sym('t');
%% Discretization of the space
N = 30;
xi = 0; xf = 1;
xline = linspace(xi,xf,N);


Interior Control region between 0.5 and 0.8

w1 = 0.5; w2 = 0.8;
D = 1;


[A,B,count] = GetABmatrix(N,w1,w2,D);


we define symbolically the vectors of the state and the control

Ys = SX.sym('y',[N 1]);
Us = SX.sym('u',[count 1]);


We create the ODE object Our ODE object will have the semi-discretization of the semilinear heat equation. We set also initial conditions, define the non linearity and the interaction of the control to the dynamics.

Initial condition

Y0 = 2*sin(pi*xline');


Diffusion part: the discretization of the 1d Laplacian

Definition of the non-linearity 

G = casadi.Function('NLT',{Ys},{10*Ys.*exp(-Ys.^2)});%%


and we define the part of the dynamics corresponding to the nonlinearity

Putting all the things together

F = casadi.Function('F',{ts,Ys,Us},{A*Ys + B*Us + G(Ys)});
%% Creation of the ODE object
%% Time horizon
T = 1;


We create the ODE-object and we change the resolution to $dt=0.01$ in order to see the variation in a small time scale. We will get the values of the solution in steps of size odeEqn.dt, if we do not care about modifying this parameter in the object, we might get the solution in certain time steps that will hide part of the dynamics.

Nt = 100;
tspan = linspace(0,T,Nt);
ipde = semilinearpde1d(Ys,Us,A,B,G,tspan,xline);
ipde.InitialCondition = Y0;


We solve the equation and we plot the free solution applying solve to odeEqn and we plot the free solution.

Yfree = solve(ipde,ZerosControl(ipde));
Yfree = full(Yfree);


figure;
surf(Yfree','EdgeColor','none');
title('Free Dynamics')
xlabel('space discretization')
ylabel('Time')
yticks([1 Nt])
yticklabels([tspan(1) tspan(end)])




We create the object that collects the formulation of an optimal control problem  by means of the object that describes the dynamics odeEqn, the functional to minimize Jfun and the time horizon T

L   = Function('L'  ,{ts,Ys,Us},{ Us.'*Us  });
Psi = Function('Psi',{Ys}      ,{ 1e6*(Ys.'*Ys) });

iocp = ocp(ipde,L,Psi);


We apply the steepest descent method to obtain a local minimum (our functional might not be convex).

U0 =ZerosControl(ipde);
[OptControl ,OptState]  = IpoptSolver(iocp,U0);


This is Ipopt version 3.12.3, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation).

Number of nonzeros in equality constraint Jacobian...:    19236
Number of nonzeros in inequality constraint Jacobian.:        0
Number of nonzeros in Lagrangian Hessian.............:     3900

Total number of variables............................:     3900
                     variables with only lower bounds:        0
                variables with lower and upper bounds:        0
                     variables with only upper bounds:        0
Total number of equality constraints.................:     3000
Total number of inequality constraints...............:        0
        inequality constraints with only lower bounds:        0
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0

iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
   0  3.0000000e+07 9.05e+00 9.67e-01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0
   1  2.4544321e+05 1.22e-01 7.92e+00  -1.0 3.10e+01  -2.0 1.00e+00 1.00e+00f  1
   2  1.8219937e+06 4.24e-03 1.73e+00  -1.0 1.31e+01  -1.6 1.00e+00 1.00e+00h  1
   3  7.1220860e+04 1.67e-02 2.92e+00  -1.0 9.12e+00  -2.1 1.00e+00 1.00e+00f  1
   4  8.9356500e+03 3.97e-04 8.51e-02  -1.0 2.41e+00  -2.5 1.00e+00 1.00e+00f  1
   5  3.1075917e+03 1.06e-04 3.19e-03  -2.5 2.96e+00  -3.0 1.00e+00 1.00e+00f  1
   6  1.3016546e+03 5.20e-05 1.19e-03  -3.8 3.60e+00  -3.5 1.00e+00 1.00e+00h  1
   7  4.5871538e+02 5.64e-05 4.84e-04  -5.7 4.41e+00  -4.0 1.00e+00 1.00e+00h  1
   8  1.6346578e+02 6.55e-05 1.92e-04  -5.7 5.26e+00  -4.4 1.00e+00 1.00e+00h  1
   9  9.7049320e+01 1.20e-04 3.78e-05  -5.7 3.10e+00  -4.9 1.00e+00 1.00e+00h  1
iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
  10  4.0069187e+01 9.26e-03 1.98e-05  -5.7 1.48e+01    -  1.00e+00 5.00e-01h  2
  11  1.6748754e+01 4.28e-04 2.92e-06  -5.7 4.34e+00    -  1.00e+00 1.00e+00h  1
  12  1.6371256e+01 4.58e-06 9.87e-09  -5.7 1.88e-01    -  1.00e+00 1.00e+00h  1
  13  1.6367177e+01 1.03e-11 1.11e-13  -8.6 7.84e-04    -  1.00e+00 1.00e+00h  1

Number of Iterations....: 13

                                   (scaled)                 (unscaled)
Objective...............:   8.1835886066017175e-04    1.6367177213203433e+01
Dual infeasibility......:   1.1096830919432588e-13    2.2193661838865175e-09
Constraint violation....:   1.0345974077452524e-11    1.0345974077452524e-11
Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00
Overall NLP error.......:   1.0345974077452524e-11    2.2193661838865175e-09


Number of objective function evaluations             = 19
Number of objective gradient evaluations             = 14
Number of equality constraint evaluations            = 19
Number of inequality constraint evaluations          = 0
Number of equality constraint Jacobian evaluations   = 14
Number of inequality constraint Jacobian evaluations = 0
Number of Lagrangian Hessian evaluations             = 13
Total CPU secs in IPOPT (w/o function evaluations)   =      0.436
Total CPU secs in NLP function evaluations           =      0.057

EXIT: Optimal Solution Found.
               t_proc [s]   t_wall [s]    n_eval
       nlp_f     0.000826     0.000828        19
       nlp_g      0.00916      0.00917        19
  nlp_grad_f      0.00139      0.00139        15
  nlp_hess_l       0.0175       0.0174        13
   nlp_jac_g       0.0333       0.0333        15
      solver        0.507        0.501         1
Elapsed time is 0.636870 seconds.



plotSHE(OptState,OptControl,Yfree,YT,xline,tspan,Nt)



    
        
            
        
         
            
               
    


Collective behavior dynamics

Now we apply the same procedure for the collective behavior dynamics. We will employ a function that does the algorithm explained before for the semilinear heat equation having the chance to set a diffusivity constant.

For the simulation of the model in collective behavior we will employ a diffusivity $D=\frac{1}{N^3}$.

D = 1/(N^3);
[A,B,count] = GetABmatrix(N,w1,w2,D);
%%
G = casadi.Function('NLT',{Ys},{10*Ys.*exp(-Ys.^2)});%%
%%
T = 1;  Nt = 100;
tspan = linspace(0,T,Nt);
%%
ipde = semilinearpde1d(Ys,Us,A,B,G,tspan,xline);
ipde.InitialCondition = Y0;
%%
iocp = ocp(ipde,L,Psi);



    
        
            
        
         
            
               
    


U0 =ZerosControl(ipde);
%%[OptControl ,OptState]  = ArmijoGradient(iocp,U0,'MaxIter',200);
[OptControl ,OptState]  = IpoptSolver(iocp,U0);


This is Ipopt version 3.12.3, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation).

Number of nonzeros in equality constraint Jacobian...:    19236
Number of nonzeros in inequality constraint Jacobian.:        0
Number of nonzeros in Lagrangian Hessian.............:     3900

Total number of variables............................:     3900
                     variables with only lower bounds:        0
                variables with lower and upper bounds:        0
                     variables with only upper bounds:        0
Total number of equality constraints.................:     3000
Total number of inequality constraints...............:        0
        inequality constraints with only lower bounds:        0
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0

iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
   0  3.0000000e+07 1.00e+00 9.72e-01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0
   1  1.1211823e+08 6.93e-02 1.07e+01  -1.7 1.75e+00   0.0 1.00e+00 1.00e+00h  1
   2  1.0201471e+08 2.53e-02 2.23e+00  -1.7 3.50e+00  -0.5 1.00e+00 1.00e+00f  1
   3  8.8642262e+07 4.18e-02 5.31e-01  -1.7 2.13e+02    -  1.00e+00 1.00e+00f  1
   4  8.2505158e+07 1.39e+00 3.13e+00  -1.7 2.72e+03    -  1.00e+00 1.00e+00F  1
   5  7.9212105e+07 9.68e+00 5.20e+00  -1.7 1.38e+03    -  1.00e+00 1.00e+00f  1
   6  7.8291279e+07 6.94e-02 2.84e+00  -1.7 2.05e+03    -  1.00e+00 1.00e+00f  1
   7  7.8090072e+07 1.57e+00 4.98e-01  -1.7 7.87e+02    -  1.00e+00 1.00e+00f  1
   8  7.8026943e+07 1.54e-01 8.25e-02  -1.7 3.33e+02    -  1.00e+00 1.00e+00f  1
   9  7.8491521e+07 4.41e-02 7.64e+00  -2.5 2.10e+00  -1.0 1.00e+00 1.00e+00h  1
iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
...
...
  54  7.8022238e+07 4.03e-07 8.33e-10  -5.7 2.15e-02    -  1.00e+00 1.00e+00h  1
  55  7.8022238e+07 9.83e-12 1.82e-12  -8.6 1.02e-04    -  1.00e+00 1.00e+00h  1

Number of Iterations....: 55

                                   (scaled)                 (unscaled)
Objective...............:   3.9011119130960064e+03    7.8022238261920124e+07
Dual infeasibility......:   1.8189894035458565e-12    3.6379788070917130e-08
Constraint violation....:   9.8296371042749797e-12    9.8296371042749797e-12
Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00
Overall NLP error.......:   9.8296371042749797e-12    3.6379788070917130e-08


Number of objective function evaluations             = 111
Number of objective gradient evaluations             = 56
Number of equality constraint evaluations            = 111
Number of inequality constraint evaluations          = 0
Number of equality constraint Jacobian evaluations   = 56
Number of inequality constraint Jacobian evaluations = 0
Number of Lagrangian Hessian evaluations             = 55
Total CPU secs in IPOPT (w/o function evaluations)   =      1.374
Total CPU secs in NLP function evaluations           =      0.308

EXIT: Optimal Solution Found.
               t_proc [s]   t_wall [s]    n_eval
       nlp_f      0.00624      0.00627       111
       nlp_g       0.0634       0.0633       111
  nlp_grad_f      0.00762      0.00768        57
  nlp_hess_l       0.0868       0.0846        55
   nlp_jac_g        0.149        0.149        57
      solver         1.74         1.73         1
Elapsed time is 1.874814 seconds.



plotSHE(OptState,OptControl,Yfree,YT,xline,tspan,Nt)



    
        
            
        
         
            
               
    


Now we will change also the time horizon and we will incorporate a non-homogeneous non-linearity, we will just divide the non-linearity $G$ by $N^3$

D = 1/(N^2);
[A,B,count] = GetABmatrix(N,w1,w2,D);
%%
G = casadi.Function('NLT',{Ys},{10*Ys.*exp(-Ys.^2)/N^2} );%%
%%
T = N^2;  Nt = 100;
tspan = linspace(0,T,Nt);
%%
ipde = semilinearpde1d(Ys,Us,A,B,G,tspan,xline);
ipde.InitialCondition = Y0;
%%
iocp = ocp(ipde,L,Psi);


U0 =ZerosControl(ipde);
[OptControl ,OptState]  = IpoptSolver(iocp,U0);


This is Ipopt version 3.12.3, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation).

Number of nonzeros in equality constraint Jacobian...:    19236
Number of nonzeros in inequality constraint Jacobian.:        0
Number of nonzeros in Lagrangian Hessian.............:     3900

Total number of variables............................:     3900
                     variables with only lower bounds:        0
                variables with lower and upper bounds:        0
                     variables with only upper bounds:        0
Total number of equality constraints.................:     3000
Total number of inequality constraints...............:        0
        inequality constraints with only lower bounds:        0
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0

iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
   0  3.0000000e+07 9.05e+00 1.17e+02  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0
   1  2.8754020e+04 1.48e-01 4.01e+00  -1.0 2.73e+00   0.0 1.00e+00 1.00e+00f  1
   2  3.0839145e+03 1.91e-02 3.08e-01  -1.0 9.19e-01  -0.5 1.00e+00 1.00e+00h  1
   3  9.2703794e+02 1.17e-03 2.31e-02  -1.7 2.08e-01  -1.0 1.00e+00 1.00e+00h  1
   4  4.2707593e+02 2.49e-03 9.65e-03  -3.8 2.61e-01  -1.4 1.00e+00 1.00e+00h  1
   5  2.2851845e+02 1.36e-03 2.60e-03  -3.8 2.10e-01  -1.9 1.00e+00 1.00e+00h  1
   6  1.5158663e+02 1.25e-03 9.80e-04  -3.8 2.38e-01  -2.4 1.00e+00 1.00e+00h  1
   7  1.0229631e+02 2.37e-03 4.26e-04  -5.7 3.11e-01  -2.9 1.00e+00 1.00e+00h  1
   8  8.6220145e+01 1.84e-01 3.73e-04  -5.7 2.84e+01    -  1.00e+00 1.25e-01h  4
   9  1.6663658e+00 6.98e-02 4.71e-05  -5.7 2.20e+00    -  1.00e+00 1.00e+00h  1
iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
  10  1.1303218e-01 1.61e-02 9.28e-07  -5.7 4.53e-01    -  1.00e+00 1.00e+00h  1
  11  3.7959122e-02 7.79e-03 2.54e-08  -5.7 2.84e-01    -  1.00e+00 1.00e+00h  1
  12  2.1316195e-02 1.77e-03 4.98e-09  -5.7 1.39e-01    -  1.00e+00 1.00e+00h  1
  13  1.8447375e-02 6.30e-06 5.26e-11  -5.7 8.15e-03    -  1.00e+00 1.00e+00h  1
  14  1.8440851e-02 2.26e-09 9.54e-15  -8.6 1.61e-04    -  1.00e+00 1.00e+00h  1

Number of Iterations....: 14

                                   (scaled)                 (unscaled)
Objective...............:   9.2204254694796125e-07    1.8440850938959225e-02
Dual infeasibility......:   9.5381182640930876e-15    1.9076236528186175e-10
Constraint violation....:   2.2559639156760625e-09    2.2559639156760625e-09
Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00
Overall NLP error.......:   2.2559639156760625e-09    2.2559639156760625e-09


Number of objective function evaluations             = 19
Number of objective gradient evaluations             = 15
Number of equality constraint evaluations            = 19
Number of inequality constraint evaluations          = 0
Number of equality constraint Jacobian evaluations   = 15
Number of inequality constraint Jacobian evaluations = 0
Number of Lagrangian Hessian evaluations             = 14
Total CPU secs in IPOPT (w/o function evaluations)   =      0.415
Total CPU secs in NLP function evaluations           =      0.065

EXIT: Optimal Solution Found.
               t_proc [s]   t_wall [s]    n_eval
       nlp_f     0.000898     0.000903        19
       nlp_g      0.00904      0.00905        19
  nlp_grad_f       0.0017      0.00169        16
  nlp_hess_l       0.0214       0.0214        14
   nlp_jac_g       0.0354        0.035        16
      solver        0.489        0.487         1
Elapsed time is 0.616536 seconds.



plotSHE(OptState,OptControl,Yfree,YT,xline,tspan,Nt)



    
        
            
        
         
            
               
    

</description>
        <pubDate>Sun, 15 Mar 2020 00:00:00 +0100</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp06/P0008</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp06/P0008</guid>
        
        
        <category>tutorial</category>
        
        <category>WP06</category>
        
      </item>
    
      <item>
        <title>POD and DMD Reduced Order Models for a 2D Burgers Equation</title>
        <description>Burgers equation

For a given viscosity parameter $\nu$ and for time $t&amp;gt;0$, we consider the 2D Burgers equation on the unit square



with zero Neumann boundary conditions and initial condition



and its numerical approximation using


  Finite Elements in space and Runge-Kutta in time,
  a POD reduction of the FEM model, and
  a DMD reduced order model.


We present the basic ideas and how they are realized in a basic Python script.

0. The Setup

Please download and run the python file burgers.py to check the
implementation, to reproduce the presented results, and to the behavior of the
routines for different parameters.

Here is the start of the script that lists the modules on which the
implementation bases on.

import dolfin

import scipy.linalg as spla
import matplotlib.pyplot as plt
import numpy as np

from scipy.integrate import solve_ivp
from spacetime_galerkin_pod.ldfnp_ext_cholmod import SparseFactorMassmat


The code uses dolfin which is the python interface to
FEniCS while the other modules scipy, numpy,
and matplotlib are standard in python, I would say. The module
ldfnp_ext_cholmod is a little wrapper for the sparsity optimizing Cholesky
decomposition of sksparse.
It is available from my github
repository and falls back
to numpy routines, in the case that sksparse is not available.

1. The FEM Discretization

The spatial discretization in FEniCS

First, the mesh is defined – here a uniform triangulation of the unit square
controlled by the parameter N. For the value N=80 and for globally smooth
and piecewise quadratic ansatz functions, the resulting dimension of the system
is 51842.

# The mesh

pone = dolfin.Point(-1, -1)
ptwo = dolfin.Point(1, 1)
mesh = dolfin.RectangleMesh(pone, ptwo, N, N)

V = dolfin.VectorFunctionSpace(mesh, 'CG', 2)

### The FENICS FEM Discretization

v = dolfin.TestFunction(V)
u = dolfin.TrialFunction(V)


Next, the discrete linear operators are defined and exported as a SciPy sparse
matrix. Also, we factorize the mass matrix mmat for later use and define the
norm that is weighted with the mass matrix as this is the discrete $L^2$ norm.

# ## the mass matrix

mform = dolfin.inner(v, u)*dolfin.dx
massm = dolfin.assemble(mform)
mmat = dolfin.as_backend_type(massm).sparray()
mmat.eliminate_zeros()

# factorize it for later


mfac = SparseFactorMassmat(mmat)

# norm induced by the mass matrix == discrete L2-norm

def mnorm(uvec):
    return np.sqrt(np.inner(uvec, mmat.dot(uvec)))

# ## the stiffness matrix
# as a form in FEniCS

aform = nu*dolfin.inner(dolfin.grad(v), dolfin.grad(u))*dolfin.dx
aassm = dolfin.assemble(aform)

# as a sparse matrix

amat = dolfin.as_backend_type(aassm).sparray()
amat.eliminate_zeros()


Then we define the convective term as function of the velocity u both in terms
of an FE-function and as a function of the associated coefficient vector.

# ## the convective term

# as a function in FEniCS

def burgers_nonl_func(ufun):
    cform = dolfin.inner(dolfin.grad(ufun)*ufun, v)*dolfin.dx
    cass = dolfin.assemble(cform)
    return cass

# as a vector to form map

def burgers_nonl_vec(uvec):
    ufun = dolfin.Function(V)
    ufun.vector().set_local(uvec)
    bnlform = burgers_nonl_func(ufun)
    bnlvec = bnlform.get_local()
    return bnlvec


The Runge-Kutta Time Integration

To apply standard time integration schemes (here, RK23 turned out to be most
efficient), we define the right hand side $f_h$ of the spatially discretized problem



In this case the right hand side is an application of the discrete diffusion
and convection operator and the inverse of the mass matrix that, simply
speaking, maps a (discrete) form onto a discrete function. Note that there is no
explicit time dependency in the Burgers equation, but SciPy’s solve_ivp
requires this parameter. Also, we define the initial value here. The time grid
is used to store the solution for the snapshots needed later, but the time
integrator uses his internal time grid.

inivstrg = 'exp(-3.*(x[0]*x[0]+x[1]*x[1]))'
inivexpr = dolfin.Expression((inivstrg, inivstrg), degree=2)
inivfunc = dolfin.interpolate(inivexpr, V)
inivvec = inivfunc.vector().get_local()

burgsol = solve_ivp(brhs, (t0, tE), inivvec, t_eval=timegrid, method='RK23')
fullsol = burgsol.y


Here is the result. Note the sharp front that develops towards the end of the time
integration.


Solution snapshots of the full FEM model 

2. POD Reduced Model

If one has snapshots of the solution $v _ h$ at some time instances $t _ i$ one
may well think that the span of the matrix of snapshots



is a good candidate for a space in which the solution evolves in. One may even
go further and look for a low-dimensional basis of this space. The span of a
matrix is best approximated by its dominant singular vectors. And this is the idea of
Proper Orthogonal Decomposition (POD) – use the leading singular vectors as a
basis for the solution space.

We use the Nts=101 snapshots of the FEM solutions to setup the matrix of
measurements $X$ and to compute the POD modes as $M^{-1/2}v _ k$, where $v _ k$
is the $k$-th leading left singular vector of $M^{1/2}X$. This procedure gives a
low-dimensional orthogonal (in the discrete $L^2$ inner product) basis that
optimally parametrizes the subspace of $L^2$ that is spanned by the solution
snapshots1. In this example, we use the poddim=25 leading singular vectors
to define the reduced model.

snapshotmat = mfac.Ft.dot(burgsol.y)
podmodes, svals, _ = spla.svd(snapshotmat, full_matrices=False)
selected_podmodes = podmodes[:, :poddim]
podvecs = mfac.solve_Ft(selected_podmodes)

In this implementation we use a sparse factor of the mass matrix instead of the
square root. The singular values (in particular those that correspond to the
discarded directions) give an indication of how good the approximation is.


The singular values of the snapshot matrix 

Here, the decay is comparatively slow, so that a one should not expect a good
low dimensional approximation by POD.

For the simulation, the state is parametrized by $u_h (t) \approx V \tilde
u_h(t)$ where $V$ is the matrix of the POD modes (in the code $V$ denoted by
podvecs), which gives a system in $\tilde u _ h$ with 25 degrees of freedom
(as opposed to the 51842 of the full order model).

redamat = podvecs.T.dot(amat.dot(podvecs))  # the projected stiffness

def redbrhs(time, redvec):
    inflatedv = podvecs.dot(redvec)
    redconv = podvecs.T.dot(burgers_nonl_vec(inflatedv))
    return -redamat.dot(redvec) - redconv.flatten()


Here we define the projected stiffness matrix and the reduced nonlinearity
through


  inflating the reduced state to full dimension
  applying the nonlinearity
  projecting down the result.


This means that our model is not completely independent of the full dimension.
For this problem there are hyperreduction techiques like DEIM.

Thus, the right hand side is readily defined the more that the projected mass
matrix is the identity. Why?

Finally, the initial value is projected into the reduced coordinates and the
reduced system is integrated in time.

redburgsol = solve_ivp(redbrhs, (t0, tE), prjinivvec,
                       t_eval=timegrid, method='RK23')
podredsol = redburgsol.y


In the solution we see that the reduced order model gives a decent approximation
in the smooth regime in the beginning and has its troubles approximating the front as can be seen in the error (log) plot.




Snapshots of the solution of the reduced system 


Snapshots of the log of the error between the full and the POD solution

3. DMD Reduced Model

POD is partially data driven – it uses data to create a basis but still
uses (a projection of) the model. If only snapshots but no model is given, one
may use the method of Dynamic Mode Decomposition2 (DMD) that tries to identify
a matrix $A$ that evolves the state like



In practice, one uses a set of snapshots and the two measurement matrices



and



Note that $X’$ is basically $X$ shifted by one time step.

Then the DMD matrix can be found by solving the linear regression problem



For the reduced DMD model we use the same snapshot matrix as for the POD. The
regression problem is solved via SVD to compute the needed pseudo inverse since
this naturally allows for a rank reduction and a factored representation of the
DMD matrix.

# ### dmd using truncated svd inverse

fburgsol = burgsol.y
Xmat = fburgsol[:, :-1]
Xdsh = fburgsol[:, 1:]
ux, sx, vxh = spla.svd(Xmat, full_matrices=False)
uxr, sxr, vxhr = ux[:, :poddim], sx[:poddim], vxh[:poddim, :]

# compute the dmd matrix in factored form: `dmda = dmdaone * dmdatwo`

dmdaone = Xdsh.dot(vxhr.T)
dmdatwo = np.linalg.solve(np.diag(sxr), uxr.T)


Once the DMD matrix $A$ is determined, the simulation of the DMD reduced model
is only a repeated multiplication by $A$.

# simulation of the dmd reduced model

dmdxo = inivvec
dmdsol = [dmdxo]
for k in np.arange(Nts):
    dmdsol.append(dmdaone.dot(dmdatwo.dot(dmdsol[-1])))

dmdsol = np.array(dmdsol).T


As can be seen from the results and the error plots, DMD does a good job in the
initial phase but fails in the region with the sharp front.


Snapshots of the DMD solution


4. Remarks

It is commonly accepted that POD does not work well for transport dominated
problems – like the current case with the low viscosity parameter nu=1e-4.

So, I think that the results for POD are quite good noting that the reduced
order model has 25 degrees of freedom whereas the full model has 51842.
Nonetheless, in my tests, increasing the number of basis functions did not help
much. One can use a larger nu to get better POD approximations.

The DMD approach shows a similar performance. If compared to POD, the
qualitative approximation looks less good but the numbers are slightly better.
All in all, the DMD approximation seems less reliable as for some parameter
choices, the performance severely deteriorated.

Differential Equations.* arXiv:1611.04050

Decomposition: Theory and Applications* arXiv:1312.0041

  
    
      See, e.g., Lemma 2.5 of Baumann, Benner, and Heiland (2018): *Space-Time Galerkin POD with Application in Optimal Control of Semi-linear Parabolic Partial &amp;#8617;
    
    
      See, e.g., Tu, Rowley, Luchtenburg, Brunton, Kutz (2013): *On Dynamic Mode &amp;#8617;
    
  

</description>
        <pubDate>Sat, 07 Mar 2020 00:00:00 +0100</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0009</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0009</guid>
        
        
        <category>tutorial</category>
        
        <category>WP04</category>
        
      </item>
    
  </channel>
</rss>
