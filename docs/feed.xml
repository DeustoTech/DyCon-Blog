<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DyCon Blog</title>
    <description>Welcome to the web interface of DyCon Toolbox, the computational platform developed within the &lt;a href='https://cmc.deusto.eus/dycon/' target='_blank'&gt;ERC DyCon - Dynamic Control&lt;/a&gt; project.</description>
    <link>https://deustotech.github.io/DyCon-Blog/</link>
    <atom:link href="https://deustotech.github.io/DyCon-Blog/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 23 Oct 2020 10:44:51 +0200</pubDate>
    <lastBuildDate>Fri, 23 Oct 2020 10:44:51 +0200</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Moving control strategy for memory-type equations</title>
        <description>This tutorial shows how to implement the moving control strategy for the heat equation in the presence of a memory term.

The standard null controllability problem for the heat equation reads as follows



where $\Omega$ is a bounded domain, $u_0\in L^2(\Omega)$ and the control $f$ acts on the subdomain $\omega\subset\Omega$. The objective is to steer the system from $u_0$ to zero in time $T$,

It is known that the null controllability of the heat equation holds in any positive time $T&amp;gt;0$ and for any open control region $\omega$. Nevertheless, the situation changes when in the model appears a memory term:



As a matter of fact, when the equation involes a memory term, it is impossible to achive null controllability in the classical sense, unless the control region coincides with the entire domain ($\omega=\Omega$).

This fact can be easily seen when applying a LQR control on both systems (\ref{heatinterior}) and (\ref{heatinterior_memory}). In the former case (without memory) the minimum of the LQR functional is an optimal control stabilizing the dynamics (Video 1), whereas when the memory enters into the model this minimum leaves the system far from the equilibrium (Vdeo 2).




    
        
          
             
           
        
        
          
             
           
        
    
    
        
          
              Video 1: LQR in the heat equation (Equation \ref{heatinterior})
           
        
        
          
              Video 2:  LQR in the heat equation with memory (Equation \ref{heatinterior_memory})
           
        
     
    
        
          
              We can see the time evolution of the two models with a control region, $\chi_{\omega}$, located in the position of the blue ellipsoid in the simulation. We observe that the LQR control is effective for the heat equation but not for the heat equation with memory.
           
        
     


To understand the reason behind the lack of null controllability for the memory-type equation (\ref{heatinterior_memory}), we can introduce the new variable



In this way, (\ref{heatinterior_memory}) is transformed into a coupled PDE/ODE system:



In (\ref{heatinterior_memory_z}), the equation $z_t = u$ is an infinte-dimensional ODE which, since it has no diffusion term, introduces non-propagation phenomena in the system. The non propagating components corresponding to this ODE are unable to reach the control region $\omega$ and, therefore, cannot be controlled.

A natural remedy to this issue is to operate with a moving control strategy. If the ODE components of the system cannot reach the control region $\omega$, then is the control region who needs to reach these components. Hence, our control region would be  function of the time variable, $\omega = \omega(t)$, and the corresponding model will be



This moving control strategy has been proposed and succesfully applied in several contributions of our team [1,2,3,4].

Memory-type equations are one of the core topics of WP5 of the DyCon ERC project. An abridged presentation of this working package can be found at the following link.

Numerical implementation of the moving control strategy

We present here the numerical implementation of the moving control strategy for the memory-type heat equation (\ref{heatinterior_memory_z_moving}). All the simulations will be in $2D$.

STEP 1: construction of the control region

The first step is to construct the moving control region $\omega(t)$, which requires to design the function $\chi_{\omega(t)}$. To this end, we made the assumption that the size and orientation of $\omega(t)$ do not change during the time interval $[0,T]$. Hence, once fixed the size and shape of the control region, we will obtain a function depending only on a point $x\in \Omega$.

In this simulation, we create the control region as a square which moves in the domain $\Omega$. This square can be constructed with a function $W(x)$ defined as



where $H(x)$ is the Heaviside function. Nevertheless, the function $W$ constructed in this way would be non-differentialbe at the points $x=a$ and $x=b$. This would represent a problem when implementing the gradient method to compute the control. To bypass this issue, in the definition of $W(x,a,b)$ we will use a smooth approximation of $H$ (See Figure 1).


  
    
      
    
  
  
    
      Figure 1: Graphicla representation of $W(x,a,b)$
    
  


Moreover, the generalization of this function to dimension two is immediate



In this way, given $(x_{min}, x_{max}, y_{min}, y_{max})$, we obtain the corresponding characteristic function which only depends on the position of a single point.

STEP 2: construction of the dynamics

Since $\chi_{\omega}$ can move and, moreover, we want to obtain the optimal trajectory for the control, we will incorporate in our system a moving particle whose position and velocity are denoted by $\textbf{d} = (d_x,d_y)$ and $\textbf{v} = (v_x,v_y)$, respectively (Video 3).


  
    
       
    
  
  
    
      Video 3: Representación gráfica de  $W_{2D}$
    
  


Besides, we will add a control $\textbf{g}(t) = (g_x(t),g_y(t))$ which will act as an exterior force moving the subset $\chi_{\omega}$.

In this way, our model takes the final form



STEP 3: optimal control problem

Following a standard optimal control strategy, the control function for our memory-type equation will be computed throught the minimization problem





Simulations

All the simulations in this tutorial have been performed using the DyCon Toolbox and CasADi. Since CasADi is incorporated into the DyCon toolbox, we can install it in the following way:

unzip('https://github.com/DeustoTech/DyCon-Computational-Platform/archive/master.zip')
addpath(genpath(fullfile(cd,'DyCon-toolbox-master'))
StartDyconPlatform


We shall now write the discrete version of equation (\ref{heatinterior_memory_z_all}):



Secondly, let us create the mesh variable

% Create the mesh variables
clear;
Ns = 7;
Nt = 15;
xline = linspace(-1,1,Ns);
yline = linspace(-1,1,Ns);
[xms,yms] = meshgrid(xline,yline);


and the matrix $A$ containing the $2D$ Laplacian and the ODE dynamics

A  = FDLaplacial2D(xline,yline);

Atotal = zeros(2*Ns^2+4,2*Ns^2+4);
%
Atotal( 1:Ns^2  , 1:Ns^2 ) = A;
%
Atotal( Ns^2+1 : 2*Ns^2   ,   1    :  Ns^2   )  =  eye(Ns^2);
Atotal(    1   :  Ns^2    , Ns^2+1 : 2*Ns^2  )  =  50*eye(Ns^2); % z = 50*y

RumbaMatrixDynamics = [0 0 1 0; ...
                       0 0 0 1; ...
                       0 0 0 0; ...
                       0 0 0 0 ];

Atotal(2*Ns^2+1:end,2*Ns^2+1:end) = RumbaMatrixDynamics;
Atotal = sparse(Atotal);


Finally, we create the function $B(\textbf{d}) = B(x_d,y_d)$ for the dynamics of the moving control
%%
% We create the B() function
xwidth = 0.3;
ywidth = 0.3;
B = @(xms,yms,xs,ys) WinWP05(xms,xs,xwidth).*WinWP05(yms,ys,ywidth);
Bmatrix =  @(xs,ys) [diag(reshape(B(xms,yms,xs,ys),1,Ns^2)) ;zeros(Ns^2)];


We now have everything we need to construct and solve the optimal control problem

opti = casadi.Opti();  % CasADi optimization structure

% ---- Input variables ---------
Ucas = opti.variable(2*Ns^2+4,Nt+1); % state trajectory
Fcas = opti.variable(Ns^2+2,Nt+1);   % control

% ---- Dynamic constraints --------
dUdt = @(y,f) Atotal*u+ [Bmatrix(u(end-3),u(end-2))*f(1:end-2) ; ...
                                         0                     ; ...
                                         0                     ; ...
                                     f(end-1)                  ; ...
                                     f(end)                    ]; 

% -----Euler backward method-------
for k=1:Nt % loop over control intervals
   y_next = Ucas(:,k) + (T/Nt)*dUdt(Ucas(:,k+1),dUdt(:,k+1)); 
   opti.subject_to(Ucas(:,k+1)==y_next); % close the gaps
end

% ---- State constraints --------
opti.subject_to(Ucas(:,1)==[Y0 ; 0.7 ; 0.7; -1.5 ; -1.5]);

% ---- Optimization objective  ----------
Cost = (Ucas(1:end-4,Nt+1))'*(Ucas(1:end-4,Nt+1));

opti.minimize(Cost); % minimizing L2 at the final time

% ---- initial guesses for solver ---
opti.set_initial(Ucas, Unum_free);
opti.set_initial(Fcas, 0);

% ---- solve NLP              ------
p_opts = struct('expand',false);
s_opts = struct('acceptable_tol',1e-4,'constr_viol_tol',1e-3,'compl_inf_tol',1e-3);
opti.solver('ipopt',p_opts,s_opts); % set numerical backend
tic
sol = opti.solve();   % actual solve
toc


The following video shows the results of our simulations, which allowed to compute an effective moving control steering the dynamics to rest at time $T$.



  
    
       
    
  



Video 3




Bibliography

[1] U. Biccari and S. Micu, Null-controllability properties of the wave equation with a second order memory term, J. Differential Equations, 267(2), 2019, 1376-1422.

[2] U. Biccari and M. Warma, Null-controllability properties of a fractional wave equation with a memory term, Evol. Eq. Control The., 9(2), 2020, 399-430.

[3] F. W. Chaves-Silva, X. Zhang and E. Zuazua, Controllability of evolution equations with memory, SIAM J. Control Optim., 55(4), 2017, 2437–2459.

[4] Q. Lü, X. Zhang and E. Zuazua, Null controllability for wave equations with memory, J. Math. Pures Appl., 108 (4), 2017, 500-531.
</description>
        <pubDate>Tue, 20 Oct 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp05/P0011</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp05/P0011</guid>
        
        
        <category>tutorial</category>
        
        <category>WP05</category>
        
      </item>
    
      <item>
        <title>Optimal Control Problem vs  Classification Problem in DyCon Toolbox</title>
        <description>En este tutorial veremos como podemos implementar un problema de clasificación simple mediante la librería de control óptimo DyCon Toolbox

Generación de datos

Para resolver un problema de clasificación supervisada necesitamos datos de entrada ${ \vec{x}{i} }{i=1}^N$ y de salida ${ \vec{y}{i} }{i=1}^N$. Eso lo generaremos de manera sintéctica con las siguientes lineas de código
clear;
N = 12;
xdata = linspace(-1,1,N);
ydata = [-1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1];

Podemos ver los datos generados
f = figure(1)
plot(xdata,ydata,'o-','LineWidth',2)
xlabel('x-data')
ylabel('y-data')
title('Data')
print('data.png','-dpng')




Ahora queremos una ecuación diferencial controlada para una variable $z(t)$ tal que cuando reciba una condición inicial $z(0) = x_i$ evolucione durante $t=1$, y devuelva $z(t)=y_i$, esto para $\forall i \in {1,\dots,N}$.

Es decir una ecuación diferencial, tal que



Donde $\mathcal{P}$ es una proyección del 
Es decir es una
sigma = @(x) tanh(x);
%% Dynamic definition
import casadi.*
As = SX.sym('A',[2 2]);
bs = SX.sym('b',[2 1]);
zs = SX.sym('z',[2*N 1]);
% Create a Big matrices
Afull =  SX.zeros(2*N,2*N);
for i = 1:N
   ind = ((i-1)*2+1):(i*2);
   Afull(ind,ind) = As; 
end
%
bfull = repmat(bs,N,1);
% sym time
ts = SX.sym('t');
% this is the control
us = [As(:);bs];
% define the dynamic equation
Fs = casadi.Function('F',{ts,zs,us},{Afull*sigma(zs) + bfull});

Creamos el objeto ode
Nt = 50; tspan = linspace(0,1,Nt);
%
idyn = ode(Fs,zs,us,tspan); % &amp;lt;- idyn is a ode object of DyCon Toolbox
SetIntegrator(idyn,'RK4') % &amp;lt;- For solve you need choose a numerical squeme
% initial condition
z0          = zeros(2*N,1);
z0(1:2:2*N) = xdata';
% put in idyn object 
idyn.InitialCondition = z0;

Optimal Control Definition

P = [1 1]; % no optimize P
Pfull = repmat(P,N,1);
Pzminusy = P*reshape(zs,2,N)-ydata; % &amp;lt;-- (P*z_i - y_i)
L   = casadi.Function('L',  {ts,zs,us},{ sum(sum(As.^2))  + bs'*bs  });
Psi = casadi.Function('Psi',{zs}      ,{ 1e5*(Pzminusy*Pzminusy') });
%
iocp = ocp(idyn,L,Psi);


Solve
[Uopt,Zopt] = IpoptSolver(iocp,ZerosControl(idyn));


%% Plot
figure(1)
clf
subplot(1,2,1)
l1 = plot(tspan,Zopt(1:2:2*N,:)','b');
hold on
l2 = plot(tspan,Zopt(2:2:2*N,:)','r');
legend([l1(1) l2(1)],{'z_i^1(t)','z_i^2(t)'},'Location','bestoutside')

title('Optimal State')
ylabel('z_i(t)')
xlabel('time')

subplot(2,2,2)
plot(tspan,Uopt(1:4,:)')
title('Optimal Control - A(t)')
xlabel('time')
legend({'A_1','A_2','A_3','A_4'},'Location','bestoutside')

subplot(2,2,4)
plot(tspan,Uopt(5:end,:)')
title('Optimal Control - b(t)')
xlabel('time')
legend({'b_1','b_2'},'Location','bestoutside')

Animation
fig = figure(2);
clf
hold on

xlim([-5 5])

ylim([-5 5])
for iter = 1:N
    if ydata(iter) &amp;gt; 0
        color = 'r';
    else
        color = 'b';
    end
    ilines(iter) = plot(Zopt(2*(iter-1)+1,1),Zopt(2*(iter-1)+2,1),'Color',color,'Marker','.','MarkerSize',20);
    jlines(iter) = plot(Zopt(2*(iter-1)+1,1),Zopt(2*(iter-1)+2,1),'Color',color,'Marker','none','MarkerSize',20);

end
% 
for it = 1:length(tspan)
   for iter = 1:N
    ilines(iter).XData = Zopt(2*(iter-1)+1,it);
    ilines(iter).YData = Zopt(2*(iter-1)+2,it);
    jlines(iter).XData = Zopt(2*(iter-1)+1,1:it);
    jlines(iter).YData = Zopt(2*(iter-1)+2,1:it);
   end
    pause(0.05)
end

</description>
        <pubDate>Mon, 27 Jul 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0012</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0012</guid>
        
        
        <category>tutorial</category>
        
        <category>WP01</category>
        
      </item>
    
      <item>
        <title>Stochastic optimization for simultaneous control</title>
        <description>What is a simultaneous control problem?

Consider the following parameter-dependent linear control system



with



The matrix $\mathbf{A}_\nu$ is associated with the Brunovsky canonical form of the linear ODE



where $x^{(N)}_\nu(t)$ denotes the $N$-th derivative of the function $x(t)$.

In (1)-(2), $x_\nu(t)\in L^2(0,T;\mathbf{R}^N)$, $N\geq 1$, denotes the state, the $N\times N$ matrix $\mathbf{A}_\nu$ describes the dynamics, and the function $u(t)\in L^2(0,T;\mathbb{R}^M)$, $1\leq M\leq N$, is the $M$-component control acting on the system through the $N\times M$ matrix $\mathbf{B}$. Here 

 is a random parameter following a probability law $\mu$, with $(\mathcal K,\mathcal F,\mu)$ the corresponding complete probability space.

With simultaneous controllability we refer to the problem of designing a unique parameter-independent control function capable to steer all the different realizations of (1) to some prescribed final target in time $T$, that is (see Figure 1 and Figure 2):





Figure 1: evolution of the free (left) and controlled (right) dynamics of (1)-(2) with $N=4$.



Figure 2: parameter-independent control function.

How can we solve a simultaneous controllability problem?

The computation of a simultaneous control for (1)-(2) can be carried out through a standard optimal control methodology by solving the following minimization problem



subject to the dynamics given by (1)-(2). Here $\mathbb{E}[\cdot]$ denotes the expectation operator.

Typical approaches to solve the optimization problem (3) are (see [3]):


  The Gradient Descent (GD) algorithm: we find the minimizer $\widehat{u}$ as the limit $k\to +\infty$ of the following iterative process




where for all $\nu\in\mathcal K$ the pair $(x_\nu,p_\nu)$ solves the coupled system




  The Conjugate Gradient (CG) algorithm: the gradient $\nabla F_\nu$ is rewritten in the form




where the operators $\mathcal L_{T,\nu}$ and $\mathcal L_{T,\nu}^\ast$ are defined as



with $U:=L^2(0,T;\mathbb{R}^M)$ and



We then use the conjugate gradient algorithm to solve the linear system $\mathbb{A}u = b$.

These two procedures are impractical when the cardinality of the parameter set $\mathcal K$ is large because they require repeated resolutions of the dynamics (4) for all $\nu\in \mathcal K$.

This issue can be bypassed by employing a stochastic optimization method. In particular, we can consider the following approaches:


  The Stochastic Gradient Descent (SGD) algorithm (see [2]): it is a drastic simplification of the classical GD in which, instead of computing the gradient of the functional for all parameters $\nu\in\mathcal K$, in each iteration this gradient is estimated on the basis of a single randomly picked configuration. This translates in the following recursion process




with $\nu_k$ selected i.i.d. from $\mathcal K$.

  The Continuous Stochastic Gradient (CSG) algorithm: it is a variant of SGD, based on the idea of reusing previously obtained information to improve the efficiency. The CSG recursion process for optimizing $F_\nu$ is given by




where the weights ${\alpha_\ell}_{\ell = 1}^k$ are obtained through the methodology presented in [4].

Experimental Results: comparison of the algorithms

We can test the efficiency of each one of the four aforementioned algorithms by performing simulations for increasing values of 
.
 In these simulations, we have chosen the initial state $x^0 =(1,1,1,1)^\top$ and the final target $x^T=(0,0,0,0)^\top$. The time horizon is set to be $T=1s$. The parameter set is a $|\mathcal K|$ points partition of the interval $[1,6]$: 

 with $\nu_1=1$ and $\nu_{|\mathcal K|}=6$.

Figure 3 displays the computational times (in logarithmic scale) the four algorithms need to compute a simultaneous control for (1)-(2).



Figure 3: computational time (in logarithmic scale) to converge to the tolerance $\varepsilon = 10^{-4}$ of the GD, CG, SGD and CSG algorithms applied to the problem (1)-(2) with different values of $|\mathcal K|$.

We can observe the following facts:


  The GD algorithm is the one showing the worst performances. This because it has to copy with a high per-iteration cost but also with the fact that controllability problems are typically bad conditioned.
  The CG algorithm is the one requiring the lower number of iterations to converge. This implies that CG is the best approach among the one considered when dealing with a low and moderate amount of parameters.
  The stochastic approaches SGD and CSG appear to be insensitive to the cardinality of the parameter set. This fact is not surprising if we consider that, no matter how many parameters enter in our control problem, with SGD and CSG each iteration of the optimization process always requires only one resolution of the coupled system (4).
  The CSG algorithm always outperforms SGD in terms of the number of iterations it requires to converge and, consequently, of the total computational time. This because in CSG the approximated gradient is close to the full gradient $\nabla F_\nu$ of the objective functional when $k\to +\infty$. This translates in a less noisy optimization process with better convergence behavior (see Figure 4).




Figure 4: convergence of the error for SGD and CSG. The plots correspond to $50$ launches of the two algorithms with a tolerance $\varepsilon = 10^{-4}$.


All these considerations corroborate the fact that, when dealing with large parameter sets, a stochastic approach is preferable to a deterministic one to address the simultaneous controllability of (1)-(2).

A more complete discussion on the employment of stochastic optimization algorithms for simultaneous controllability can be found in [1].

Bibliography

[1] U. Biccari, A. Navarro-Quiles and E. Zuazua, Stochastic optimization methods for the simultaneous controllability of parameter-dependent systems, preprint (2020).

[2] L. Bottou, F. E. Curtis and J. Nocedal, Optimization methods for large-scale machine learning, SIAM Rev., Vol. 60, No. 2 (2018), pp. 223-311.

[3] J. Nocedal and S. Wright, S, Numerical optimization, Springer Science &amp;amp; Business Media, 2006.

[4] L. Pflug, N. Bernhardt, M. Grieshammer and M. Stingl, A new stochastic gradient method for the efficient solution of structural optimization problems with infinitely many state problems, preprint (2020).
</description>
        <pubDate>Wed, 03 Jun 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0011</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0011</guid>
        
        
        <category>tutorial</category>
        
        <category>WP01</category>
        
      </item>
    
      <item>
        <title>Synchronized Oscillators</title>
        <description>Consider the Kuramoto model for the synchronization of coupled oscillators



Here $\theta_i(t)$, $i = 1,\ldots,N$, is the phase of the $i$-th oscillator, $\omega_i$ is its natural frequency and $K$ is the coupling strength. The frequencies $\omega_i$ are distributed with a given probability density $f(\omega)$, unimodal and symmetric around the mean frequency



that is, $f(\Omega+\omega) = f(\Omega-\omega)$.

It is known that, if the coupling $K$ is sufficiently strong, the oscillators will synchronize asymptotically, i.e.



We want to design a control capable to ensure synchronization in a final time horizon $T$, that is



To compute this optimal control allowing us to reach the synchronized configuration (3) we can adopt a classical approach based on the resolution of the following optimization problem



subject to the dynamics (1).

For the resolution of (3), we use the standard GD method, which looks for the minimum $u$ as the limit $k\to +\infty$ of the following iterative process



This gradient technique is most often chosen because it is very easy to be implemented.

In order to fully define the iterative scheme (4), we need to compute the gradient $\nabla J(u)$. This can be done via the Pontryagin maximum principle.

To this end, let us first rewrite the dynamics (1) in a vectorial form as follows



with $\Theta :=(\theta_1,\ldots,\theta_N)^\top$, $\Theta^0:=(\theta_1^0,\ldots,\theta_N^0)^\top$ and $\Omega :=(\omega_1,\ldots,\omega_N)^\top$, and where $F$ is the vector field given by



Using the notation just introduced, we obtain the following expression for the gradient of $J(u)$



where $\mathcal D_uF$ indicates the Jacobian of the vector field $F$, computed with respect to the variable $u$.

In (5), we denoted with $p = (p_1,\ldots,p_N)$ the solution of the adjoint equation associated with (1), which is given by



where $\mathcal D_\Theta F$ stands again for the Jacobian of the vector field $F$, this time computed with respect to the variable $\Theta$.

Taking into account the expression of the vector field $F$, we can then see that the iterative scheme (4) becomes



with



We then see that, at each iteration $k$ the optimization scheme (6) requires to solve a $N$-dimensional non-linear dynamical system. This may rapidly become computationally very expensive, especially when the number $N$ of oscillators in our system is large.

In order to reduce this computational burden, we can combine the standard GD algorithm with the so-called Random Batch Method (RBM), which is a recently developed approach (see [2]) allowing for an efficient numerical simulation of high-dimensional collective behavior problems. This technique is based on the following simple idea: at each time step $t_m = m\cdot dt$ in the mesh we employ to solve the dynamics, we divide randomly the $N$ particles into $n$ small batches with size $2\leq P&amp;lt;N$, denoted by $C_q$, $q = 1,\ldots,n$, that is



Once this partition of ${1,\ldots,N}$ has been performed, we solve the dynamics by interacting only particles within the same batch. In this way, we are now dealing with systems of lower dimension and the computational cost of the GD algorithm is reduced.

Numerical simulations

We can check the effectiveness of the proposed approach through some numerical simulations for the computation of the optimal control $\widehat{u}$.

Firstly, we can show that the optimization problem (3) indeed allows to compute an effective control function which is capable to steer the Kuramoto model (1) to a synchronized configuration.

In Figure 1-top, we show the evolution of the uncontrolled dynamics, which corresponds to taking $u\equiv 1$ in (1). As we can see, the oscillators are evolving towards a synchronized configuration, but synchronization is not reached in the short time horizon we are providing. In Figure 1-bottom, we show the evolution of the same dynamics, this time under the action of the control function $u$ computed through the minimization of $J(u)$. The subplot on the left corresponds to the simulations done with the GD approach, while the one on the right is done employing GD-RBM. We can clearly see how, in both cases, the oscillators are all synchronized at the final time $T=3s$. This means that both algorithms managed to compute an effective control.



  Figure 1. Top - evolution of the free dynamics of the Kuramoto model (1) with $N=10$ oscillators. Bottom - evolution of the controlled dynamics of the Kuramoto model (1) with $N=10$ oscillators. The control function $\widehat{u}$ is obtained with the GD (left) and the GD-RBM (right) approach.


In Figure 2, we display the behavior of the control function $\widehat{u}$ computed via the GD-RBM algorithm. We can see how, at the beginning of the time interval we are considering, this control is close to one and it is increasing with a small slope. On the other hand, this growth becomes more pronounced as we get closer to the final time $T=3s$.



  Figure 2. control function $\widehat{u}$ obtained through the GD-RBM algorithm applied to the Kuramoto model (1) with $N=10$ oscillators.


Notice that, in (1), $\widehat{u}$ enters as a multiplicative control which modifies the strength of the coupling $K$. Hence, according to the profile displayed in Figure 2, the control function $\widehat{u}$ we computed is initially letting the system evolving following its natural dynamics. Then, as the time evolves towards the horizon $T=3s$, $\widehat{u}$ enhances the coupling strength $K$ in order to reach the desired synchronized configuration (2).

We can now compare the performances of the GD and GD-RBM algorithms for the computation of the optimal control $\widehat{u}$. To this end, we can run simulations for increasing values of $N$, namely $N=10,50,100,250,1000$.

Figure Figure 3 shows the computational times required by the two methodologies to solve the optimization problem (3).



  Figure 3. computational times required by the GD and GD-RBM algorithm to compute the optimal control $\widehat{u}$ with increasing values of $N$.


Our simulations show how, for low values of $N$, the two approaches show similar behaviors. Nevertheless, when increasing the number of oscillators in our system, the advantages of the GD-RBM methodology with respect to GD become evident. In particular, the growth of the computational time for GD-RBM is significantly less pronounced than for GD. As a matter of fact, Figure 3 is not considering the case of $N=1000$ oscillators with GD, since the behavior with smaller values of $N$ already suggested that this experiment would be computationally too expensive.

On the other hand, even with $N=1000$ oscillators in the system, the GD-RBM approach turns out to be able to compute an effective control for the Kuramoto model (1) in about $29$ seconds (see Figure 4).



  Figure 4. evolution of the controlled dynamics of the Kuramoto model (1) with $N=1000$ oscillators. The control has been computed with the GD-RBM algorithm.


More details on the contents of this post can be found in [1].

Bibliography

[1] U. Biccari and E. Zuazua, A stochastic approach to the synchronization of coupled oscillators, to appear in Front. Energy Res.

[2] S. Jin, L. Li and J.-G. Liu, Random Batch Methods (RBM) for nteracting particle systems, J. Comput. Phys. 400 (2020), 108877.
</description>
        <pubDate>Wed, 03 Jun 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0006</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0006</guid>
        
        
        <category>tutorial</category>
        
        <category>WP01</category>
        
      </item>
    
      <item>
        <title>The interplay of control and deep learning</title>
        <description>It is superfluous to state the impact deep learning has had on modern technology. From a mathematical point of view however, a large number of the employed models remain rather ad hoc.

When formulated mathematically, deep supervised learning roughly consists in solving an optimal control problem subject to a nonlinear discrete-time dynamical system, called an artificial neural network.





1. Setup

Deep supervised learning [1, 3] (and more generally, supervised machine learning), can be summarized by the following scheme. 
We are interested in approximating a function $f: \R^d \rightarrow \R^m$, of some class, which is unknown a priori. We have data: its values 
at $S$ distinct points  (possibly noisy).
We generally split the $S$ data points into training  data  and testing data . In practice, $N\gg S-N-1$.

“Learning” generally consists in:


  Proposing a candidate approximation 
 $f_\Theta( \cdot): \R^d \rightarrow \R^m$, depending on tunable parameters $\Theta$ and fixed hyper-parameters $L\geq 1$, ${ d_k}$;
  
    Tune $\Theta$ as to minimize the empirical risk: 
where $\ell \geq 0$, $\ell(x, x) = 0$ (e.g. $\ell(x, y) = |x-y|^2$). This is called training.
  
  A posteriori analysis: check if test error  is small. 
This is called generalization.


Remark:


  
    There are two types of tasks in supervised learning:
classification ($\vec{y}_i \in {-1,1}^m$ or more generally, a discrete set), and regression ($\vec{y}_i \in \R^m$).
We will henceforth only present examples of binary classification, namely $\vec{y}_i \in {-1, 1}$, for simple presentation purposes.
  
  
    Point 3 is inherently linked with the size of the control parameters $\Theta$. 
Namely, a penalisation of the empirical risk in theory provides better generalisation.
  





 Figure 1. 
Underfitting, good generalization, and overfitting. We wish to recover the function $f(x) = \cos(\frac32 \pi x)$ (blue) on $(0, 1)$ from $S=20$ noisy data samples.
Constructed approximations using $N=12$ training data, while the remaining $8$ samples are used for testing the results. The most complicated model (right) is not necessarily the best (Occam's razor).
This is related to the Runge phenomenon.


Remark: It is at the point of generalisation where the objective of supervised learning differs slightly from classical optimisation/optimal control. Indeed, whilst in deep learning one too is interested in “matching” the labels $\vec{y}_i$ of the training set, one also needs to guarantee satisfactory performance on points oustide of the training set.



2. Artificial Neural Networks

There exists an entire jungle (see [1, 3]) of specific neural networks used in practice and studied in theory.
For the sake of presentation, we will discuss two of the most simple examples.

2.1. Multi-layer perceptron

We henceforth assume that we are given a training dataset 
.


Definition (Neural network):  Let  and  be given. Set $d_0 := d$ and $d_{L+1} :=m$.
A neural network with $L$ hidden layers is a map



where $z^L = z^L_i \in \R^m$ being given by the scheme



Here  are given parameters such that $A^k \in \R^{d_{k+1}\times d_k}$ and $b^k \in \R^{d_k}$, and $\sigma \in C^{0, 1}(\R)$ is a fixed, non-decreasing function.



Remark: Several remarks are in order:


  The above-defined neural network is usually referred to as the multi-layer perceptron (see Multilayer_perceptron)
  Observe that $z^k \in \R^{d_k}$ for .
  The function $\sigma$ is always nonlinear in practice (as otherwise the optimisation problem roughly coincides with least squares for a linear regression). It is called the activation function.
  Generally, $\sigma(x) = \max(x, 0)$ or $\sigma(x) = \tanh(x)$ (but others work too).
  Deep learning means optimisation subject to a multi-layered neural net: $L\geq 2$ at least.



Let us denote $\Lambda_k x :=A^k x + b^k$.




 Figure 2. 
The commonly used graph representation for a neural net.
This figure essentially represents the discrete-time dynamics of a single datum from the training set through the nonlinear scheme.



An MLP scheme with $\sigma(x) = \tanh(x)$ can be coded in pytorch more or less as follows:

import torch.nn as nn

class OneBlock(nn.Module):
	def __init__(self, d1, d2):
		super(OneBlock, self).__init__()
		self.d1 = d1
		self.d2 = d2

		self.mlp = nn.Sequential(
			nn.Linear(d1, d2),
			nn.Tanh()
		)
	def forward(self, x): 
		return self.mlp(x)

class Perceptron(nn.Module):
	def __init__(self, dimensions, num_layers):
		super(Perceptron, self).__init__()

		self.dimensions = dimensions
		_ = \
			[OneBlock(self.dimensions[k], self.dimensions[k-1]) for k in range(1, num_layers-1)]

		self.blocks = nn.Sequential(*_)
		self.projector = nn.Linear(self.dimensions[-2], self.dimensions[-1])

	def forward(self, x)_
		return self.projector(self.blocks(x))


We can save this class in a file called model.py.
Then one may create an instance of a Perceptron model for practical usage as follows:

import torch
device = torch.device('cpu')
from model import Perceptron

model = Perceptron(device, dimensions=[1,3,4,3,1], num_layers=5)




How it works.

We now see, in some overly-simplified scenarios, how the forward propagation of the inputs in the context of binary classification.




    
        
        
    


 Figure 3. 
Here $A^0 \in R^{2\times 1}$ and $b^0 \in \R^2$.
We see how the neural net essentially generates a nonlinear transform, such that the originally mixed points are now linearly separable.


An interesting blog on visualising the transitions is the following: https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/.



2.2. Residual neural networks

We now impose that the dimensions of the iterations and the parameters stay fixed over each step.
This will allow us to add an addendum term in the scheme.


Definition (Residual neural network [2]):  Let  and  be given. Set $d_0 := d$ and $d_{L+1} :=m$.
A residual neural network (ResNet) with $L$ hidden layers is a map



where $z^L = z^L_i \in \R^d$ being given by the scheme



Here $\Theta = {A^k, b^k}_{k=0}^{L}$ are given parameters such that $A^k \in \R^{d\times d}$ and $b^k \in \R^{d}$ for $k&amp;lt;L$ and $A^L \in \R^{m\times d}, b^L \in \R^m$, and $\sigma \in C^{0, 1}(\R)$ is a fixed, non-decreasing function.



3. Training

Training consists in solving the optimization problem:




  $\epsilon&amp;gt;0$ is a penalization parameter.
  It is a non-convex optimization problem because of the non-linearity of $f_L$.
  Existence of a minimizer may be shown by the direct method.


Once training is done, and we have a minimizer $\widehat{\Theta}$, we consider $f_{\widehat{\Theta}}(\cdot)$ and use it on other points of interest $\vec{x} \in \R^d$ outside the training set.

3.1. Computing the minimizer

The functional to be minimized is of the form



We could do gradient descent:



$\eta$ is step-size. But often $N \gg 1$ ($N=10^3$ or much more).

Stochastic gradient descent: (Robbins-Monro [7], Bottou et al [8]):


  
    pick $i \in {1, \ldots, N}$ uniformly at random
  
  
    $\Theta^{n+1} := \Theta^n - \eta \nabla J_{i}(\Theta^n)$
  



  Mini-batch GD: can also be considered (pick a subset of data instead of just one point)
  Use chain rule and adjoints to compute these gradients (“backpropagation”)
  Issues: might not converge to global minimizer; also how does one initialize the weights in the iteration (usually done at random)?



We come back to the file model.py. Here is a snippet on how to call training modules.

# Given generated data
# Coded a function def optimize() witin
# a class Trainer() which
# does the optimization of paramterss

epochs = input()
optimizer = torch.optim.SGD(perceptron.parameters(), lr=1e-3, weight_decay=1e-3)

trainer = Trainer(perceptron, optimizer, device)
trainer.train(data, epochs)



3.2. Continuous-time optimal control problem

We begin by visualising how the flow map of a continuous-time neural net separates the training data in a linear fashion at time $1$ (even before in fact).


  
   

    


 
 
 

 Figure 4. 
The time-steps play the role of layers. We are working with an ode on $(0, 1)$. We see that the points are linearly separable at the final time, so the flow map defines a &quot;linearisation&quot; of the training dataset.


Recall that often $\varphi \equiv \sigma$ (classification)  or $\varphi(x) = x$ (regression).

It can be advantageous to consider the continuous-time optimal control problem:



where $z = z_i$ solves




Remark:

The ResNet neural network can then be seen as the forward Euler discretisation with $\Delta t = 1$.
The relevance of this scale when passing from continuous to discrete has not been addressed in the litearature.


Thus, in the continuous-time limit, deep supervised learning for ResNets can be seen as an optimal control problem for a parametrised, high-dimensional ODE.

This idea of viewing deep learning as finite dimensional optimal control was (mathematically) formulated in [12], and subsequently investigated from a theoretical and computational viewpoint in [11, 10, 5, 6, 13, 14], among others.


Remark:

There are many tricks which can be used in the above ODE to improve performance.


  For instance, we may embed the initial data in a higher dimensional space at the beginning, and consider the system in an even bigger dimension. It is rather intuitive that the bigger the dimension where the system evolves is, the easier it is to separate the points by a hyperplane.
  However, characterising the optimal dimension where one needs to consider the evolution of the neural net in terms of the topology of the training data is, up to the best of our knowledge, an open problem. 
The choice in practice is done by cross-validation.





 Figure 5. 
Analogous scenario as in Figure 4, this time in dimension 3.


References:

[1] Ian Goodfellow and Yoshua Bengio and Aaron Courville. (2016). Deep Learning, MIT Press.

[2] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
770–778.

[3] LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature,
521(7553):436–444.

[4] LeCun, Y., Boser, B. E., Denker, J. S., Henderson, D., Howard, R. E., Hubbard,
W. E., and Jackel, L. D. (1990). Handwritten digit recognition with a back-propagation network. In
Advances in neural information processing systems, pages 396–404.

[5] Chen, T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. (2018). Neural
ordinary differential equations. In Advances in neural information processing systems, pages 6571–
6583.

[6] Dupont, E., Doucet, A., and Teh, Y. W. (2019). Augmented neural odes. In
Advances in Neural Information Processing Systems, pages 3134–3144.

[7] Herbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Mathematical Statistics, 22(3):400–407, 1951

[8] Léon Bottou, Frank E. Curtis and Jorge Nocedal: Optimization Methods for Large-Scale Machine Learning, Siam Review, 60(2):223-311, 2018.

[9] Matthew Thorpe and Yves van Gennip. Deep limits of residual neural networks. arXiv preprint arXiv:1810.11741, 2018.

[10] Weinan, E., Han, J., and Li, Q. (2019). A mean-field optimal control formulation
of deep learning. Research in the Mathematical Sciences, 6(1):10.

[11] Li, Q., Chen, L., Tai, C., and Weinan, E. (2017). Maximum principle based algorithms
for deep learning. The Journal of Machine Learning Research, 18(1):5998–6026.

[12] Weinan, E. (2017). A proposal on machine learning via dynamical systems. Communications in Mathematics and Statistics, 5(1):1–11.

</description>
        <pubDate>Thu, 30 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0010</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0010</guid>
        
        
        <category>tutorial</category>
        
        <category>WP01</category>
        
      </item>
    
      <item>
        <title>Control of reaction-diffusion under state constraints - Heterogeneous setting: Gene-flow.</title>
        <description>
    
    Control of reaction-diffusion
    This is a post in a collection of several on control of reaction-diffusion under state constraints
    See more ...
    


Equation and drift effect

The equation under concern is:



where $N$ is a positive regular function.

The physical meaning of the equation is a reaction-diffusion process on the proportion $u$ of individuals along a static heterogeneous distribution $N$.

The effect of this heterogeneous drift can be qualitatively understood in Figure 1. The blue line in the figure is the distribution $N$ and the orange one the term $-2\frac{N_x}{N}$. In the picture in the left we have the Gaussian with variance $\sigma$, leading to a drift effect that is pushing $u$ towards the boundary. At the right, we have the function $e^{x^2/\sigma}$ leading to the contrary effect, the population in the boundary is pushed inside the domain.

From the discussion above we expect the boundary control to be enhanced in the case of $N(x)=e^{x^2/\sigma}$ while diminished in the case of $N(x)=e^{-x^2/\sigma}$.



    
        
            
        
         
            
               
    


Figure 1. In blue the curves $N(x)=e^{\mp x^2/\sigma}$ and in orange the term $-2\frac{N_x}{N}$.

Slowly varying drifts: Perturbed Stair-case method

Whenever the drift term $-2\frac{N_x}{N}$ is small enough, one can consider the already developed path in the homogeneous setting (see Blog staircaise), taka a sequence and apply the implicit function theorem to obtain a sequence close enough to guarantee the controllability (see [1] for details). This guarantees the controllability from the steady state $0$ to the target $\theta$.

In Figure 2 an intuitive sketch of this procedure is represented.




Figure 2. Perturbed staircase method.

Large drifts: New Barriers

One of the main concerns when state-constraints are present is whether or not barriers exist (see Blog Barriers). If $\theta&amp;lt;\frac{1}{2}$ then, for large $L&amp;gt;L_{\sigma}(0)&amp;gt;0$ we will have obstructions to reach $\theta$ and $0$ from $1$.

In the homogeneous case, nontrivial elliptic solutions with boundary value $1$ do not exist. However, in the case of heterogeneous drift this is no longer true in general.

If we set $N(x)=e^{-x^2/\sigma}$ then for every $\sigma$ one can find an $L_{\sigma}(1)&amp;gt;0$ big enough such that for every $L&amp;gt;L_{\sigma}$ there is a nontrivial solution with boundary value $1$. This nontrivial solution does not correspond to a global minima of the associated energy functional since the trivial solution $u=1$ is the global minimizer.

In order to understand how this barrier can exist we have to study the following non-autonomous dynamics in the phase-plane:



Any solution of the ODE above is an elliptic solution for some boundary condition. What we want to find is that there exist an $L_{\sigma}(1)$ such that the solution of the ODE at $L_{\sigma}(1)$ is $1$.

Making use of the energy of the ODE



and seting $a$ small one can see that the trajectory of the dynamics will eventually cross $1$. See Figure 3.



    
        
            
        
         
            
               
    



Figure 3. At the left, the trajectory in the phase-plane associated with the elliptic solution. At the right, the resulting nontrivial solution.

In Figure 4 one can see the effect of the nontrivial solution in a simulation. The red lines are snapshots of the controlled trajectory, getting darker when advancing the time, the dotted blue line is the profile $N(x)$.




Figure 4. Simulation of the equation with control $a=1$ with $N(x)=e^{-x^2/\sigma}$.

Minimal controllability time depending on the drift

We have seen that the influence of the drift is key. Figure 5 shows the minimal controllability time for a fixed $L$ depending on the parameter $\frac{1}{\sigma}$. The initial state is the $0$ configuration and the target the constant steady state $\theta$.

When the variance is very big we tend to the homogeneous case, while when the variance decreases the opposite phenomena is observed; when the drift enhances the control the minimal controllability time is reduced making it tend to zero while, when the drift is pushing the population outside the minimal controllability time increases and eventually blows up with the emergence of the barrier.



    
        
            
        
         
            
               
    



Figure 5. Minimal controllability time depending on the variance. At the left for $N(x)=e^{x^2/\sigma}$ and at the right for $N(x)=e^{-x^2/\sigma}$. 

References:
[1] I. Mazari, D. Ruiz-Balet, and E. Zuazua, Constrained control of bistable reaction-diffusion equations: Gene-flow and spatially heterogeneous models, preprint: https://hal.archives-ouvertes.fr/hal-02373668/document (2019).

[2] D. Pighin, E. Zuazua, Controllability under positivity constraints of multi-d wave equations, in:
Trends in Control Theory and Partial Differential Equations, Springer, 2019, pp. 195–232.

[3] J.-M. Coron, E. Trélat, Global steady-state controllability of one-dimensional semilinear heat equa-
tions, SIAM J. Control. Optim. 43 (2) (2004) 549–569.

[4] C. Pouchol, E. Trélat, E. Zuazua, Phase portrait control for 1d monostable and bistable reac-
635
tion–diffusion equations, Nonlinearity 32 (3) (2019) 884–909.

[5]   D.  Ruiz-Balet  and  E.  Zuazua. Controllability  under  constraints  for  reaction-diffusionequations:   The  multi-dimensional  case.   Preprint  available  athttps://cmc.deusto.eus/domenec-ruiz-balet/.

[6] D.  Ruiz-Balet  and  E.  Zuazua. Control of certain parabolic models from biology and social sciences.   Preprint  available  at https://cmc.deusto.eus/domenec-ruiz-balet/.

</description>
        <pubDate>Sun, 19 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0008</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0008</guid>
        
        
        <category>tutorial</category>
        
        <category>WP03</category>
        
      </item>
    
      <item>
        <title>Control of reaction-diffusion under state constraints - Numerical exploration of controls</title>
        <description>
    
    Control of reaction-diffusion
    This is a post in a collection of several on control of reaction-diffusion under state constraints
    See more ...
    


In this tutorial we will observe different phenomena that arise in the control with state-constraints and moreover we will observe different ways to approach a desired target.

Observation of several phenomena

In the simulations below, we will consider the following optimal control problem:



In Figure 1, the initial state was $u_0=1$ and we can observe how the solution has been successfully driven to the target $u_1=0.33$. For this length of the domain there is no barrier (see Barriers)), for this reason the optimal solution can achieve the target.

The trajectory of the control can also be understood from the theoretical understanding of the controllability under state constraints.

In the first phase, the control takes values that are below the target $u_1=0.33$. This behavior is necessary because for this length of the domain there exist nontrivial solutions with boundary value $0.33$ that are between $0$ and $1$.

These nontrivial solutions do not constitute an intrinsic obstruction since we know that the equation is controllable for this length of the domain (see [1,3] and Staircase post and Barriers post). However, the strategy of setting boundary equal to $0.33$ for a long time and then doing a local control will not work.

The second oscillatory phase has been already observed in the unconstrained case (see [4]).



    
        
            
        
         
            
               
    



Figure 1. At the left, the controlled state, at the right, the control function. $u_0=1$ and $u_1=0.33$. 

Another important feature is the existence of a minimal controllability time. In Figure 2, $u_0=0$ and the target is $u_1=0.33$. From the theoretical perspective, if the time horizon is large enough we are always able to go from the initial state to the target. This is due to the existence of an admissible continuous path of steady states (Staircase post).

However, in Figure 2, one observes that the target $u_1=0.33$ has been not achieved. The reason is that $T$ is not large enough for the equation to be controllable.



    
        
            
        
         
            
               
    



Figure 2. At the left, the controlled state displayed in space-time, at the right, different snapshots of the state, darker curves are associated with larger times. $u_0=0$ and $u_1=0.33$ .

The other phenomenology already mentioned above and in the blog post Barriers is the lack of controllability due to the emergence of nontrivial solutions. In Figure 3, the initial state is $u_0=1$ and the target is $u_1=0$. We can observe how the state gets stuck to the barrier.



    
        
            
        
         
            
               
    



Figure 3. At the left, the controlled state displayed in space-time, at the right, different snapshots of the state, darker curves are associated with larger times. $u_0=1$ and $u_1=0$ .

Exploration

Now we turn our attention to the exploration of different control strategies.

Quasistatic

In Figure 4, we seek to find a control that is quasistatic, for doing so we penalize the time derivative of the control in the discrete level and we add the restriction $u(x,T)=0.33$.





    
        
            
        
         
            
               
    



Figure 4. At the left, the controlled state displayed in space-time, at the right, the control function. $u_0=0$ and $u_1=0.33$ . 

Minimal control time

In Figure 5, we compute the control in minimal time by minimizing the functional:





    
        
            
        
         
            
               
    



Figure 5. At the left, the controlled state displayed in space-time, at the right, the control function. $u_0=0$ and $u_1=0.33$ .

Figure 5 points out that the control in minimal time has the bang-bang property.

Minimal flow control

In Figure 6, we compute the control with minimal flow by minimizing the functional:





    
        
            
        
         
            
               
    



Figure 6. 

References:

[1] D.  Ruiz-Balet  and  E.  Zuazua. Control of certain parabolic models from biology and social sciences.   Preprint  available  at https://cmc.deusto.eus/domenec-ruiz-balet/.

[2] J.-M. Coron, E. Trélat, Global steady-state controllability of one-dimensional semilinear heat equa-
tions, SIAM J. Control. Optim. 43 (2) (2004) 549–569.

[3] C. Pouchol, E. Trélat, E. Zuazua, Phase portrait control for 1d monostable and bistable reac
tion–diffusion equations, Nonlinearity 32 (3) (2019) 884–909.

[4]  R. Glowinski, J. L. Lions and J. He. “Exact and approximate controllability for distributed parameter systems: a numerical approach.” Encyclopedia of Mathematics and its Applications (2008).

</description>
        <pubDate>Sat, 04 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0009</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0009</guid>
        
        
        <category>tutorial</category>
        
        <category>WP03</category>
        
      </item>
    
      <item>
        <title>Inverse problem for Hamilton-Jacobi equations</title>
        <description>Background and motivation

We consider the initial-value problem for a Hamilton-Jacobi equation of the form



where $u_0\in C^{0,1}(\mathbb{R}^n)$ and the Hamiltonian $H: \mathbb{R}^n\rightarrow\mathbb{R}$ is assumed to satisfy the following hypotheses:



Here, the unknown $u$ is a function $[0,T]\times\mathbb{R}^n\longrightarrow\mathbb{R}$,
and the inequality $H_{pp}(p)&amp;gt;0$ means that the Hessian matrix of $H$ at $p$ is positive definite.
The study of Hamilton-Jacobi equations
arises in the context of optimal control theory and calculus of variations,
where the value function satisfies, in a weak sense, a Hamilton-Jacobi equation.
In this context, Hamilton-Jacobi equations have applications in a wide range of fields such as
economics, physics, mathematical finance, traffic flow and geometrical optics.

Our goal in this tutorial is to study the inverse design problem associated to (HJ).
More precisely, for a given target function $u_T$ and a time horizon $T&amp;gt;0$,
we want to construct all the initial conditions $u_0$ such that the viscosity solution of (HJ) coincides with $u_T$ at time $T$.

The study of this problem can also be motivated by considering the following question:

Given an observation of the solution to (HJ) at time $T&amp;gt;0$, can we construct all the possible initial data that agree we the observation at time $T$ ?

For this purpose, for a fixed $T&amp;gt;0$, 
we define the following nonlinear operator, which associates to any initial condition $u_0$, the function
$u(T,\cdot)$, where $u$ is the viscosity solution of (HJ):



The inverse design problem that we are considering is then reduced to, for $u_T$ and $T&amp;gt;0$ given,
characterize all the initial conditions $u_0$ satisfying $S_T^+ u_0 =u_T$.

Reachability of the target

The first thing one notices when addressing this problem is that not all the Lipschitz targets are reachable.
Indeed, as it is well known, the viscosity solution to (HJ) is always a semiconcave function (see [2,4]). 
Therefore, an obvious necessary (but not sufficient) condition for the reachability of $u_T$ is that it must be a semiconcave function.
See Figures 1 and 2 for some examples of unreachable targets in dimension 1 and 2 respectively. Observe that, in particular, they are not semiconcave functions.



  




 Figure 1:  Two unreachable target functions in dimension 1.


We then start our study by characterizing the set of targets that are reachable.
In other words, for a given $u_T$ and $T&amp;gt;0$, we aim to determine whether or not there exists at least one initial condition $u_0$ satisfying $S_T^+ u_0=u_T$.
The natural candidate is the one obtained by reversing the time in the equation, considering $u_T$ as terminal condition. Here, we have to consider the class of backward viscosity solutions, for which the terminal-value problem associated to (HJ) is well-posed (see for example [1]).

We then define the backward operator as follows:



which associates to any terminal condition $u_T$, the function $w(0,\cdot)$,
i.e. the unique backward viscosity solution at time $0$, with terminal condition $u_T$.



  




 Figure 2:  Two unreachable target functions in dimension 2.


Let us introduce, for a target $u_T$ and a time horizon $T&amp;gt;0$, the set



of initial conditions $u_0$ satisfying $S_T^+ u_0 = u_T$.
We say that a target $u_T$ is reachable for the time horizon $T$ if $I_T(u_T)\neq \emptyset$.
We can equivalently say that $u_T$ is an admissible observation at time $T&amp;gt;0$.

Here we give a result that identifies the set of reachable targets (or admissible observations) at time $T$,
with the fix-points of the composition operator $S_T^+\circ S_T^-$.



Theorem 1: Let $H$ satisfy (2), $u_T\in C^{0,1}(\mathbb{R}^n)$ and $T&amp;gt;0$. Then, the set $I_T(u_T)$ is nonempty 
if and only if  $S_T^+ \left( S_T^- u_T\right) = u_T.$



In other words, a target is reachable if and only if the initial condition $\tilde{u}_0:= S_T^- u_T$, obtained by applying the backward operator, satisfies $\tilde{u}_0\in I_T(u_T)$.
Or equivalently, an observation $u_T$ at time $T$ is admissible if and only if $S_T^+ \left( S_T^- u_T\right) = u_T.$

Projection on the set of reachable targets

In the case the observation $u_T$ at time $T$ is not admissible, maybe due to noise effects or to errors in the measurements, we can actually project it on the set of admissible observations by applying the operator $S_T^+\circ S_T^-$. For a given $u_T\in C^{0,1}(\mathbb{R}^n)$, we define this projection as follows:



Observe that $I_T(u_T^\ast)\neq \emptyset$ for any $u_T\in C^{0,1}(\mathbb{R}^n)$. 
Indeed, $\tilde{u}_0:=S_T^- u_T\in I_T (u_T^\ast)$ by definition.



  

  




 Video 1:  Projection of the examples in Figure 1 on the set of admissible observations at times $T=0.3$ and $T=0.8$ respectively.


In the Videos 1 and 2, we observe how the examples in Figures 1 and 2 are projected on the set of reachable targets.
We recall that the projection is obtained by solving the problem (HJ) backward in time and the forward.
Observe that, when the time goes backward, the solution is semiconvex, while in the forward resolution, it becomes semiconcave. See [1,4] for the classical results concerning this regularizing effect and [2] for the definition and properties of semiconcave and semiconvex functions.



  

  




 Video 2:  Projection of the examples in Figure 2 on the set of admissible observations at time $T=1$.


Construction of initial data

Once we have projected the observation $u_T$ on the set of reachable targets at time $T$, we proceed to the construction of all the initial data $u_0$ satisfying $S_T^+ u_0 = u_T^\ast$.
This construction is contained in the recent work [3], and reads as follows:


Theorem 2: Let $H$ satisfy (2) and $T&amp;gt;0$.
Let $u_T\in C^{0,1}(\mathbb{R}^n)$ and define the functions



Then, for any $u_0\in C^{0,1} (\mathbb{R}^n)$, the two following statements are equivalent:

  $u_0\in I_T(u_T^\ast)$;
  $u_0(x)\geq \tilde{u}_0 (x), \ \forall x\in \mathbb{R}^n \quad \text{and} \quad u_0(x) = \tilde{u}_0(x), \ \forall x\in X_T(u_T^\ast),$


where $X_T(u_T^\ast)$ is the subset of $\mathbb{R}^n$ given by





In view of this result, we observe that, for a given admissible observation $u_T^\ast$ at time $T$,
the construction of the possible initial data can be carried out after obtaining the two following ingredients:

  the function $\tilde{u}_0 = S_T^- u_T^\ast$, obtained as the backward viscosity solution to (HJ) with terminal condition $u_T^\ast$;
  and the set $X_T(u_T^\ast)\subset \mathbb{R}^n$, that can be deduced from the differentiability points of $u_T^\ast$.


In Figures 3 and 4 below, we can see the scheme of the construction of all the initial data for the projection $u_T^\ast = S_T^+ (S_T^- u_T)$ of the non-admissible observations in Figure 1.
Observe that all the initial data in $I_T(u_T^\ast)$ must coincide with $\tilde{u}_0$ on the red region, while on the black region, they must be greater or equal than $\tilde{u}_0$.



  




 Figure 3:  Reconstruction scheme for the admissible target obtained in Video 1 at the left.




  




 Figure 4:  Reconstruction scheme for the admissible target obtained in Video 1 at the right.


An important consequence of Theorem 2 is that the initial data in $I_T(u_T^\ast)$ are not unique whenever $X_T(u_T^\ast) \neq \mathbb{R}^n$.
Indeed, the set $I_T(u_T^\ast)$ can be given in the following way:



In the case of the examples illustrated in Figures 3 and 4,
we can see in the videos 3 below, examples of different initial conditions whose solution coincides with $u_T^\ast$ at time $T$.
These exmaples have been generated randomly by adding to $\tilde{u}_0$ a nonegative a Lipschitz function vanishing in $X_T(u_T^\ast)$.



  
  




 Video 3:  Examples of initial data $u_0\in I_T(u_T^\ast)$ for the reconstruction schemes depicted in Figures 3 and 4.


A possible interpretation of this reconstruction result is that the initial datum can only be reconstructed in the region $X_T(u_T^\ast)$, where it is uniquely determined by $\tilde{u}_0$,
while in the region $\mathbb{R}^n\setminus X_T(u_T^\ast)$, only a lower bound can be deduced.
In this last region, we can say that the information of the initial data has been partially lost after the time-interval $[0,T]$.
In the case of smooth solutions (when they exists) we have backward uniqueness, i.e. $I_T(u_T)$ is either empty or a singleton (see [1]).
In this case it holds that $X_T(u_T)=\mathbb{R}^n$.

Next, we see the reconstruction schemes for the examples depicted in Video 2, that correspond to the projection on the set of admissible observations of $u_T$ from Figure 2.
Observe the different structure of the set $\mathbb{R}^n\setminus X_T(u_T^\ast)$ (white region in the plot at the right)
in the surrounding area of bumps and wells of $u_T^\ast$.

  Near a bump, the set where $u_T^\ast$ is not differentiable is an isolated point, and then, it produces a ball for the set $\mathbb{R}^n\setminus X_T(u_T^\ast)$.
  Near a well, $u_T^\ast$ is non-differentiable on a circumference. This produces an annulus for the set $\mathbb{R}^n\setminus X_T(u_T^\ast)$.




  




 Figure 5:  Reconstruction scheme for the admissible target obtained in Video 2 at the left.




  




 Figure 6:  Reconstruction scheme for the admissible target obtained in Video 2 at the right.


Evolution of $X_T(u_T)$

As we have seen in the previous section, given an admissible observation $u_T$ of the solution at time $T$,
the initial datum can only be reconstructed in a region $X_T(u_T)$, while in its complementary, only a lower bound can be obtained.
Here, we are interested in the  the role that $T$ plays in this reconstruction issue.
For this purpose, we fix an initial datum $u_0$ and then, for each $T&amp;gt;0$,
we construct the set $X_T(u_T)$, where $u_T := S_T^+ u_0$.

Our goal is to observe how $X_T(u_T)$ evolves as we increase $T$,
or in other words, we want to know what information from $u_0$ can be recoverd when the solution is observed at time $T$.
As we see in the following videos, $X_T(u_T)$ becomes smaller as we increase $T$, meaning that less information from the initial datum can be recovered if the observation is made after a long time interval.

In Figure 7 we fix two initial data in dimension 1.



  




 Figure 7:  Two initial data in dimension 1.


Next, we can see in in Video 4, how the reconstruction scheme for $u_0$ (at the left in Figure 7) evolves as we increase the observation time.
For $T$ small, very little information is lost, however, for $T$ large, $X_T(u_T)$ only intersects the nonpositive region of $u_0$, and therefore, the two bumps can no longer be reconstructed.



  




 Video 4:  Evolution of the target and the reconstruction scheme for the initial datum at the left in Figure 7.


In Video 5, we see the evolution of the reconstruction scheme for $u_0$ at the right in Figure 7.
In this case, $u_0$ is constituted of two wells, however, if the observation is made at time $T$ large enough, only the deepest well can be identified.



  




 Video 5:  Evolution of the target and the reconstruction scheme for the initial datum at the right in Figure 7.


We end this blog post with three examples in the two-dimensional case.
Here we have the three examples of initial data that we have considered.



  




 Figure 8:  Three initial data in dimension 2.


In Video 6, we see the evolution of the reconstruction scheme for our first 2-dimensional example.
In this case, $u_0$ has a well and a bump. We observe that, as we increase the observation time $T$,
the white region, which corresponds to $\mathbb{R}^n\setminus X_T(u_T)$ becomes bigger and bigger.
This corresponds to the fact that, when the observation is made after a long time-interval, less information from the initial datum can be recovered. In addition, if we look at $u_T^\ast$ and $\tilde{u}_0$ in the two first plots, we see that the bump disappears for $T$ large enough.
This implies that after a long time-interval, the bump cannot be reconstructed.



  




 Video 6:  Evolution of the reconstruction scheme for the first initial datum in Figure 8.


In Video 7, we see the evolution of the reconstruction scheme for the second example in Figure 8.
Here, the initial datum has two bumps of different height but same shape and size in their support.
Observe that, when $T$ is large enough, both bumps look the same, and therefore we cannot guess which one was taller at $t=0$. In fact, only the shape of their support can be reconstructed.



  




 Video 7:  Evolution of the reconstruction scheme for the second initial datum in Figure 8.


In Video 8, we see the evolution of the reconstruction scheme for the third example in Figure 8.
Here, the initial datum has two wells of different depth.
We observe the same behaviour as in Video 5, where only the information of the deepest well is preserved for $T$ large enough.



  




 Video 8:  Evolution of the reconstruction scheme for the third initial datum in Figure 8.


References

[1] E. Barron, P. Cannarsa, R. Jensen, C. Sinestrari,  Regularity of Hamilton-Jacobi equations when forward is backward. Indiana University mathematics journal, pages 385-409, 1999.

[2] P. Cannarsa, C. Sinestrari,  Semiconcave functions, Hamilton-Jacobi equations, and optmial control. volume 58. Springer Science &amp;amp; Business Media, 2004.

[3] C. Esteve, E. Zuazua,  The inverse problem for Hamilton-Jacobi equations and semiconcave envelopes. Preprint &amp;lt;https://arxiv.org/abs/2003.06914

[4] P.L. Lions,  Generalized solutions of Hamilton-Jacobi equations. volume 69. London Pitman, 1982.
</description>
        <pubDate>Fri, 03 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0010</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp04/P0010</guid>
        
        
        <category>tutorial</category>
        
        <category>WP04</category>
        
      </item>
    
      <item>
        <title>Control of reaction-diffusion under state constraints - Application of the staircase method</title>
        <description>
    
    Control of reaction-diffusion
    This is a post in a collection of several on control of reaction-diffusion under state constraints
    See more ...
    


In this tutorial, we will present how to generate admissible paths of steady states for the homogeneous reaction-diffusion equation:



This a key argument for ensuring controllability that has been studied in [2,3,4,5].

Stair-case method
The stair-case method (see [1]) guarantees the following:

If we have an admissible continuous path of steady states, for any initial datum in the path, any target function in the path and for $T&amp;gt;0$ sufficiently large, there exists a function $a$ such that drives the system from the initial datum to the target fulfilling the state-constraints along the trajectory.




Figure 1. Qualitative understanding of the Stair-case method. When we have a continuous admissible path of steady-states, one can find a control that is connecting the initial steady state with the target by &quot;jumping&quot; along the continuous path.

Extension to ball, and the phase-plane analysis

We will restrict ourselves to the construction of paths that connect the steady state $w\equiv 0$ with the steady state $w\equiv \theta$. For doing such example with our model bistable equation.

In order to construct the paths we will use phase-plane techniques for which we use radial coordinates. Our original domain might not be a ball, for this reason we extend it to a ball and firstly construct the path there.




Figure 2. Extension of our domain to a ball.

Remind that the important issue is to be able to guarantee that for every domain $w\equiv0$ and $w\equiv\theta$ are connected in an admissable way and this is seen in the phase plane representation of the elliptic equation.





Now, considering the energy


where $F(u)=\int_0^u f(s)ds$, one can see that the radial ODE dissipates.

Define the following region:

Let $\theta_1$ be defined as:


Note that the region defined by 

Note that $\Gamma\subset D$.

Take $(u_0,0)\in \Gamma$, then the solution of the radial equation with initial
datum $(u_0,0)$ satisfies:



So $(u,v)\in\Gamma$ for all $r&amp;gt;0$.

We have that the blue line (the border of $\Gamma$) in the following figure determines a positively invariant region.

Then, making $a$ change continuously from $0$ to $\theta$ we generate a continuous path (by the Gromwall inequality) that is admissible since the invariant region $\Gamma$ is inside the admissible set.




Figure 3. Invariant region and construction of the path.

Animation of the path



  


Figure 4. Path of steady states.

If our domain is not a ball we restrict our path to the original domin to obtain the desired path.

Other features

General existence of admisible paths is not true due to the comparison principle. Non-trivial steady states can exist that block any possibity to control.

Here we restrict ourselves in the one dimensional case. In the following figure one can see how can we connect the steady state $w\equiv 0$ with the first nontrivial solution with boundary value $0$.




    
        
            
        
         
            
                           
    



Figure 5. Path of steady states.

However, since this path touches the boundary of the admissible set controllability cannot be guaranteed due to the comparison principle.

Another important remark to be mentioned is that if we forget about the state constraints, more paths can be generated, provided that the ODE representation of the elliptic equation does not blow up. The following figure is an example of a path connecting $w\equiv 0$ with $w\equiv 1$ that violates the constraints:



    
        
            
        
         
            
                           
    


Figure 6. Path of steady states violating constraints.

References:

[1] D. Pighin, E. Zuazua, Controllability under positivity constraints of multi-d wave equations, in:
Trends in Control Theory and Partial Differential Equations, Springer, 2019, pp. 195–232.

[2] J.-M. Coron, E. Trélat, Global steady-state controllability of one-dimensional semilinear heat equa-
tions, SIAM J. Control. Optim. 43 (2) (2004) 549–569.

[3] C. Pouchol, E. Trélat, E. Zuazua, Phase portrait control for 1d monostable and bistable reac-
635
tion–diffusion equations, Nonlinearity 32 (3) (2019) 884–909.

[4]   D.  Ruiz-Balet  and  E.  Zuazua. Controllability  under  constraints  for  reaction-diffusionequations:   The  multi-dimensional  case.   Preprint  available  athttps://cmc.deusto.eus/domenec-ruiz-balet/.

[5] D.  Ruiz-Balet  and  E.  Zuazua. Control of certain parabolic models from biology and social sciences.   Preprint  available  at https://cmc.deusto.eus/domenec-ruiz-balet/.

</description>
        <pubDate>Fri, 03 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0005</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp03/P0005</guid>
        
        
        <category>tutorial</category>
        
        <category>WP03</category>
        
      </item>
    
      <item>
        <title>The Optimal control on the Kuramoto adaptative coupling model with DyCon Toolbox</title>
        <description>In this tutorial, we present how to use Pontryagin environment to control a consensus system that models the complex emergent dynamics over a given network. The control basically minimize the cost functional which contains the running cost and desired final state.

Setup

In this tutorial we need DyCon Toolbox, to install it we will have to write the following in our MATLAB console:

unzip('https://github.com/DeustoTech/DyCon-Computational-Platform/archive/master.zip')
addpath(genpath(fullfile(cd,'DyCon-toolbox-master')))

Model

The Kuramoto model describes the phases $\theta_i$ of active oscillators, which is described by the following dynamics:



Here, the first constant terms $\omega_i$ denote the natural oscillatory behaviors, and the interactions are nonlinearly affected by the relative phases. The amplitude and connections of interactions are determined by the coupling strength, $K_{i,j}$.

Control strategy

The control interface is on the coupling strength as follows:



This is a nonlinear version of bi-linear control problem for the Kuramoto interactions. The idea is as follows;


  
    There are $N$ number of oscillators, oscillating with their own natural frequencies.
  
  
    We want to make a collective behavior using their own decision process. The interaction is given by the Kuramoto model, or may follow other interaction rules. The network can be given or flexible with control.
  
  
    The cost of control will be related to the collective dynamics we want, such as the variance of frequencies or phases.
  


Numerical simulation

Here, we consider a simple problem: we control the all-to-all network system to get gathered phases at final time $T$. We first need to define the system of ODEs in terms of symbolic variables.

clear all
m = 50;  %% [m]: number of oscillators.

import casadi.*

t     = SX.sym('t');
symTh = SX.sym('y', [m,1]);   %% [y] :  phases of oscillators, $\theta_i$.
symOm = SX.sym('om', [m,1]);  %% [om]:  natural frequencies of osc., $\omega_i$.
symK  = SX.sym('K',[m,m]);    %% [K] :  the coupling network matrix, $K_{i,j}$.
symU  = SX.sym('u',[1,1]);    %% [u] :  the control function along time, $u(t)$.


syms Vsys;    %% [Vsys]: the vector fields of ODEs.
symThth = repmat(symTh,[1 m]);


Vsys = casadi.Function('V',{symOm,symK,symTh,symU}, ...
{ symOm + (symU./m)*sum(symK.*sin(repmat(symTh,[1 m]).' - repmat(symTh,[1 m])),2) }); 


The parameter $\omega_i$ and $K_{i,j}$ should be specified for the calculations. Practically, $K_{i,j}u(t) &amp;gt; \vert \max\Omega - \min\Omega \vert$ leads to the synchronization of frequencies. We normalize the coupling strength to 1, and give random values for the natural frequencies from the normal distribution $N(0,0.1)$. We also choose initial data from $N(0,\pi/4)$.

rng('default');
rng(1,'twister');
Om_init = normrnd(0,0.2,m,1);
Om_init = Om_init - mean(Om_init);  %% Mean zero frequencies
Th_init = normrnd(0,pi/8,m,1);


K_init = ones(m,m);                 %% Constant coupling strength, 1.
T = 5;                              %% We give enough time for the frequency synchronization.


symF = casadi.Function('F',{t,symTh,symU},{Vsys(Om_init,K_init,symTh,symU)});

tspan = linspace(0,T,110);
odeEqn = ode(symF,symTh,symU,tspan);
SetIntegrator(odeEqn,'RK4')
odeEqn.InitialCondition = Th_init;


We next construct cost functional for the control problem.

PathCost  = casadi.Function('L'  ,{t,symTh,symU},{ (1/2)*(symU'*symU)           });
FinalCost = casadi.Function('Psi',{symTh}      ,{  1e5*(1/m^2)*sum(sum((symThth.' - symThth).^2))  });

iCP_1 = ocp(odeEqn,PathCost,FinalCost);


Solve Gradient descent

tic
ControlGuess = ZerosControl(odeEqn);
[OptControl_1 ,OptThetaVector_1] =  ArmijoGradient(iCP_1,ControlGuess);
toc


Length Step has been change: LenghtStep = 0.00025
iter: 001 | error: 2.215e+02 | LengthStep: 5.00e-04 | J: 1.16893e+03 | Distance2Target: NaN 
iter: 002 | error: 2.214e+02 | LengthStep: 1.00e-03 | J: 1.16780e+03 | Distance2Target: NaN 
iter: 003 | error: 2.212e+02 | LengthStep: 2.00e-03 | J: 1.16554e+03 | Distance2Target: NaN 
iter: 004 | error: 2.207e+02 | LengthStep: 4.00e-03 | J: 1.16103e+03 | Distance2Target: NaN 
iter: 005 | error: 2.198e+02 | LengthStep: 8.00e-03 | J: 1.15206e+03 | Distance2Target: NaN 
iter: 006 | error: 2.180e+02 | LengthStep: 1.60e-02 | J: 1.13430e+03 | Distance2Target: NaN 
iter: 007 | error: 2.144e+02 | LengthStep: 3.20e-02 | J: 1.09950e+03 | Distance2Target: NaN 
iter: 008 | error: 2.074e+02 | LengthStep: 6.40e-02 | J: 1.03270e+03 | Distance2Target: NaN 
iter: 009 | error: 1.938e+02 | LengthStep: 1.28e-01 | J: 9.09692e+02 | Distance2Target: NaN 
iter: 010 | error: 1.682e+02 | LengthStep: 2.56e-01 | J: 7.01968e+02 | Distance2Target: NaN 
iter: 011 | error: 1.235e+02 | LengthStep: 5.12e-01 | J: 4.10547e+02 | Distance2Target: NaN 
iter: 012 | error: 6.257e+01 | LengthStep: 1.02e+00 | J: 1.47399e+02 | Distance2Target: NaN 
Length Step has been change: LenghtStep = 0.256
iter: 013 | error: 4.113e+01 | LengthStep: 5.12e-01 | J: 1.20852e+02 | Distance2Target: NaN 
iter: 014 | error: 2.215e+01 | LengthStep: 1.02e+00 | J: 9.58348e+01 | Distance2Target: NaN 
Length Step has been change: LenghtStep = 0.256
Length Step has been change: LenghtStep = 0.064
Length Step has been change: LenghtStep = 0.016
Length Step has been change: LenghtStep = 0.004
Length Step has been change: LenghtStep = 0.001
Length Step has been change: LenghtStep = 0.00025
Length Step has been change: LenghtStep = 6.25e-05
Length Step has been change: LenghtStep = 1.5625e-05
Length Step has been change: LenghtStep = 3.9063e-06
Length Step has been change: LenghtStep = 9.7656e-07
Length Step has been change: LenghtStep = 2.4414e-07
Length Step has been change: LenghtStep = 6.1035e-08
Length Step has been change: LenghtStep = 1.5259e-08
Length Step has been change: LenghtStep = 3.8147e-09

    Mininum Length Step have been achive !! 

Elapsed time is 2.131655 seconds.



Visualization

First, we present the dynamics without control,

FreeThetaVector = solve(odeEqn,ControlGuess);
FreeThetaVector = full(FreeThetaVector);
figure
plot(FreeThetaVector')
ylabel('Phases [rad]')
xlabel('Time [sec]')
title('The dynamics without control (incoherence)')




and see the controled dynamics.

figure
plot(OptThetaVector_1')
ylabel('Phases [rad]')
xlabel('Time [sec]')
title(&quot;The dynamics under control L^2 | N_{osc}=&quot;+m)




We also can plot the control function along time.

figure
plot(OptControl_1)
legend(&quot;norm(u(t)) = &quot;+norm(OptControl_1))
ylabel('u(t)')
xlabel('Time [sec]')
title('The control function')




The problem with different regularization

In this part, we change the regularization into $L^1$-norm and see the difference.

PathCost  = casadi.Function('L'  ,{t,symTh,symU},{ sqrt(symU.^2+1e-3)});
iCP_2 = ocp(odeEqn,PathCost,FinalCost);


tic
[OptControl_2 ,OptThetaVector_2] =  ArmijoGradient(iCP_2,ControlGuess);
toc


Length Step has been change: LenghtStep = 0.00025
iter: 001 | error: 2.363e+01 | LengthStep: 5.00e-04 | J: 1.11735e+02 | Distance2Target: NaN 
iter: 002 | error: 2.363e+01 | LengthStep: 1.00e-03 | J: 1.11730e+02 | Distance2Target: NaN 
iter: 003 | error: 2.361e+01 | LengthStep: 2.00e-03 | J: 1.11720e+02 | Distance2Target: NaN 
iter: 004 | error: 2.359e+01 | LengthStep: 4.00e-03 | J: 1.11699e+02 | Distance2Target: NaN 
iter: 005 | error: 2.353e+01 | LengthStep: 8.00e-03 | J: 1.11659e+02 | Distance2Target: NaN 
iter: 006 | error: 2.343e+01 | LengthStep: 1.60e-02 | J: 1.11579e+02 | Distance2Target: NaN 
iter: 007 | error: 2.322e+01 | LengthStep: 3.20e-02 | J: 1.11420e+02 | Distance2Target: NaN 
iter: 008 | error: 2.282e+01 | LengthStep: 6.40e-02 | J: 1.11109e+02 | Distance2Target: NaN 
iter: 009 | error: 2.208e+01 | LengthStep: 1.28e-01 | J: 1.10509e+02 | Distance2Target: NaN 
iter: 010 | error: 2.081e+01 | LengthStep: 2.56e-01 | J: 1.09388e+02 | Distance2Target: NaN 
iter: 011 | error: 1.890e+01 | LengthStep: 5.12e-01 | J: 1.07391e+02 | Distance2Target: NaN 
iter: 012 | error: 1.674e+01 | LengthStep: 1.02e+00 | J: 1.04033e+02 | Distance2Target: NaN 
iter: 013 | error: 1.607e+01 | LengthStep: 2.05e+00 | J: 9.86861e+01 | Distance2Target: NaN 
iter: 014 | error: 4.081e+01 | LengthStep: 4.10e+00 | J: 9.24707e+01 | Distance2Target: NaN 
Length Step has been change: LenghtStep = 1.024
Length Step has been change: LenghtStep = 0.256
Length Step has been change: LenghtStep = 0.064
Length Step has been change: LenghtStep = 0.016
Length Step has been change: LenghtStep = 0.004
Length Step has been change: LenghtStep = 0.001
Length Step has been change: LenghtStep = 0.00025
Length Step has been change: LenghtStep = 6.25e-05
Length Step has been change: LenghtStep = 1.5625e-05
Length Step has been change: LenghtStep = 3.9063e-06
Length Step has been change: LenghtStep = 9.7656e-07
Length Step has been change: LenghtStep = 2.4414e-07
Length Step has been change: LenghtStep = 6.1035e-08
Length Step has been change: LenghtStep = 1.5259e-08
Length Step has been change: LenghtStep = 3.8147e-09

    Mininum Length Step have been achive !! 

Elapsed time is 2.494788 seconds.



figure

line(tspan,OptThetaVector_2')
ylabel('Phases [rad]')
xlabel('Time [sec]')
title(&quot;The dynamics under control L^1 | N_{osc}=&quot;+m)




figure
plot(OptControl_1)
line(1:length(OptControl_2),OptControl_2,'Color','red')

Psi_1 = norm(sin(OptThetaVector_1(:,end).' - OptThetaVector_1(:,end)),'fro');
Psi_2 = norm(sin(OptThetaVector_2(:,end).' - OptThetaVector_2(:,end)),'fro');

legend(&quot;u(t) with L^2-norm; Terminal cost = &quot;+Psi_1,&quot;u(t) with L^1-norm; Terminal cost = &quot;+Psi_2)
ylabel('The coupling strength (u(t))')
xlabel('Time [sec]')
title('The comparison between two different control cost functionals')




As one can expected from the regularization functions, the control function from $L^2$-norm acting more smoothly from $0$ to the largest value. The function from $L^2$-norm draws much stiff lines.

YFr = FreeThetaVector';
YL1 = OptThetaVector_1';
YL2 = OptThetaVector_2';
%%
animationpendulums({YFr,YL1,YL2},tspan,{'Free','L^2 Control','L^1 Control'})


Finally, we can see the behavior of the two control types against the evolution of free dynamics.



</description>
        <pubDate>Wed, 01 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://deustotech.github.io/DyCon-Blog/tutorial/wp06/P0007</link>
        <guid isPermaLink="true">https://deustotech.github.io/DyCon-Blog/tutorial/wp06/P0007</guid>
        
        
        <category>tutorial</category>
        
        <category>WP06</category>
        
      </item>
    
  </channel>
</rss>
