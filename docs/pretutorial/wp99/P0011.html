<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Welcome to the web interface of DyCon Toolbox, the computational platform developed within the <a href='https://cmc.deusto.eus/dycon/' target='_blank'>ERC DyCon - Dynamic Control</a> project.">
	<link rel='shortcut icon' type='image/png' href='/DyCon-Blog/favicon.png' />

  <title>
    Understanding Deep Learning: Neural Networks as bending manifolds - DyCon Blog
    
  </title>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <link rel="canonical" href="https://deustotech.github.io/DyCon-Blog/pretutorial/wp99/P0011">

  <!-- Bootstrap Core CSS -->
  <link rel="stylesheet" href="https://deustotech.github.io/DyCon-Blog/css/bootstrap.min.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="https://deustotech.github.io/DyCon-Blog/css/clean-blog.css">

   <!-- Adjust Colors -->
  <link rel="stylesheet" href="https://deustotech.github.io/DyCon-Blog/colorscheme.css">

  <!-- Pygments Github CSS -->
  <link rel="stylesheet" href="https://deustotech.github.io/DyCon-Blog/css/syntax.css">

  <link rel="stylesheet" href="https://deustotech.github.io/DyCon-Blog/css/google-search.css">


  
  <script src="https://deustotech.github.io/DyCon-Blog/lib/js/libgif.js "></script>
  <script src="https://deustotech.github.io/DyCon-Blog/lib/js/rubbable.js "></script>


  <!-- Google Fonts Raleway -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:400,700" rel="stylesheet">

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->
	<!-- MathJax -->
  <script type="text/x-mathjax-config">
	MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    },
		TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      },
      extensions: ["AMSmath.js", "AMSsymbols.js"],
      Macros:{
        norm: ["|| #1 ||",1],
        abs: ["| #1 |",1],
        hdots: "...",
        RR: "\\mathbb{R}",
        ffl: ["(d_x^2)^{#1}",1],
        ccs: "c_{1,s}",
        kernel: ["|x-y|^{#1}",1],
        ue: ["#1^{\\epsilon}",1]
      }
    },
    CommonHTML: {
      styles: {
        "@font-face /* vec */ ": {
          "font-family": "MJX-VEC",
          src: [
            "url(data:application/x-font-woff;charset=utf-8;base64,",
            "d09GRgABAAAAAARMAA0AAAAABiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABGRlRNAAAEMAAAABwA",
            "AAAccRGLqUdERUYAAAQMAAAAJAAAACgANAAmT1MvMgAAAaAAAABJAAAAYFc5gfFjbWFwAAACAAAA",
            "AEYAAAFKQxjlbWdhc3AAAAQEAAAACAAAAAj//wADZ2x5ZgAAAlQAAACUAAAAlIQU9HBoZWFkAAAB",
            "MAAAADAAAAA2C748e2hoZWEAAAFgAAAAHgAAACQFXQDqaG10eAAAAewAAAAUAAAAFAYvAE9sb2Nh",
            "AAACSAAAAAwAAAAMACgAcm1heHAAAAGAAAAAHQAAACAASQAmbmFtZQAAAugAAADxAAAB5uwlD0hw",
            "b3N0AAAD3AAAACgAAAA34M9aEXjaY2BkYGAA4uOfgq7G89t8ZeBmfgEUYbga+icNTssClVxnOgXk",
            "cjAwgUQBdqEMW3jaY2BkYGBu+feBgYHxCwMQMF5nYGRABawAcaQESgAAeNpjYGRgYGBlUGZgYgAB",
            "EMnIABJzAPMZAAZZAHAAAAB42mNgYSxn/MLAysDA1MW0h4GBoQdCMz5gMGRkAooysDEzwAAjECvA",
            "OAFprikMBxgUFCcxt/z7wMDA3MIoAFUDAwoMjACD8Qv5AAAAAfQAMgAAAAABTQAAAPoAAAH0AB14",
            "2mNgYGBmgGAZBkYGEHAB8hjBfBYGDSDNBqQZGZgYFBQn/f8P5IPp/4/vFUHVAwEjGwOcw8gEJJgY",
            "UAEjxIrhDACLGwmnAAAAAAAUABQAFAAUAEoAAgAyAAABwgIVAAMABwAAMxEhESUhESEyAZD+ogEs",
            "/tQCFf3rMgGxAAEAHQIEAdcCygAiAAABNDYzMhcWFxYXFhUUBgcGBw4DIyImNTQ3IycmNTQ3ISYB",
            "eQsJBwUFBQofCwYLKyACBwMGBAcNL6urDQ0BZxgCtggMBQQPJRQHCwkGBhYpAgkCAgsJEygBCQoH",
            "DSYAeNqVjrGKwkAURe/EGBBUrG12CishMrFQcDuLsAi26YMOmsIMxAh+hN8i+CF+gd+y4B3zttjC",
            "woHhncu7774HoI8rFP5eIKzQw5dwgAjfwi2McBEO6bkLtzHAr3CEnhrTqcKOpDasMCQ1HKCLWLiF",
            "HyyFQ3puwm1oPIQjDFXfR62Ro8YeK9YzMlhsqB0qtvJ6v8rPmd3Uzkv2tihwwoHCbosTa0pryYH0",
            "NbKjRWPB/z626Se8dsY/hSHPGeTKOnXVzuqF/reYOoln8dQk80+u9b0KR97rD9Rc4xdNXtVQI7PV",
            "sXClNiaZGGP0B+FPMN1KMAAAAHjaY2BiAIP/zQxGDNgAKxAzMjAxMDMycSUWFeWXF2WmZ5QAAGh5",
            "BhgAAAAB//8AAnjaY2BkYGDgA2IJBgUgycTAyMDMwAIkWcA8BgZGCAYACgIAWwAAAAEAAAAAxmW3",
            "ygAAAADVVeemAAAAANVV7Dg=) format('woff')"
          ].join('')
        },
        ".MJX-VEC": {
          "font-family": "MJX-VEC"
        }
      }
    }
  });
  MathJax.Hub.Register.StartupHook("CommonHTML Jax Ready", function () {
    var MML = MathJax.ElementJax.mml;
    var fixCombiningChar = MML.mo.prototype.CHTMLfixCombiningChar;
    MML.mo.Augment({
      CHTMLfixCombiningChar: function (node) {
        if (node.textContent === '\u20D7') {
          //
          //  Safari makes combining characters into non-combining ones when
          //  they don't have anything to combine with; replace \vec arrow
          //  with a non-combining one so all browsers get the same thing
          //
          node = node.firstChild;
          node.firstChild.nodeValue = "\u2192";
          node.className = "mjx-char MJX-VEC";
          node.style.marginLeft = "-.5em";
          node.style.paddingTop = ".45em";
        } else {
          fixCombiningChar.call(this,node);
        }
      }
    });
  });
	</script>
	<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
	<!--<script src="http://cdnjs.cloudflare.com/ajax/libs/d3/3.4.13/d3.min.js" type="text/javascript"></script>-->

    <!-- jQuery -->
	<script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>

	<!-- Bootstrap Core JavaScript -->
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>

	<!-- Custom Theme JavaScript -->
  <script src="https://deustotech.github.io/DyCon-Blog/js/clean-blog.min.js "></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-128123935-3"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-128123935-3');
  </script>
  <!-- End Google Analytics -->



  <!-- marKdown editor -->

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/simplemde/latest/simplemde.min.css">
  <script src="https://cdn.jsdelivr.net/simplemde/latest/simplemde.min.js"></script>
</head>

<body>
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <!-- <a class="navbar-brand" href="/DyCon-Blog/">
                 <img src="https://deustotech.github.io//DyCon-Blog//assets/logo_DyCon.png">
            </a> -->
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="/DyCon-Blog/">Home</a>
                </li>
                
                
                    
                        
                    
                                   
                <li>
                    <a href="/DyCon-Blog/projects/posts">Documentation</a>
                </li>
                
                    
                
                    
                <li>
                        <a href="https://deustotech.github.io/DyCon-Blog/projects/search.byWP">Sitemap</a>
                </li>
                <li>
                        <a href="http://cmc.deusto.eus/">Chair of Computational Mathematics</a>
                </li>
                
            </ul>

        
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

    <!-- Post Header -->
<header class="intro-header">
  <div class="container" style="padding-top: 70px;">
    <div class="row">
      <div class="col-lg-10 col-md-10 col-md-offset-1">
        <div class="post-heading" style="padding: 0px 0"><h2><small class="light-grey"><a href="https://deustotech.github.io//DyCon-Blog/projects/posts">Work Packages</a>
            &#8827;<a href="https://deustotech.github.io//DyCon-Blog/workpackage/WP99">
            Other Topics</a> &#8827;</small>
          </h2>
          <h1>Understanding Deep Learning: Neural Networks as bending manifolds</h1>
					<p class="meta">
            <div class="post-preview-authors ellipsis-one-line">
              Author:
              
              
                
                <span style="text-decoration: none;" itemprop="author" itemscope itemtype="http://schema.org/Person">
                  <span itemprop="name">
                    <a style="text-decoration: none;" href="https://deustotech.github.io/DyCon-Blog/author/SergiA"></a>
                  </span>
                </span>
              
              - 01 November 2020
            </div>
					</p>
        </div>
      </div>
		</div>
	</div>
</header>
<!-- Post Content -->
<style>
img {
	display:block;
	max-width:  100%;
	margin-left: auto;
	margin-right: auto;
}

@media only screen and (min-width: 1000px) {
img {
	-moz-transition:-moz-transform 0.5s ease-in; 
	-webkit-transition:-webkit-transform 0.5s ease-in; 
	-o-transition:-o-transform 0.5s ease-in;
}
img:active{
	-moz-transform:scale(1.2); 
	-webkit-transform:scale(1.2);
	-o-transform:scale(1.2);
}
}

@media only screen and (min-width: 1250px) {
img {
	-moz-transition:-moz-transform 0.5s ease-in; 
	-webkit-transition:-webkit-transform 0.5s ease-in; 
	-o-transition:-o-transform 0.5s ease-in;
}
img:active{
	-moz-transform:scale(1.5); 
	-webkit-transform:scale(1.5);
	-o-transform:scale(1.5);
}
}

@media only screen and (min-width: 1500px) {
img {
	-moz-transition:-moz-transform 0.5s ease-in; 
	-webkit-transition:-webkit-transform 0.5s ease-in; 
	-o-transition:-o-transform 0.5s ease-in;
}
img:active{
	-moz-transform:scale(2); 
	-webkit-transform:scale(2);
	-o-transform:scale(2);
}
}
</style>
<article>
  <div id="content" class="container">
    <div class="row">
      <div class="col-md-10 col-md-offset-1">
        
          
            <a href="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0008/P0016_Propagation_of_discretes_waves.zip""><i class="fa fa-download fa-lg"></i>Download Code</a>
          
        
        <h2 id="data-and-classification">Data and classification</h2>
<h3 id="supervised-learning">Supervised Learning</h3>

<p>Consider a dataset $\mathcal{D}$ consisting on $S$ distinct points ${x_i}_{i=1}^{S}$, each of them with a corresponding value $y_i$ such that</p>

<script type="math/tex; mode=display">\mathcal{D} = \{(x_i , y_i)\}_{i=1}^S</script>

<script type="math/tex; mode=display">y_i = \mathcal{F} (x_i) + \epsilon _i \quad \forall i \in \{1, ... S\}</script>

<p>where $\epsilon _i$ is included so as to consider, in principle, noisy data.</p>

<p>The aim of supervised learning is to recover the function $\mathcal{F}(\cdot)$ such that not only fits well in the database $\mathcal{D}$ but also generalises to previously unseen data.</p>

<p>That is, it is inferred the existence of</p>

<script type="math/tex; mode=display">\tilde{\mathcal{D}} = \{ (\tilde{x}_i , \tilde{y}_i) \}_{i=1}^{\infty}</script>

<p>where</p>

<script type="math/tex; mode=display">\tilde{y}_i \approx \mathcal{F}(\tilde{x}_i) \quad \forall i</script>

<p>being $\mathcal{F} (\cdot)$ the desired previous application, and $\mathcal{D} \subset \tilde{\mathcal{D}}$.</p>

<p><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/generaliseddataset.jpg" alt="GeneralisedDataset" /></p>

<p><br /></p>

<h3 id="noisy-vs-noisless-data">Noisy vs. noisless data</h3>

<p>The generalised dataset $\tilde{\mathcal{D}}$ can be seen as the realisation of a random vector $(\mathbf{x}, \mathbf{y})$ generated by an unknown multivariate joint distribution between $\mathcal{X}$ and  $\mathcal{Y}$.</p>

<p>Then, the datasets used in Supervised Learning can be seen as a finite sample of the random vector $(\mathbf{x}, \mathbf{y})$, and so the underlying joint distribution between $\mathcal{X}$ and $\mathcal{Y}$ could be in principle recovered in the limit of infinitely many samples, $\tilde{\mathcal{D}}$ .</p>

<p>We refer to noisy data when the samples in our dataset $\mathcal{D}$ actually comes from a random vector $(\mathbf{x}, \mathbf{y})$ with a non-deterministic joint distribution, such that it is not possible to find a function $\mathcal{F}$ such that $\mathcal{F}(\mathbf{x}) = \mathbf{y}$.</p>

<p>The aim of supervised learning, in this case, would be to find a function such that $\mathcal{F}(\tilde{x}_i) \approx \tilde{y}_i$ for almost every sample, as with respect to a given loss function.</p>

<p>In case the data is considered to be noisless (which does not happen in most practical situations, but it makes things far simpler) then the joint distribution between $\mathcal{X}$ and $\mathcal{Y}$ of the random vector $(\mathbf{x}, \mathbf{y})$ is rather deterministic. In this case, it is possible to find a function $F$ such that $\mathcal{F}(\tilde{x}_i) = \tilde{y}_i \; \forall i$, or equivalently $\mathcal{F}(\mathbf{x}) = \mathbf{y}$.</p>

<p>Note that, given that the probability of a collision (of having two different data-points such that $x_i = x_j$ for $i \neq j$) is practically $0$, it is in principle possible to find a function $\mathcal{F}$ such that $\mathcal{F}(x_i)=y_i \; \forall i = { 1, …  ,S }$, even in the noisy case (i.e. we may use interpolation). 
However, the aim of <em>Supervised Learning</em> is to be able to generalise well to previously unseen data, $\tilde{\mathcal{D}}$, and so interpolation is not effective.</p>

<p>Some systems are often approximated as being noisless. Since the aim of supervised learning is finding functions $\mathcal{F}$, there is always the implicit assumption that the generalised dataset $\tilde{\mathcal{D}}$ can be approximately represented by a function $\mathcal{F}$, and so the noise is expected to be small, compared to the “real data”. Distinguishing data from noise is often far a from trivial issue.</p>

<p>The idea of working with noisy data can be recasted as a Mean-Field problem, as proposed by Weinan E. et al [2]. The idea is to control a distribution, instead of a set of points in an euclidean space. Although understanding Supervised Learning with noisless data from a Dynamical Control perspective is often enough ill-posed, considering noisy data unlocks the full theoretical experience.</p>

<p><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/generalnoisless.gif" alt="Noisless" /></p>

<p><br /></p>

<h3 id="binary-classification">Binary classification</h3>
<p>In most cases, the data points $x_i$ are in $\mathbb{R}^{n}$. In general, $x_i \in X \; \forall i$, with $X \subset \mathbb{R}^{n}$. So as to ease the training of the models, it is usually considered $X = [-1,+1]^{n}$, with $n$ an arbitrary dimension. This is called <em>data normalization</em> in the Machine Learning literature.</p>

<p>There are different supervised learning problems depending on the space $Y$ of the corresponding labels, such that $y_i \in Y$.<br />
If $Y$ is discrete, such that $Y = { 0, … , L-1}$, then we are dealing with the problem of classification. If $L=2$, then it is a binary classification problem.</p>

<p>In this particular setting, the aim is to find a function $F (\cdot)$ such that it returns $0$ whenever the corresponding data point is of class $0$, and $1$ conversely. 
Note that such a function cannot be continuous except for trivial cases, given that  the data points in $X$ cover all the space $[-1, +1]$, due to the existence of a boundary between points of class $0$ and $1$.</p>

<h3 id="data-subspaces-and-manifolds">Data, subspaces and manifolds</h3>

<p>Considering $X^0$ the subset of $X$ such that $F(\tilde{x}_i) =0 \; \; \forall \tilde{x}_i \in X^0$ and $X^1$ s.t.  $\, F(\tilde{x}_i)=1 \; \; \forall \tilde{x}_i \in X^1$, it is often useful to consider $d(X^0 , X^1) &gt; \delta \in \mathbb{R}^{+}$. 
Consider that the $x-$points in the dataset $\tilde{\mathcal{D}}$ are in $X= X^0 \cup X^1$.</p>

<p>In this aforementioned case, it is in principle possible to find a continuous function $F(\cdot)$ such that $F(\tilde{x}_i) = \tilde{y}_i \; \forall i$ , since it does not need to be defined in the boundary (the boundary is not in $X$).</p>

<p>The simpler case is one in which $X^0$ and $X^1$ are connected spaces.</p>

<p><br /></p>

<h2 id="deep-learning">Deep Learning</h2>
<h3 id="neural-networks">Neural Networks</h3>

<p>The problem is now how to obtain the functions $\mathcal{F}$.</p>

<p>In classical settings, an hypothesis space is often proposed as</p>

<script type="math/tex; mode=display">\mathcal{H} = \left\{  f (\cdot \, ; \mathbf{\theta }) \; \bigg\vert  f(\cdot ; \theta ) = \phi \left(\sum_{i=1}^{m} a_i \psi _i (\cdot) \right) \right\}</script>

<p>where $\theta = { a_i }_{i=1}^{m}$ are the coefficients, ${ \psi _i}$ are the proposed functions (for example, they would be monomials if we are doing polynomial regression, sine and cosines if using Fourier, …) and the function $\phi$ is the terminal loss function, that is used to match the dimensionality of $f(x_i \; ;  \theta)$ with $y_i$; for example, in the case of binary classification, $\phi (  \cdot )$ can be chosen to be the function $sign ( \cdot )$, and so $\phi (\cdot ) : \mathbb{R} \to {0,1 }$.</p>

<p>The function $\mathcal{F}$ correspondent to the dataset $\mathcal{D}$ is then defined as</p>

<script type="math/tex; mode=display">\mathcal{F}(\cdot ) \equiv f(\cdot \, ; \theta ^*) \in \mathcal{H} \quad s.t. \quad \theta ^* = \arg \min \sum_{i=1}^{S} d(\phi (f(x_i \, ; \theta )) , y_i)</script>

<p>where $d ( , )$ is a distance defined by a loss function. 
That is, it is the function in the hypothesis space that minimizes the “distance” with the target function, which connects the datapoints $x_i$ with their correspondent labels $y_i$.</p>

<p>When using Neural Networks, the hypothesis space $\mathcal{H}$ is generated by the composition of functions, such that</p>

<script type="math/tex; mode=display">\mathcal{H} =  \left\{  \text{NN}(\cdot \, ; \Theta ) \; \bigg\vert  \text{NN}(\cdot ; \Theta ) = \phi \circ f_{\theta_L}^L \circ f_{\theta_{L-1}}^{L-1} \circ ... \circ f_{\theta_1}^1 (\cdot) \right\}</script>

<p>where $\Theta = { \theta_k }_{k=1}^{L}$ are the training weights of the network and $L$ the number of hidden layers.</p>

<h4 id="multilayer-perceptrons">Multilayer perceptrons</h4>

<p>In the case of the so-called multilayer perceptrons, the functions $f^k_{\theta_k}$ are constructed as</p>

<script type="math/tex; mode=display">f^k_{\theta_k} (\cdot) = \sigma (A^k  (\cdot) +  b^k)</script>

<p>being $A^k$ a matrix,$b^k$ a vector and $\sigma \in C^{0,1}(\mathbb{R})$ a fixed nondecreasing function.</p>

<p>Since we restrict to the case in which the number of neurons is constant through the layers, we have that $A^k \in \mathbb{R}^{d \times d}$ and $b^k \in \mathbb{R}^d$.</p>

<p><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/multilayerdiagram.jpg" alt="Multilayer" /></p>

<h4 id="residual-neural-networks">Residual Neural Networks</h4>
<p>In the simple case of a Residual Neural Network (often referred to as ResNets) the functions $f_{\theta_k}^{k} (\cdot)$ are constructed as</p>

<script type="math/tex; mode=display">f_{\theta_k}^{k} (\cdot) = \mathbb{I} (\cdot) + \sigma (A^k  (\cdot) +  b^k)</script>

<p>where $\mathbb{I}$ is the identity function. The parameters $\sigma$, $A^k$ and $b^k$ are equivalent to those of the multilayer perceptrons.</p>

<p>It is sometimes convenient to numerically add a parameter $h$ to the residual block, such that</p>

<script type="math/tex; mode=display">f_{\theta_k}^{k} (\cdot) = \mathbb{I} (\cdot) + h  \, \sigma (A^k  (\cdot) +  b^k)</script>

<p>being $h \in \mathbb{R}$ a scalar.</p>

<p><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/resnetsdiagram.jpg" alt="ResNet" /></p>

<h3 id="training-neural-networks">Training Neural Networks</h3>

<p>The aim is to find the weights ${ A, b }$ of every layer such that the final function represented by the Neural Network minimizes a given loss function.</p>

<p>The weights are initialized randomly, and the algorithms (i.e. gradient descent) are aimed at updating them, such that eventually the Neural Network represents a function  $\hat{F}$  such that $\hat{F} (x_i) \approx y_i \, \forall i$.</p>

<p><br /></p>

<h2 id="visualizing-deep-learning">Visualizing Deep Learning</h2>

<table style="width:100%">
  <tr>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/layertransformation0.gif" /></th>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/layertransformation1.gif" /></th>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/layertransformation2.gif" /></th>
  </tr>
  <tr>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/layertransformation3.gif" /></th>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/layertransformation4.gif" /></th>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/layertransformation5.gif" /></th>
  </tr>
</table>

<h3 id="classification">Classification</h3>

<p>When dealing with data classification, it is very useful to just assign a color / shape to every label, and so be able to visualize data in a lower-dimensional plot.</p>

<p>The aim of classification is to associate different classes to different regions of the initial space,
When using Neural Networks, the initial space $X$ is propagated by the Neural Network as $f ^L \circ  f^{L-1} \circ … \circ f^1 (X)$. 
Then the function $\phi$ would be the one in charge of associating different classes to different regions, but the functions $\phi$ are pretty simple (essentially linear separators)!</p>

<p>So, while it may be difficult to classificate the datapoints in the initial space $X$, the Neural Networks should make things simpler and just propagate the data so as to make it linearly separable (and we would see this in real action!).</p>

<h3 id="neural-networks-as-homeomorphisms">Neural Networks as homeomorphisms</h3>

<p><strong><em>Lemma 1.1</em></strong>
<em>A Neural Network represents a continuous function if each one of its blocks are continuous.</em></p>

<p><strong><em>Lemma 1.2</em></strong> 
<em>A Neural Network represents a bijection if each one of its blocks are bijective.</em></p>

<p>These come trivially from the fact that the composite of continuous functions is continuous, and that the composite of bijections is a bijection.</p>

<p><strong><em>Theorem 1</em></strong> <strong>(Sufficient condition for invertible ResNets [1]):</strong></p>

<p>Let $\hat{\mathcal{F}} \, : \, \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ with $\hat{\mathcal{F}} (\cdot ) = ( L^{1}_{\theta} \, \circ … \circ \, L^{T}_{\theta}  )$ denote a <em>ResNet</em> with blocks defined by 
<script type="math/tex">L^{t} _{ \theta } = \mathbb{I} + h \cdot g_{\theta _t}</script></p>

<p>Then, the ResNet $\hat{\mathcal{F}}_{\theta}$ is invertible if</p>

<script type="math/tex; mode=display">% <![CDATA[
h \cdot \text{Lip} (g_{\theta_t}) < 1 \text{ for all } t = 1, ..., T %]]></script>

<p><strong><em>Proof</em></strong> 
Consider that each block of the <em>ResNet</em> is given by</p>

<script type="math/tex; mode=display">Z(x) =  x + h\cdot G(x)</script>

<p>where $x$ is the input of the block, and $G(x)$ given by the residual block.
That can be rewritten as</p>

<script type="math/tex; mode=display">x_{t+1} = x_t + g(x)</script>

<p>We can construct the inverse function of the block as</p>

<script type="math/tex; mode=display">x_t = x_{t+1} - g(x)</script>

<p>Which is not an explicit inverse, since $g(x)$ depends on $x$. Approximating the solution as an interation,</p>

<script type="math/tex; mode=display">x^{0}_{t}  \doteq x_{t+1} \quad \text{and} \quad x_t ^{k+1} \doteq x_{t+1} - g (x_{t}^{k})</script>

<p>where $\lim_{k \to \infty} x_t ^{k} = x_t$ if the iteration converges. 
Since $g(\cdot) : \mathbb{R}^{d} \to \mathbb{R}^{d}$ is an operator on a Banach space, due to the Banach fixed point theorem is it enough that $Lip(g) &lt; 1$, or equally that $Lip(h\cdot G) &lt;1$ for the iteration to converge, and so for the block of the <em>ResNet</em> to be invertible. [1]</p>

<p><strong><em>Comment:</em></strong> <em>In the limit $h \to 0$, and $G(x)$ Lipschitz, we have a trivial explicit expression of the inverse of the function represented by the ResNet.</em></p>

<p>Consider the <em>ResNet</em> as generated by the iteration of blocks</p>

<script type="math/tex; mode=display">Z(x) =  x + h\cdot G(x)</script>

<p>where $G(x)$ is generated by a residual block <a href=""></a>. We consider $G(x)$ Lipschitz.</p>

<p>Then,</p>

<script type="math/tex; mode=display">Z^{-1} (x) \equiv x - h\cdot G(x)</script>

<p>is an second order approximation of the inverse function of $Z$ in terms of $h$, since</p>

<script type="math/tex; mode=display">Z^{-1} (Z(x)) = Z^{-1}  (x + h\cdot G(x) ) =</script>

<script type="math/tex; mode=display">= x + h\cdot G(x) - h \cdot G(x + h \cdot G(x)) =</script>

<script type="math/tex; mode=display">= x + \mathcal{o}(h^2)</script>

<p>Indeed, if we express the residual blocks as</p>

<script type="math/tex; mode=display">x_{t+1} = x_{t} + h\cdot G(x_t)</script>

<p>the limit $h\to 0$ can be recasted as</p>

<script type="math/tex; mode=display">\lim_{h \to 0} \frac{x_{t+1}-x_{t}}{h} = \dot{x}_t = G(x_t)</script>

<p>and, since the discrete index $t$ becomes continuous in this limit, it is useful to express it as</p>

<script type="math/tex; mode=display">\dot{x}(t) = G(x(t), t)</script>

<p>where in this case we make explicit the dependence of $G$ on $t$, that was hidden in the previous notation.</p>

<p><br /></p>

<table style="width:100%">
  <tr>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/3dmultilayer_.gif" /></th>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/3dmultilayer_spiral.gif" /></th>
  </tr>
</table>

<h2 id="setup">Setup</h2>

<p>Consider a binary classification problem with noisless data, such that</p>

<script type="math/tex; mode=display">\mathcal{D} =  \{ ( x_i , y_i = \mathcal{F}(x_i) )\} _{i=1}^{S}</script>

<script type="math/tex; mode=display">x_i \in [-1,+1]^{n} , \; \; y_i \in \{0,1\} \quad \forall i \in \{1, ... , S\}</script>

<table style="width:100%">
  <tr>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/2dbending0.gif" /></th>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/2dbending1.gif" /></th>
  </tr>
  <tr>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/2dbending2.gif" /></th>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/2dbending3.gif" /></th>
  </tr>
</table>

<p><br /></p>

<table style="width:100%">
  <tr>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/2dbendingspiral0.gif" /></th>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/2dbendingspiral1.gif" /></th>
  </tr>
  <tr>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/2dbendingspiral2.gif" /></th>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/2dbendingspiral3.gif" /></th>
  </tr>
</table>

<p><br /></p>

<table style="width:100%">
  <tr>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/3dbendingspiral2.gif" /></th>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/3dbendingspiral3.gif" /></th>
  </tr>
</table>

<p><br /></p>

<table style="width:100%">
  <tr>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/bending0.gif" /></th>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/bending1.gif" /></th>
  </tr>
  <tr>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/bending2.gif" /></th>
    <th><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/bending3.gif" /></th>
  </tr>
</table>

<p><br /></p>

<h2 id="open-questions-issues-and-perspectives">Open questions, issues and perspectives</h2>

<p><img src="https://deustotech.github.io/DyCon-Blog/assets/imgs/WP99/P0011/NoisyRing.gif" alt="NoisyDistribution" /></p>

<h2 id="references">References</h2>

<p>[1] J. Behrmann, W. Grathwohl, R. T. Q. Chen, D. Duvenaud, J. Jacobsen (2018). <i> Invertible Residual Networks</i> <a href="https://arxiv.org/abs/1811.00995">pdf</a></p>

<p>[2] Weinan E, Jiequn Han, Qianxiao Li, (2018). <i> A Mean-Field Optimal Control Formulation of Deep Learning</i>. volume 6. Research in the Mathematical Sciences<a href="https://doi.org/10.1007/s40687-018-0172-y">pdf</a></p>

<p>[3] S. Shalev-Shwartz, S. Ben-David, <i> The Understanding Machine Learning: From Theory to Algorithms </i>. <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/">pdf</a></p>

<p>[4] B. Hanin, (2018). <i> Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients? </i>  <a href="https://arxiv.org/abs/1801.03744">pdf</a></p>

<p>[5] C. Olah, <i> Neural Networks, Manifolds and Topology </i>. <a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">blog post</a></p>

<p>[6] G. Naitzat, A. Zhitnikov, L-H. Lim, (2020). <i> Topology of Deep Neural Networks </i> <a href="https://arxiv.org/abs/2004.06093">pdf</a></p>

<p>[7] Q. Li, T. Lin, Z. Shen, (2019). <i> Deep Learning via Dynamical Systems: An Approximation Perspective </i><a href="https://arxiv.org/abs/1912.10382">pdf</a></p>

<p>[8] B. Geshkovski, <i> The interplay of control and deep learning </i>. <a href="https://deustotech.github.io/DyCon-Blog/tutorial/wp01/P0010">blog post</a></p>

<p>[9] C. Fefferman, S. Mitter, H. Narayanan, <i> Testing the manifold hypothesis </i>. <a href="http://www.mit.edu/~mitter/publications/121_Testing_Manifold.pdf">pdf</a></p>

<p>[10] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, D. Duvenaud, (2018). <i> Neural Ordinary Differential Equations </i> <a href="https://arxiv.org/abs/1806.07366">pdf</a></p>


        <hr>
      </div>
    </div>
  </div>
</article>
<hr>

    <!-- Footer -->
<footer>
  <hr>
  <div class="container">
    <div class="row">
      <ul class="list-inline text-center">
        <li style="margin-right: 50px;">
          <img style="width: 250px;" src="/DyCon-Blog/img/logos/logo_DyCon.png " alt="">
        </li>
        <li style="padding-bottom: 5px;">
          <img style="width: 230px;" src="/DyCon-Blog/img/logos/logo_CCM_subDerecha_v002.png " alt="">
        </li>
      </ul>
    </div>
  </div>
</footer>

</body>
</html>
